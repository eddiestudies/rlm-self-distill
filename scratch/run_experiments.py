#!/usr/bin/env python3
"""
Run experiments across multiple models with rule tracking.

Tracks:
- Rule creation costs (tokens spent generating rules)
- Rule usage (tokens saved by using rules instead of LLM)
- Comparison: with rules vs without rules

Usage:
    python run_experiments.py
    python run_experiments.py --models llama3.2:3b qwen2.5-coder:32b
"""

import argparse
import json
import re
import sys
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Callable

sys.path.insert(0, str(Path(__file__).parent.parent))

from self_distill.clients.ollama_client import OllamaClient
from self_distill.datasets import DATA, load_dataset


@dataclass
class CallRecord:
    """Record of a single LLM call."""

    call_id: int
    model: str
    messages: list[dict]
    response: str
    prompt_tokens: int
    completion_tokens: int
    duration_ms: float
    timestamp: str


@dataclass
class Rule:
    """A generated rule that can replace LLM calls."""

    rule_id: str
    name: str
    description: str
    category: str
    code: str  # Python code for the rule
    creation_prompt: str  # Prompt used to generate the rule
    creation_response: str  # LLM response with the rule
    creation_tokens: int  # Tokens spent creating this rule
    creation_model: str
    times_used: int = 0
    tokens_saved: int = 0  # Total tokens saved by using this rule
    correct_uses: int = 0
    incorrect_uses: int = 0
    example_inputs: list[str] = field(default_factory=list)
    example_outputs: list[str] = field(default_factory=list)


@dataclass
class ExperimentResult:
    """Result of a single experiment."""

    experiment_id: str
    name: str
    category: str
    model: str
    input_messages: list[dict]
    expected: str | None
    actual_response: str
    is_correct: bool | None
    calls: list[CallRecord]
    total_tokens: int
    total_duration_ms: float
    timestamp: str
    # Rule tracking
    used_rule: bool = False
    rule_id: str | None = None
    tokens_if_no_rule: int = 0  # Estimated tokens if we had called LLM instead
    tokens_saved: int = 0
    metadata: dict = field(default_factory=dict)


@dataclass
class ExperimentBatch:
    """A batch of experiments with rule tracking."""

    batch_id: str
    name: str
    models: list[str]
    results: list[ExperimentResult]
    rules: list[Rule]
    summary: dict
    rule_summary: dict
    created_at: str


class RuleEngine:
    """Manages rules and their execution."""

    def __init__(self):
        self.rules: dict[str, Rule] = {}
        self.rule_functions: dict[str, Callable] = {}

    def add_rule(self, rule: Rule):
        """Add a rule and compile its code."""
        self.rules[rule.rule_id] = rule

        # Compile the rule code
        try:
            local_ns = {}
            exec(rule.code, {"re": re}, local_ns)
            if "apply_rule" in local_ns:
                self.rule_functions[rule.rule_id] = local_ns["apply_rule"]
        except Exception as e:
            print(f"Warning: Could not compile rule {rule.rule_id}: {e}")

    def apply_rule(self, rule_id: str, input_text: str) -> tuple[str | None, bool]:
        """Apply a rule and return (result, success)."""
        if rule_id not in self.rule_functions:
            return None, False

        try:
            result = self.rule_functions[rule_id](input_text)
            self.rules[rule_id].times_used += 1
            return str(result), True
        except Exception as e:
            print(f"Rule {rule_id} failed: {e}")
            return None, False

    def get_rule(self, rule_id: str) -> Rule | None:
        return self.rules.get(rule_id)

    def all_rules(self) -> list[Rule]:
        return list(self.rules.values())


class ExperimentRunner:
    """Run experiments with rule tracking."""

    def __init__(self, models: list[str], verbose: bool = True):
        self.models = models
        self.verbose = verbose
        self.client = OllamaClient(model_name=models[0])
        self.results: list[ExperimentResult] = []
        self.rule_engine = RuleEngine()
        self.experiment_counter = 0

        # Track baseline token usage for comparison
        self.baseline_tokens: dict[
            str, int
        ] = {}  # category -> avg tokens without rules

    def log(self, msg: str):
        if self.verbose:
            print(msg)

    def add_hardcoded_rule(
        self,
        name: str,
        category: str,
        description: str,
        code: str,
        examples: list[tuple[str, str]],
    ) -> Rule:
        """Add a hardcoded rule (not generated by LLM)."""
        rule_id = f"rule_{len(self.rule_engine.rules) + 1:03d}"

        self.log(f"\n{'─' * 60}")
        self.log(f"Adding hardcoded rule: {name}")
        self.log(f"{'─' * 60}")

        rule = Rule(
            rule_id=rule_id,
            name=name,
            description=description,
            category=category,
            code=code,
            creation_prompt="[hardcoded]",
            creation_response="[hardcoded]",
            creation_tokens=0,  # No LLM cost
            creation_model="hardcoded",
            example_inputs=[e[0] for e in examples],
            example_outputs=[e[1] for e in examples],
        )

        self.rule_engine.add_rule(rule)
        self.log(f"Rule added: {rule_id}")

        return rule

    def generate_rule(
        self,
        name: str,
        category: str,
        description: str,
        examples: list[tuple[str, str]],  # [(input, expected_output), ...]
        model: str,
    ) -> Rule:
        """Generate a rule from examples using LLM."""
        rule_id = f"rule_{len(self.rule_engine.rules) + 1:03d}"

        # Build prompt for rule generation
        examples_str = "\n".join(
            [f"  Input: {inp}\n  Output: {out}" for inp, out in examples]
        )

        prompt = f"""Create a Python function that implements this rule.

Rule: {description}

Examples:
{examples_str}

Write a Python function called `apply_rule(input_text: str) -> str` that:
1. Takes the input text
2. Applies the rule logic
3. Returns the result

Only output the Python code, no explanations. The function should be self-contained.
Use the `re` module if needed for regex operations.

```python
def apply_rule(input_text: str) -> str:
    # Your implementation here
```"""

        messages = [
            {
                "role": "system",
                "content": "You are a Python expert. Write clean, working code.",
            },
            {"role": "user", "content": prompt},
        ]

        self.log(f"\n{'─' * 60}")
        self.log(f"Generating rule: {name}")
        self.log(f"{'─' * 60}")

        start_time = time.time()
        response = self.client.completion(messages, model=model)
        (time.time() - start_time) * 1000

        usage = self.client.get_last_usage()
        creation_tokens = usage.total_input_tokens + usage.total_output_tokens

        # Extract code from response
        code_match = re.search(r"```python\s*(.*?)\s*```", response, re.DOTALL)
        if code_match:
            code = code_match.group(1)
        else:
            # Try to find function directly
            code_match = re.search(
                r"(def apply_rule.*?)(?=\n\n|\Z)", response, re.DOTALL
            )
            code = code_match.group(1) if code_match else response

        rule = Rule(
            rule_id=rule_id,
            name=name,
            description=description,
            category=category,
            code=code,
            creation_prompt=prompt,
            creation_response=response,
            creation_tokens=creation_tokens,
            creation_model=model,
            example_inputs=[e[0] for e in examples],
            example_outputs=[e[1] for e in examples],
        )

        self.rule_engine.add_rule(rule)

        self.log(f"Rule generated: {rule_id}")
        self.log(f"Tokens used: {creation_tokens}")
        self.log(f"Code:\n{code[:200]}...")

        return rule

    def run_with_rule(
        self,
        name: str,
        category: str,
        input_text: str,
        rule_id: str,
        expected: str | None = None,
        baseline_tokens: int = 50,  # Estimated tokens if we called LLM
    ) -> ExperimentResult:
        """Run an experiment using a rule instead of LLM."""
        self.experiment_counter += 1
        exp_id = f"exp_{self.experiment_counter:04d}"

        self.log(f"\n{'─' * 60}")
        self.log(f"[{exp_id}] {name} | Using rule: {rule_id}")
        self.log(f"{'─' * 60}")

        start_time = time.time()
        result, success = self.rule_engine.apply_rule(rule_id, input_text)
        duration_ms = (time.time() - start_time) * 1000

        if not success:
            self.log("Rule failed, falling back to LLM")
            # Fall back to LLM
            return self.run_single(
                name,
                category,
                [{"role": "user", "content": input_text}],
                self.models[0],
                expected,
            )

        # Check correctness
        is_correct = None
        if expected is not None:
            is_correct = expected.lower() in result.lower()
            rule = self.rule_engine.get_rule(rule_id)
            if is_correct:
                rule.correct_uses += 1
            else:
                rule.incorrect_uses += 1

        # Calculate tokens saved
        tokens_saved = baseline_tokens  # We saved all the tokens we would have used
        rule = self.rule_engine.get_rule(rule_id)
        rule.tokens_saved += tokens_saved

        self.log(f"Result: {result}")
        self.log(f"Tokens saved: {tokens_saved} (rule used instead of LLM)")
        if expected:
            status = "✓ CORRECT" if is_correct else "✗ INCORRECT"
            self.log(f"Expected: {expected} | {status}")

        exp_result = ExperimentResult(
            experiment_id=exp_id,
            name=name,
            category=category,
            model="rule:" + rule_id,
            input_messages=[{"role": "user", "content": input_text}],
            expected=expected,
            actual_response=result,
            is_correct=is_correct,
            calls=[],  # No LLM calls
            total_tokens=0,
            total_duration_ms=duration_ms,
            timestamp=datetime.now().isoformat(),
            used_rule=True,
            rule_id=rule_id,
            tokens_if_no_rule=baseline_tokens,
            tokens_saved=tokens_saved,
        )

        self.results.append(exp_result)
        return exp_result

    def run_single(
        self,
        name: str,
        category: str,
        messages: list[dict],
        model: str,
        expected: str | None = None,
        correctness_fn: Callable = None,
        metadata: dict = None,
    ) -> ExperimentResult:
        """Run a single experiment with LLM."""
        self.experiment_counter += 1
        exp_id = f"exp_{self.experiment_counter:04d}"

        self.log(f"\n{'─' * 60}")
        self.log(f"[{exp_id}] {name} | Model: {model}")
        self.log(f"{'─' * 60}")

        start_time = time.time()
        response = self.client.completion(messages, model=model)
        duration_ms = (time.time() - start_time) * 1000

        usage = self.client.get_last_usage()
        prompt_tokens = usage.total_input_tokens
        completion_tokens = usage.total_output_tokens
        total_tokens = prompt_tokens + completion_tokens

        call = CallRecord(
            call_id=1,
            model=model,
            messages=messages,
            response=response,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            duration_ms=duration_ms,
            timestamp=datetime.now().isoformat(),
        )

        is_correct = None
        if expected is not None:
            if correctness_fn:
                is_correct = correctness_fn(response, expected)
            else:
                is_correct = expected.lower() in response.lower()

        self.log(f"Response: {response[:200]}{'...' if len(response) > 200 else ''}")
        self.log(
            f"Tokens: {prompt_tokens} in / {completion_tokens} out | {duration_ms:.0f}ms"
        )
        if expected:
            status = "✓ CORRECT" if is_correct else "✗ INCORRECT"
            self.log(f"Expected: {expected} | {status}")

        # Track baseline for this category
        if category not in self.baseline_tokens:
            self.baseline_tokens[category] = total_tokens
        else:
            self.baseline_tokens[category] = (
                self.baseline_tokens[category] + total_tokens
            ) // 2

        result = ExperimentResult(
            experiment_id=exp_id,
            name=name,
            category=category,
            model=model,
            input_messages=messages,
            expected=expected,
            actual_response=response,
            is_correct=is_correct,
            calls=[call],
            total_tokens=total_tokens,
            total_duration_ms=duration_ms,
            timestamp=datetime.now().isoformat(),
            used_rule=False,
            tokens_if_no_rule=total_tokens,
            metadata=metadata or {},
        )

        self.results.append(result)
        return result

    def run_across_models(
        self,
        name: str,
        category: str,
        messages: list[dict],
        expected: str | None = None,
        correctness_fn: Callable = None,
        metadata: dict = None,
    ) -> list[ExperimentResult]:
        """Run the same experiment across all models."""
        results = []
        for model in self.models:
            result = self.run_single(
                name=name,
                category=category,
                messages=messages,
                model=model,
                expected=expected,
                correctness_fn=correctness_fn,
                metadata=metadata,
            )
            results.append(result)
        return results

    def get_summary(self) -> dict:
        """Get summary statistics."""
        by_model = {}
        by_category = {}

        for r in self.results:
            model_key = r.model if not r.used_rule else "rules"

            if model_key not in by_model:
                by_model[model_key] = {
                    "total": 0,
                    "correct": 0,
                    "incorrect": 0,
                    "tokens": 0,
                    "duration_ms": 0,
                    "tokens_saved": 0,
                }
            by_model[model_key]["total"] += 1
            by_model[model_key]["tokens"] += r.total_tokens
            by_model[model_key]["duration_ms"] += r.total_duration_ms
            by_model[model_key]["tokens_saved"] += r.tokens_saved
            if r.is_correct is True:
                by_model[model_key]["correct"] += 1
            elif r.is_correct is False:
                by_model[model_key]["incorrect"] += 1

            if r.category not in by_category:
                by_category[r.category] = {
                    "total": 0,
                    "correct": 0,
                    "incorrect": 0,
                    "tokens": 0,
                    "tokens_saved": 0,
                }
            by_category[r.category]["total"] += 1
            by_category[r.category]["tokens"] += r.total_tokens
            by_category[r.category]["tokens_saved"] += r.tokens_saved
            if r.is_correct is True:
                by_category[r.category]["correct"] += 1
            elif r.is_correct is False:
                by_category[r.category]["incorrect"] += 1

        return {
            "total_experiments": len(self.results),
            "by_model": by_model,
            "by_category": by_category,
        }

    def get_rule_summary(self) -> dict:
        """Get rule-specific summary."""
        rules = self.rule_engine.all_rules()
        total_creation_tokens = sum(r.creation_tokens for r in rules)
        total_tokens_saved = sum(r.tokens_saved for r in rules)
        total_uses = sum(r.times_used for r in rules)

        return {
            "total_rules": len(rules),
            "total_creation_tokens": total_creation_tokens,
            "total_tokens_saved": total_tokens_saved,
            "net_tokens_saved": total_tokens_saved - total_creation_tokens,
            "total_rule_uses": total_uses,
            "avg_tokens_saved_per_use": total_tokens_saved / total_uses
            if total_uses > 0
            else 0,
            "rules": [
                {
                    "rule_id": r.rule_id,
                    "name": r.name,
                    "creation_tokens": r.creation_tokens,
                    "times_used": r.times_used,
                    "tokens_saved": r.tokens_saved,
                    "correct_uses": r.correct_uses,
                    "incorrect_uses": r.incorrect_uses,
                    "accuracy": r.correct_uses / (r.correct_uses + r.incorrect_uses)
                    if (r.correct_uses + r.incorrect_uses) > 0
                    else None,
                }
                for r in rules
            ],
        }


def run_experiments_with_rules(runner: ExperimentRunner):
    """Run experiments including rule generation and usage."""

    # === PHASE 1: Generate rules from examples ===
    print("\n" + "=" * 60)
    print("PHASE 1: RULE GENERATION")
    print("=" * 60)

    # Rule 1: Simple math operations - use hardcoded implementation for reliability
    math_rule = runner.add_hardcoded_rule(
        name="Simple Arithmetic",
        category="math_simple",
        description="Evaluate simple arithmetic expressions with +, -, *, /",
        code='''import re

def apply_rule(input_text: str) -> str:
    """Safely evaluate simple arithmetic: 5 + 3, 12 * 4, etc."""
    # Clean the input
    expr = input_text.strip()

    # Validate: only allow digits, operators, spaces, and decimal points
    if not re.match(r'^[\\d\\s\\+\\-\\*\\/\\.\\(\\)]+$', expr):
        raise ValueError(f"Invalid characters in expression: {expr}")

    # Use ast.literal_eval approach for safety
    try:
        # Simple single-operation expressions
        result = eval(expr, {"__builtins__": {}}, {})
        # Return as int if whole number, else float
        if isinstance(result, float) and result.is_integer():
            return str(int(result))
        return str(result)
    except Exception as e:
        raise ValueError(f"Could not evaluate: {expr}") from e
''',
        examples=[
            ("2 + 2", "4"),
            ("10 * 5", "50"),
            ("100 / 4", "25"),
            ("20 - 8", "12"),
        ],
    )

    # Rule 2: Email validation
    email_rule = runner.generate_rule(
        name="Email Validation",
        category="validation",
        description="Check if an email address is valid",
        examples=[
            ("user@example.com", "valid"),
            ("john.doe@company.org", "valid"),
            ("userexample.com", "invalid"),
            ("user@.com", "invalid"),
            ("@example.com", "invalid"),
        ],
        model=runner.models[0],
    )

    # Rule 3: Grammar check (simple)
    runner.generate_rule(
        name="Basic Grammar Check",
        category="grammar",
        description="Check if a sentence has basic grammatical issues like missing verbs or wrong word order",
        examples=[
            ("The cat sat on the mat.", "grammatical"),
            ("They drank the pub.", "ungrammatical"),
            ("She runs quickly.", "grammatical"),
            ("The professor talked us.", "ungrammatical"),
        ],
        model=runner.models[0],
    )

    # === PHASE 2: Run experiments WITH rules ===
    print("\n" + "=" * 60)
    print("PHASE 2: EXPERIMENTS WITH RULES")
    print("=" * 60)

    # Test math rule
    math_tests = [
        ("5 + 3", "8"),
        ("12 * 4", "48"),
        ("81 / 9", "9"),
        ("25 - 7", "18"),
    ]
    for expr, expected in math_tests:
        runner.run_with_rule(
            name=f"Math (rule): {expr}",
            category="math_simple",
            input_text=expr,
            rule_id=math_rule.rule_id,
            expected=expected,
            baseline_tokens=50,
        )

    # Test email rule
    email_tests = [
        ("test@domain.com", "valid"),
        ("invalid.email", "invalid"),
        ("name@sub.domain.org", "valid"),
    ]
    for email, expected in email_tests:
        runner.run_with_rule(
            name=f"Email (rule): {email}",
            category="validation",
            input_text=email,
            rule_id=email_rule.rule_id,
            expected=expected,
            baseline_tokens=60,
        )

    # === PHASE 3: Run experiments WITHOUT rules (baseline comparison) ===
    print("\n" + "=" * 60)
    print("PHASE 3: EXPERIMENTS WITHOUT RULES (BASELINE)")
    print("=" * 60)

    # Same math tests with LLM
    for expr, expected in math_tests[:2]:  # Just a couple for comparison
        runner.run_across_models(
            name=f"Math (LLM): {expr}",
            category="math_simple_llm",
            messages=[
                {"role": "system", "content": "Answer with just the number."},
                {"role": "user", "content": f"What is {expr}?"},
            ],
            expected=expected,
        )

    # === PHASE 4: More complex experiments (always use LLM) ===
    print("\n" + "=" * 60)
    print("PHASE 4: COMPLEX EXPERIMENTS (LLM ONLY)")
    print("=" * 60)

    # GSM8K word problems
    gsm8k = load_dataset(DATA.GSM8K, "train")
    for i, item in enumerate(list(gsm8k)[:3]):
        expected = (
            item.answer.split("####")[-1].strip() if "####" in item.answer else None
        )
        runner.run_across_models(
            name=f"GSM8K #{i + 1}",
            category="math_word_problem",
            messages=[
                {
                    "role": "system",
                    "content": "Solve step by step. End with 'The answer is X'.",
                },
                {"role": "user", "content": item.question},
            ],
            expected=expected,
            metadata={"full_solution": item.answer},
        )

    # Reasoning
    runner.run_across_models(
        name="Logic Puzzle",
        category="reasoning",
        messages=[
            {"role": "system", "content": "Think step by step."},
            {
                "role": "user",
                "content": "If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Answer yes or no and explain.",
            },
        ],
        expected="no",
    )


def save_results(batch: ExperimentBatch, output_path: Path):
    """Save results to JSON."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    data = {
        "batch_id": batch.batch_id,
        "name": batch.name,
        "models": batch.models,
        "created_at": batch.created_at,
        "summary": batch.summary,
        "rule_summary": batch.rule_summary,
        "rules": [asdict(r) for r in batch.rules],
        "results": [
            {
                **{k: v for k, v in asdict(r).items() if k != "calls"},
                "calls": [asdict(c) for c in r.calls],
            }
            for r in batch.results
        ],
    }

    with open(output_path, "w") as f:
        json.dump(data, f, indent=2)

    print(f"\nResults saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Run LLM experiments with rule tracking"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        default=["llama3.2:3b", "qwen2.5-coder:32b", "llama3.3:70b"],
        help="Models to test",
    )
    parser.add_argument("--output", default=None, help="Output JSON file path")
    parser.add_argument("--quiet", action="store_true", help="Reduce verbosity")
    args = parser.parse_args()

    client = OllamaClient(model_name="llama3.2:3b")
    available = set(client.list_models())
    models = [m for m in args.models if m in available]

    if not models:
        print("Error: None of the requested models are available.")
        print(f"Available: {available}")
        sys.exit(1)

    print(f"{'=' * 60}")
    print("EXPERIMENT RUN WITH RULE TRACKING")
    print(f"{'=' * 60}")
    print(f"Models: {models}")
    print(f"{'=' * 60}")

    runner = ExperimentRunner(models=models, verbose=not args.quiet)
    run_experiments_with_rules(runner)

    batch_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    batch = ExperimentBatch(
        batch_id=batch_id,
        name="Multi-model Comparison with Rules",
        models=models,
        results=runner.results,
        rules=runner.rule_engine.all_rules(),
        summary=runner.get_summary(),
        rule_summary=runner.get_rule_summary(),
        created_at=datetime.now().isoformat(),
    )

    # Print summary
    print(f"\n{'=' * 60}")
    print("SUMMARY")
    print(f"{'=' * 60}")

    summary = batch.summary
    print(f"Total experiments: {summary['total_experiments']}")

    print("\nBy Execution Method:")
    for model, stats in summary["by_model"].items():
        acc = (
            stats["correct"] / (stats["correct"] + stats["incorrect"]) * 100
            if (stats["correct"] + stats["incorrect"]) > 0
            else 0
        )
        print(f"  {model}:")
        print(f"    Correct: {stats['correct']}/{stats['total']} ({acc:.1f}%)")
        print(f"    Tokens used: {stats['tokens']:,}")
        print(f"    Tokens saved: {stats['tokens_saved']:,}")

    print(f"\n{'=' * 60}")
    print("RULE STATISTICS")
    print(f"{'=' * 60}")

    rule_summary = batch.rule_summary
    print(f"Total rules created: {rule_summary['total_rules']}")
    print(f"Tokens spent on rule creation: {rule_summary['total_creation_tokens']:,}")
    print(f"Tokens saved by using rules: {rule_summary['total_tokens_saved']:,}")
    print(f"NET tokens saved: {rule_summary['net_tokens_saved']:,}")
    print(f"Total rule uses: {rule_summary['total_rule_uses']}")

    print("\nPer-rule breakdown:")
    for r in rule_summary["rules"]:
        acc = f"{r['accuracy'] * 100:.0f}%" if r["accuracy"] is not None else "N/A"
        print(f"  {r['name']} ({r['rule_id']}):")
        print(f"    Created with: {r['creation_tokens']} tokens")
        print(f"    Used: {r['times_used']} times")
        print(f"    Tokens saved: {r['tokens_saved']}")
        print(f"    Accuracy: {acc}")

    output_path = (
        Path(args.output)
        if args.output
        else Path(f"scratch/experiment_results/run_{batch_id}.json")
    )
    save_results(batch, output_path)

    print(f"\n{'=' * 60}")
    print("To explore results, open: scratch/explore_results.html")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
