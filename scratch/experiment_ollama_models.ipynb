{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Model Experiments\n",
    "\n",
    "This notebook runs Ollama models against custom and dataset examples:\n",
    "- With and without RLM (Recursive Language Model)\n",
    "- With different prefix prompts\n",
    "- Generates parseable rule files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from string import Template\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from self_distill.clients.ollama_client import OllamaClient\n",
    "from self_distill.datasets import DATA, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rule Output Format\n",
    "\n",
    "We define a JSON-based rule format that LLMs can generate and we can parse reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RuleExample:\n",
    "    input: str\n",
    "    output: str\n",
    "    matches: bool\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Rule:\n",
    "    rule_name: str\n",
    "    description: str\n",
    "    conditions: list[str]\n",
    "    action: str\n",
    "    examples: list[RuleExample]\n",
    "    confidence: float = 0.0\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"rule_name\": self.rule_name,\n",
    "            \"description\": self.description,\n",
    "            \"conditions\": self.conditions,\n",
    "            \"action\": self.action,\n",
    "            \"examples\": [asdict(e) for e in self.examples],\n",
    "            \"confidence\": self.confidence,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: dict) -> \"Rule\":\n",
    "        examples = [RuleExample(**e) for e in d.get(\"examples\", [])]\n",
    "        return cls(\n",
    "            rule_name=d[\"rule_name\"],\n",
    "            description=d[\"description\"],\n",
    "            conditions=d.get(\"conditions\", []),\n",
    "            action=d[\"action\"],\n",
    "            examples=examples,\n",
    "            confidence=d.get(\"confidence\", 0.0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rule_from_response(response: str) -> Rule | None:\n",
    "    \"\"\"Parse a Rule from LLM response. Handles JSON in markdown code blocks.\"\"\"\n",
    "    # Try to extract JSON from code blocks first\n",
    "    json_match = re.search(r\"```(?:json)?\\s*\\n?({[^`]+})\\s*\\n?```\", response, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(1)\n",
    "    else:\n",
    "        # Try to find raw JSON object\n",
    "        json_match = re.search(\n",
    "            r\"(\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\})\", response, re.DOTALL\n",
    "        )\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        return Rule.from_dict(data)\n",
    "    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "        print(f\"Parse error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_rules(rules: list[Rule], filepath: str | Path):\n",
    "    \"\"\"Save rules to a JSON file.\"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump([r.to_dict() for r in rules], f, indent=2)\n",
    "    print(f\"Saved {len(rules)} rules to {filepath}\")\n",
    "\n",
    "\n",
    "def load_rules(filepath: str | Path) -> list[Rule]:\n",
    "    \"\"\"Load rules from a JSON file.\"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    return [Rule.from_dict(d) for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates\n",
    "\n",
    "Different prompt strategies for rule generation. Uses `string.Template` with `$variable` syntax to avoid issues with JSON curly braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RULE_FORMAT_INSTRUCTION = \"\"\"\n",
    "Output your rule as a JSON object with this exact structure:\n",
    "```json\n",
    "{\n",
    "  \"rule_name\": \"descriptive_snake_case_name\",\n",
    "  \"description\": \"What this rule detects/validates\",\n",
    "  \"conditions\": [\"condition 1\", \"condition 2\"],\n",
    "  \"action\": \"What to do when rule matches\",\n",
    "  \"examples\": [\n",
    "    {\"input\": \"example input\", \"output\": \"expected output\", \"matches\": true}\n",
    "  ],\n",
    "  \"confidence\": 0.85\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "PROMPTS = {\n",
    "    \"direct\": {\n",
    "        \"system\": \"You are a rule extraction expert. Extract clear, testable rules from examples.\",\n",
    "        \"user\": Template(\n",
    "            \"Given these examples, create a rule that explains the pattern:\\n\\n$examples\\n\"\n",
    "            + RULE_FORMAT_INSTRUCTION\n",
    "        ),\n",
    "    },\n",
    "    \"chain_of_thought\": {\n",
    "        \"system\": \"You are a rule extraction expert. Think step by step to extract clear, testable rules.\",\n",
    "        \"user\": Template(\n",
    "            \"\"\"Given these examples, create a rule that explains the pattern.\n",
    "\n",
    "Examples:\n",
    "$examples\n",
    "\n",
    "Think step by step:\n",
    "1. What patterns do you notice in the positive examples?\n",
    "2. What patterns do you notice in the negative examples?\n",
    "3. What distinguishes positive from negative?\n",
    "4. Formulate a precise rule.\n",
    "\n",
    "\"\"\"\n",
    "            + RULE_FORMAT_INSTRUCTION\n",
    "        ),\n",
    "    },\n",
    "    \"few_shot\": {\n",
    "        \"system\": \"\"\"You are a rule extraction expert. Here's an example of good rule extraction:\n",
    "\n",
    "Input examples:\n",
    "- \"The cat sat on the mat\" -> valid\n",
    "- \"Cat the sat mat on the\" -> invalid\n",
    "\n",
    "Output:\n",
    "```json\n",
    "{\n",
    "  \"rule_name\": \"subject_verb_object_order\",\n",
    "  \"description\": \"English sentences follow Subject-Verb-Object word order\",\n",
    "  \"conditions\": [\"Subject appears before verb\", \"Verb appears before object\"],\n",
    "  \"action\": \"Mark as grammatically valid if SVO order is maintained\",\n",
    "  \"examples\": [\n",
    "    {\"input\": \"The cat sat on the mat\", \"output\": \"valid\", \"matches\": true},\n",
    "    {\"input\": \"Cat the sat mat on the\", \"output\": \"invalid\", \"matches\": false}\n",
    "  ],\n",
    "  \"confidence\": 0.9\n",
    "}\n",
    "```\n",
    "Now extract a rule from the user's examples.\"\"\",\n",
    "        \"user\": Template(\n",
    "            \"Extract a rule from these examples:\\n\\n$examples\\n\"\n",
    "            + RULE_FORMAT_INSTRUCTION\n",
    "        ),\n",
    "    },\n",
    "    \"rlm_decompose\": {\n",
    "        \"system\": \"\"\"You are an RLM (Recursive Language Model) agent. You solve complex problems by:\n",
    "1. Breaking them into sub-tasks\n",
    "2. Solving each sub-task\n",
    "3. Combining results\n",
    "\n",
    "For rule extraction:\n",
    "- Sub-task 1: Identify all positive examples and their common features\n",
    "- Sub-task 2: Identify all negative examples and their common features  \n",
    "- Sub-task 3: Find discriminating features (present in positive, absent in negative)\n",
    "- Sub-task 4: Formulate rule from discriminating features\n",
    "- Sub-task 5: Validate rule against all examples\"\"\",\n",
    "        \"user\": Template(\n",
    "            \"\"\"Apply RLM decomposition to extract a rule from these examples:\n",
    "\n",
    "$examples\n",
    "\n",
    "Show your work for each sub-task, then provide the final rule.\n",
    "\"\"\"\n",
    "            + RULE_FORMAT_INSTRUCTION\n",
    "        ),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    model: str\n",
    "    prompt_type: str\n",
    "    examples_used: str\n",
    "    raw_response: str\n",
    "    parsed_rule: Rule | None\n",
    "    usage: dict\n",
    "    timestamp: str\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"prompt_type\": self.prompt_type,\n",
    "            \"examples_used\": self.examples_used,\n",
    "            \"raw_response\": self.raw_response,\n",
    "            \"parsed_rule\": self.parsed_rule.to_dict() if self.parsed_rule else None,\n",
    "            \"usage\": self.usage,\n",
    "            \"timestamp\": self.timestamp,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    client: OllamaClient,\n",
    "    examples: str,\n",
    "    prompt_type: str = \"direct\",\n",
    "    model: str | None = None,\n",
    ") -> ExperimentResult:\n",
    "    \"\"\"Run a single rule extraction experiment.\"\"\"\n",
    "    prompt_template = PROMPTS[prompt_type]\n",
    "    user_content = prompt_template[\"user\"].substitute(examples=examples)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_template[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    response = client.completion(messages, model=model)\n",
    "    usage = client.get_last_usage()\n",
    "    parsed_rule = parse_rule_from_response(response)\n",
    "\n",
    "    return ExperimentResult(\n",
    "        model=model or client.model_name,\n",
    "        prompt_type=prompt_type,\n",
    "        examples_used=examples,\n",
    "        raw_response=response,\n",
    "        parsed_rule=parsed_rule,\n",
    "        usage=usage,\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "    )\n",
    "\n",
    "\n",
    "def run_comparison(\n",
    "    client: OllamaClient,\n",
    "    examples: str,\n",
    "    prompt_types: list[str] | None = None,\n",
    "    models: list[str] | None = None,\n",
    ") -> list[ExperimentResult]:\n",
    "    \"\"\"Run experiments across multiple prompt types and/or models.\"\"\"\n",
    "    prompt_types = prompt_types or list(PROMPTS.keys())\n",
    "    models = models or [client.model_name]\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        for prompt_type in prompt_types:\n",
    "            print(f\"Running: model={model}, prompt={prompt_type}\")\n",
    "            result = run_experiment(client, examples, prompt_type, model)\n",
    "            results.append(result)\n",
    "            print(f\"  -> Rule parsed: {result.parsed_rule is not None}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Ollama Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OllamaClient(model_name=\"llama3.2:3b\")\n",
    "\n",
    "print(\"Available models:\")\n",
    "for m in client.list_models():\n",
    "    print(f\"  - {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_EXAMPLES = \"\"\"\n",
    "Valid:\n",
    "- user@example.com\n",
    "- john.doe@company.org\n",
    "- test123@sub.domain.net\n",
    "\n",
    "Invalid:\n",
    "- userexample.com (missing @)\n",
    "- user@.com (missing domain name)\n",
    "- @example.com (missing local part)\n",
    "- user@example (missing TLD)\n",
    "\"\"\"\n",
    "\n",
    "PII_EXAMPLES = \"\"\"\n",
    "Contains PII (should be flagged):\n",
    "- \"My SSN is 123-45-6789\"\n",
    "- \"Call me at 555-123-4567\"\n",
    "- \"My credit card is 4111-1111-1111-1111\"\n",
    "\n",
    "No PII (safe):\n",
    "- \"The meeting is at 3pm\"\n",
    "- \"Order number: ABC123\"\n",
    "- \"The price is $99.99\"\n",
    "\"\"\"\n",
    "\n",
    "CODE_QUALITY_EXAMPLES = \"\"\"\n",
    "Good (follows conventions):\n",
    "- \"def calculate_total(items: list) -> float:\"\n",
    "- \"class UserService:\"\n",
    "- \"MAX_RETRY_COUNT = 3\"\n",
    "\n",
    "Bad (violates conventions):\n",
    "- \"def CalculateTotal(Items):\"  # wrong naming convention\n",
    "- \"class user_service:\"  # wrong class naming\n",
    "- \"maxRetryCount = 3\"  # should be SCREAMING_SNAKE for constants\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single experiment\n",
    "result = run_experiment(client, EMAIL_EXAMPLES, prompt_type=\"direct\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RAW RESPONSE:\")\n",
    "print(\"=\" * 60)\n",
    "print(result.raw_response)\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"PARSED RULE:\")\n",
    "print(\"=\" * 60)\n",
    "if result.parsed_rule:\n",
    "    print(json.dumps(result.parsed_rule.to_dict(), indent=2))\n",
    "else:\n",
    "    print(\"Failed to parse rule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prompt types on email examples\n",
    "email_results = run_comparison(\n",
    "    client, EMAIL_EXAMPLES, prompt_types=[\"direct\", \"chain_of_thought\", \"rlm_decompose\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison results\n",
    "for r in email_results:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Prompt: {r.prompt_type}\")\n",
    "    print(f\"Tokens: {r.usage}\")\n",
    "    if r.parsed_rule:\n",
    "        print(f\"Rule: {r.parsed_rule.rule_name}\")\n",
    "        print(f\"Conditions: {r.parsed_rule.conditions}\")\n",
    "        print(f\"Confidence: {r.parsed_rule.confidence}\")\n",
    "    else:\n",
    "        print(\"FAILED TO PARSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Examples (CoLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_train = load_dataset(DATA.COLA, \"train\", include_rule_id=True)\n",
    "\n",
    "print(f\"Total examples: {len(cola_train)}\")\n",
    "print(f\"Rule IDs: {cola_train.get_rule_ids()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_cola_examples(df, n: int = 5) -> str:\n",
    "    \"\"\"Format CoLA DataFrame as examples string.\n",
    "\n",
    "    Args:\n",
    "        df: Polars DataFrame with 'text' and 'label' columns\n",
    "        n: Number of examples per category\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    acceptable = df.filter(df[\"label\"] == 1).head(n)\n",
    "    unacceptable = df.filter(df[\"label\"] == 0).head(n)\n",
    "\n",
    "    lines.append(\"Grammatically acceptable:\")\n",
    "    for row in acceptable.iter_rows(named=True):\n",
    "        lines.append(f'- \"{row[\"text\"]}\"')\n",
    "\n",
    "    lines.append(\"\\nGrammatically unacceptable:\")\n",
    "    for row in unacceptable.iter_rows(named=True):\n",
    "        lines.append(f'- \"{row[\"text\"]}\"')\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples for a specific rule\n",
    "rule_ids = cola_train.get_rule_ids()\n",
    "sample_rule_id = rule_ids[0] if rule_ids else None\n",
    "\n",
    "if sample_rule_id:\n",
    "    rule_examples_df = cola_train.filter_by_rule(sample_rule_id)\n",
    "    print(f\"Rule ID: {sample_rule_id}\")\n",
    "    print(f\"Examples for this rule: {len(rule_examples_df)}\")\n",
    "\n",
    "    cola_example_str = format_cola_examples(rule_examples_df, n=3)\n",
    "    print(\"\\nFormatted examples:\")\n",
    "    print(cola_example_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rule extraction on CoLA examples\n",
    "if sample_rule_id:\n",
    "    cola_result = run_experiment(\n",
    "        client, cola_example_str, prompt_type=\"chain_of_thought\"\n",
    "    )\n",
    "\n",
    "    print(\"Response:\")\n",
    "    print(cola_result.raw_response[:1000])\n",
    "    print(\"\\n...\")\n",
    "\n",
    "    if cola_result.parsed_rule:\n",
    "        print(\"\\nExtracted Rule:\")\n",
    "        print(json.dumps(cola_result.parsed_rule.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Examples (GSM8K - Math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_train = load_dataset(DATA.GSM8K, \"train\")\n",
    "\n",
    "print(f\"Total examples: {len(gsm8k_train)}\")\n",
    "print(\"\\nSample question:\")\n",
    "print(gsm8k_train[0].question)\n",
    "print(\"\\nAnswer:\")\n",
    "print(gsm8k_train[0].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_gsm8k_for_pattern(items, n: int = 3) -> str:\n",
    "    \"\"\"Format GSM8K items to extract problem-solving patterns.\"\"\"\n",
    "    lines = [\"Math word problems with solutions:\\n\"]\n",
    "\n",
    "    for i, item in enumerate(items[:n], 1):\n",
    "        lines.append(f\"Problem {i}:\")\n",
    "        lines.append(item.question)\n",
    "        lines.append(f\"\\nSolution {i}:\")\n",
    "        lines.append(item.answer)\n",
    "        lines.append(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "gsm8k_example_str = format_gsm8k_for_pattern(list(gsm8k_train)[:3])\n",
    "print(gsm8k_example_str[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATH_PATTERN_PROMPT = {\n",
    "    \"system\": \"\"\"You are a math education expert. Extract problem-solving strategies and patterns.\n",
    "Focus on: problem types, key information extraction, solution strategies, common pitfalls.\"\"\",\n",
    "    \"user\": Template(\n",
    "        \"\"\"Analyze these math problems and extract a reusable problem-solving rule:\n",
    "\n",
    "$examples\n",
    "\n",
    "\"\"\"\n",
    "        + RULE_FORMAT_INSTRUCTION\n",
    "    ),\n",
    "}\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": MATH_PATTERN_PROMPT[\"system\"]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": MATH_PATTERN_PROMPT[\"user\"].substitute(examples=gsm8k_example_str),\n",
    "    },\n",
    "]\n",
    "\n",
    "math_response = client.completion(messages)\n",
    "math_rule = parse_rule_from_response(math_response)\n",
    "\n",
    "print(\"Math Pattern Rule:\")\n",
    "if math_rule:\n",
    "    print(json.dumps(math_rule.to_dict(), indent=2))\n",
    "else:\n",
    "    print(\"Failed to parse. Raw response:\")\n",
    "    print(math_response[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_TO_COMPARE = [\n",
    "    \"llama3.2:3b\",\n",
    "    # \"qwen2.5-coder:32b\",\n",
    "    # \"llama3.3:70b\",\n",
    "]\n",
    "\n",
    "available = set(client.list_models())\n",
    "models_to_test = [m for m in MODELS_TO_COMPARE if m in available]\n",
    "print(f\"Will test models: {models_to_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_test:\n",
    "    multi_model_results = run_comparison(\n",
    "        client,\n",
    "        PII_EXAMPLES,\n",
    "        prompt_types=[\"direct\", \"rlm_decompose\"],\n",
    "        models=models_to_test,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    for r in multi_model_results:\n",
    "        status = \"OK\" if r.parsed_rule else \"FAIL\"\n",
    "        print(f\"{r.model:20} | {r.prompt_type:20} | {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"rules\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_rules = []\n",
    "\n",
    "if \"email_results\" in dir():\n",
    "    for r in email_results:\n",
    "        if r.parsed_rule:\n",
    "            all_rules.append(r.parsed_rule)\n",
    "\n",
    "if \"cola_result\" in dir() and cola_result.parsed_rule:\n",
    "    all_rules.append(cola_result.parsed_rule)\n",
    "\n",
    "if \"math_rule\" in dir() and math_rule:\n",
    "    all_rules.append(math_rule)\n",
    "\n",
    "print(f\"Total rules extracted: {len(all_rules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_rules:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_rules(all_rules, OUTPUT_DIR / f\"extracted_rules_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment_results(results: list[ExperimentResult], filepath: str | Path):\n",
    "    filepath = Path(filepath)\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump([r.to_dict() for r in results], f, indent=2)\n",
    "    print(f\"Saved {len(results)} experiment results to {filepath}\")\n",
    "\n",
    "\n",
    "all_experiments = []\n",
    "if \"email_results\" in dir():\n",
    "    all_experiments.extend(email_results)\n",
    "if \"multi_model_results\" in dir():\n",
    "    all_experiments.extend(multi_model_results)\n",
    "\n",
    "if all_experiments:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_experiment_results(\n",
    "        all_experiments, OUTPUT_DIR / f\"experiments_{timestamp}.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Usage Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token Usage Summary:\")\n",
    "print(client.get_usage_summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
