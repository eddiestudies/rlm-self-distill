{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Experiments - Detailed LLM Call Inspection\n",
    "\n",
    "This notebook runs examples with detailed logging to inspect:\n",
    "- Tool calls and intermediate steps\n",
    "- LLM reasoning and responses\n",
    "- Answer correctness\n",
    "- Behavior validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from self_distill.clients.ollama_client import OllamaClient\n",
    "from self_distill.datasets import DATA, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verbose Client Wrapper\n",
    "\n",
    "Wrap the client to capture and display all calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CallRecord:\n",
    "    \"\"\"Record of a single LLM call.\"\"\"\n",
    "\n",
    "    call_id: int\n",
    "    model: str\n",
    "    messages: list[dict]\n",
    "    response: str\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentRun:\n",
    "    \"\"\"A complete experiment run with all calls.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    input_text: str\n",
    "    expected_output: str | None\n",
    "    calls: list[CallRecord] = field(default_factory=list)\n",
    "    final_response: str = \"\"\n",
    "    is_correct: bool | None = None\n",
    "    notes: str = \"\"\n",
    "\n",
    "\n",
    "class VerboseClient:\n",
    "    \"\"\"Wrapper that logs all LLM calls with full details.\"\"\"\n",
    "\n",
    "    def __init__(self, client: OllamaClient, verbose: bool = True):\n",
    "        self.client = client\n",
    "        self.verbose = verbose\n",
    "        self.call_history: list[CallRecord] = []\n",
    "        self.call_counter = 0\n",
    "\n",
    "    def completion(self, prompt: str | list[dict], model: str | None = None) -> str:\n",
    "        \"\"\"Make a completion call with logging.\"\"\"\n",
    "        self.call_counter += 1\n",
    "        model = model or self.client.model_name\n",
    "\n",
    "        # Normalize to messages format\n",
    "        if isinstance(prompt, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        else:\n",
    "            messages = prompt\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"CALL #{self.call_counter} | Model: {model}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            for msg in messages:\n",
    "                role = msg[\"role\"].upper()\n",
    "                content = msg[\"content\"]\n",
    "                print(f\"\\n[{role}]\")\n",
    "                print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "\n",
    "        # Make the actual call\n",
    "        response = self.client.completion(messages, model=model)\n",
    "        usage = self.client.get_last_usage()\n",
    "\n",
    "        # Extract token counts (usage is ModelUsageSummary)\n",
    "        prompt_tokens = usage.total_input_tokens\n",
    "        completion_tokens = usage.total_output_tokens\n",
    "\n",
    "        # Record the call\n",
    "        record = CallRecord(\n",
    "            call_id=self.call_counter,\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response=response,\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            completion_tokens=completion_tokens,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "        )\n",
    "        self.call_history.append(record)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\n[RESPONSE] ({completion_tokens} tokens)\")\n",
    "            print(response[:1000] + \"...\" if len(response) > 1000 else response)\n",
    "            print(f\"\\n{'─' * 60}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear call history for a new experiment.\"\"\"\n",
    "        self.call_history = []\n",
    "        self.call_counter = 0\n",
    "\n",
    "    def get_history(self) -> list[CallRecord]:\n",
    "        \"\"\"Get copy of call history.\"\"\"\n",
    "        return list(self.call_history)\n",
    "\n",
    "    def list_models(self):\n",
    "        return self.client.list_models()\n",
    "\n",
    "    def get_usage_summary(self):\n",
    "        return self.client.get_usage_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    \"\"\"Run and track experiments with detailed logging.\"\"\"\n",
    "\n",
    "    def __init__(self, client: VerboseClient):\n",
    "        self.client = client\n",
    "        self.experiments: list[ExperimentRun] = []\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        name: str,\n",
    "        messages: list[dict],\n",
    "        expected: str | None = None,\n",
    "        model: str | None = None,\n",
    "        correctness_fn: callable = None,\n",
    "    ) -> ExperimentRun:\n",
    "        \"\"\"Run a single experiment.\"\"\"\n",
    "        self.client.clear_history()\n",
    "\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# EXPERIMENT: {name}\")\n",
    "        print(f\"{'#' * 60}\")\n",
    "\n",
    "        # Extract input text for recording\n",
    "        input_text = \"\\n\".join(f\"[{m['role']}] {m['content']}\" for m in messages)\n",
    "\n",
    "        # Run the completion\n",
    "        response = self.client.completion(messages, model=model)\n",
    "\n",
    "        # Check correctness\n",
    "        is_correct = None\n",
    "        if expected is not None:\n",
    "            if correctness_fn:\n",
    "                is_correct = correctness_fn(response, expected)\n",
    "            else:\n",
    "                # Default: check if expected is in response\n",
    "                is_correct = expected.lower() in response.lower()\n",
    "\n",
    "        # Create experiment record\n",
    "        experiment = ExperimentRun(\n",
    "            name=name,\n",
    "            input_text=input_text,\n",
    "            expected_output=expected,\n",
    "            calls=self.client.get_history(),\n",
    "            final_response=response,\n",
    "            is_correct=is_correct,\n",
    "        )\n",
    "        self.experiments.append(experiment)\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\"EXPERIMENT SUMMARY\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"Total calls: {len(experiment.calls)}\")\n",
    "        if expected:\n",
    "            status = \"CORRECT\" if is_correct else \"INCORRECT\"\n",
    "            print(f\"Expected: {expected}\")\n",
    "            print(f\"Status: {status}\")\n",
    "\n",
    "        return experiment\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        batch_name: str,\n",
    "        examples: list[dict],\n",
    "        system_prompt: str | None = None,\n",
    "        model: str | None = None,\n",
    "    ) -> list[ExperimentRun]:\n",
    "        \"\"\"Run a batch of experiments.\n",
    "\n",
    "        Args:\n",
    "            batch_name: Name for this batch\n",
    "            examples: List of dicts with 'input' and optionally 'expected'\n",
    "            system_prompt: Optional system prompt to prepend\n",
    "            model: Model to use\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'*' * 60}\")\n",
    "        print(f\"* BATCH: {batch_name}\")\n",
    "        print(f\"* Examples: {len(examples)}\")\n",
    "        print(f\"{'*' * 60}\")\n",
    "\n",
    "        results = []\n",
    "        for i, ex in enumerate(examples, 1):\n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            messages.append({\"role\": \"user\", \"content\": ex[\"input\"]})\n",
    "\n",
    "            result = self.run(\n",
    "                name=f\"{batch_name} #{i}\",\n",
    "                messages=messages,\n",
    "                expected=ex.get(\"expected\"),\n",
    "                model=model,\n",
    "            )\n",
    "            results.append(result)\n",
    "\n",
    "        # Batch summary\n",
    "        correct = sum(1 for r in results if r.is_correct is True)\n",
    "        incorrect = sum(1 for r in results if r.is_correct is False)\n",
    "        unknown = sum(1 for r in results if r.is_correct is None)\n",
    "\n",
    "        print(f\"\\n{'*' * 60}\")\n",
    "        print(f\"* BATCH COMPLETE: {batch_name}\")\n",
    "        print(f\"* Correct: {correct} | Incorrect: {incorrect} | Unknown: {unknown}\")\n",
    "        print(f\"{'*' * 60}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def summary(self) -> dict:\n",
    "        \"\"\"Get summary of all experiments.\"\"\"\n",
    "        return {\n",
    "            \"total\": len(self.experiments),\n",
    "            \"correct\": sum(1 for e in self.experiments if e.is_correct is True),\n",
    "            \"incorrect\": sum(1 for e in self.experiments if e.is_correct is False),\n",
    "            \"unknown\": sum(1 for e in self.experiments if e.is_correct is None),\n",
    "            \"total_calls\": sum(len(e.calls) for e in self.experiments),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "base_client = OllamaClient(model_name=\"llama3.2:3b\")\n",
    "client = VerboseClient(base_client, verbose=True)\n",
    "runner = ExperimentRunner(client)\n",
    "\n",
    "print(\"Available models:\")\n",
    "for m in client.list_models():\n",
    "    print(f\"  - {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example: Simple Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple math examples\n",
    "math_examples = [\n",
    "    {\"input\": \"What is 2 + 2?\", \"expected\": \"4\"},\n",
    "    {\"input\": \"What is 15 * 3?\", \"expected\": \"45\"},\n",
    "    {\"input\": \"What is 100 / 4?\", \"expected\": \"25\"},\n",
    "]\n",
    "\n",
    "runner.run_batch(\n",
    "    \"Simple Math\",\n",
    "    math_examples,\n",
    "    system_prompt=\"You are a math assistant. Answer with just the number.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example: Rule Extraction (Email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_PROMPT = \"\"\"Given these examples, what rule determines if an email is valid?\n",
    "\n",
    "Valid:\n",
    "- user@example.com\n",
    "- john.doe@company.org\n",
    "\n",
    "Invalid:\n",
    "- userexample.com\n",
    "- user@.com\n",
    "\n",
    "Explain the rule briefly.\"\"\"\n",
    "\n",
    "result = runner.run(\n",
    "    name=\"Email Rule Extraction\", messages=[{\"role\": \"user\", \"content\": EMAIL_PROMPT}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example: GSM8K Word Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K\n",
    "gsm8k = load_dataset(DATA.GSM8K, \"train\")\n",
    "\n",
    "# Take first 3 examples\n",
    "gsm_examples = []\n",
    "for item in list(gsm8k)[:3]:\n",
    "    # Extract the final numerical answer\n",
    "    answer_line = (\n",
    "        item.answer.split(\"####\")[-1].strip()\n",
    "        if \"####\" in item.answer\n",
    "        else item.answer.split()[-1]\n",
    "    )\n",
    "    gsm_examples.append({\"input\": item.question, \"expected\": answer_line})\n",
    "\n",
    "print(\"GSM8K Examples:\")\n",
    "for i, ex in enumerate(gsm_examples, 1):\n",
    "    print(f\"\\n{i}. {ex['input'][:100]}...\")\n",
    "    print(f\"   Expected: {ex['expected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GSM8K batch\n",
    "runner.run_batch(\n",
    "    \"GSM8K Math\",\n",
    "    gsm_examples,\n",
    "    system_prompt=\"Solve this math problem step by step. End with 'The answer is X' where X is a number.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example: CoLA Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CoLA\n",
    "cola = load_dataset(DATA.COLA, \"train\")\n",
    "\n",
    "# Take a few examples\n",
    "cola_examples = []\n",
    "for row in cola.data.head(6).iter_rows(named=True):\n",
    "    label = \"grammatical\" if row[\"label\"] == 1 else \"ungrammatical\"\n",
    "    cola_examples.append(\n",
    "        {\n",
    "            \"input\": f'Is this sentence grammatically correct? \"{row[\"text\"]}\"',\n",
    "            \"expected\": label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"CoLA Examples:\")\n",
    "for ex in cola_examples:\n",
    "    print(f\"  {ex['input'][:60]}... -> {ex['expected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CoLA batch\n",
    "runner.run_batch(\n",
    "    \"CoLA Grammar\",\n",
    "    cola_examples,\n",
    "    system_prompt=\"You are a grammar expert. Answer with 'grammatical' or 'ungrammatical' only.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test same prompt with different models\n",
    "TEST_PROMPT = \"What is the capital of France? Answer in one word.\"\n",
    "\n",
    "models_to_test = [\"llama3.2:3b\"]  # Add more: \"qwen2.5-coder:32b\", \"llama3.3:70b\"\n",
    "\n",
    "for model in models_to_test:\n",
    "    if model in client.list_models():\n",
    "        runner.run(\n",
    "            name=f\"Capital Test ({model})\",\n",
    "            messages=[{\"role\": \"user\", \"content\": TEST_PROMPT}],\n",
    "            expected=\"Paris\",\n",
    "            model=model,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = runner.summary()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total experiments: {summary['total']}\")\n",
    "print(f\"Correct: {summary['correct']}\")\n",
    "print(f\"Incorrect: {summary['incorrect']}\")\n",
    "print(f\"Unknown: {summary['unknown']}\")\n",
    "print(f\"Total LLM calls: {summary['total_calls']}\")\n",
    "\n",
    "if summary[\"total\"] > 0:\n",
    "    known = summary[\"correct\"] + summary[\"incorrect\"]\n",
    "    if known > 0:\n",
    "        accuracy = summary[\"correct\"] / known * 100\n",
    "        print(f\"Accuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inspect Specific Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_experiment(exp: ExperimentRun):\n",
    "    \"\"\"Display detailed view of an experiment.\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"EXPERIMENT: {exp.name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"\\nCalls made: {len(exp.calls)}\")\n",
    "    print(f\"Correct: {exp.is_correct}\")\n",
    "    print(f\"Expected: {exp.expected_output}\")\n",
    "\n",
    "    for call in exp.calls:\n",
    "        print(f\"\\n{'─' * 40}\")\n",
    "        print(f\"Call #{call.call_id} | Model: {call.model}\")\n",
    "        print(f\"Tokens: {call.prompt_tokens} in / {call.completion_tokens} out\")\n",
    "        print(\"\\nMESSAGES:\")\n",
    "        for msg in call.messages:\n",
    "            print(\n",
    "                f\"  [{msg['role']}]: {msg['content'][:200]}...\"\n",
    "                if len(msg[\"content\"]) > 200\n",
    "                else f\"  [{msg['role']}]: {msg['content']}\"\n",
    "            )\n",
    "        print(\"\\nRESPONSE:\")\n",
    "        print(\n",
    "            f\"  {call.response[:500]}...\"\n",
    "            if len(call.response) > 500\n",
    "            else f\"  {call.response}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Inspect last experiment\n",
    "if runner.experiments:\n",
    "    inspect_experiment(runner.experiments[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments for selection\n",
    "print(\"All experiments:\")\n",
    "for i, exp in enumerate(runner.experiments):\n",
    "    status = \"\" if exp.is_correct is None else (\" OK\" if exp.is_correct else \" WRONG\")\n",
    "    print(f\"  {i}: {exp.name}{status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a specific experiment by index\n",
    "# Change the index to inspect different experiments\n",
    "idx = 0\n",
    "if idx < len(runner.experiments):\n",
    "    inspect_experiment(runner.experiments[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_experiments(experiments: list[ExperimentRun], filepath: str):\n",
    "    \"\"\"Export experiments to JSON.\"\"\"\n",
    "    data = []\n",
    "    for exp in experiments:\n",
    "        data.append(\n",
    "            {\n",
    "                \"name\": exp.name,\n",
    "                \"input\": exp.input_text,\n",
    "                \"expected\": exp.expected_output,\n",
    "                \"response\": exp.final_response,\n",
    "                \"is_correct\": exp.is_correct,\n",
    "                \"calls\": [\n",
    "                    {\n",
    "                        \"call_id\": c.call_id,\n",
    "                        \"model\": c.model,\n",
    "                        \"messages\": c.messages,\n",
    "                        \"response\": c.response,\n",
    "                        \"prompt_tokens\": c.prompt_tokens,\n",
    "                        \"completion_tokens\": c.completion_tokens,\n",
    "                    }\n",
    "                    for c in exp.calls\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Exported {len(experiments)} experiments to {filepath}\")\n",
    "\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "export_experiments(runner.experiments, f\"debug_runs/experiments_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Custom Experiment\n",
    "\n",
    "Use this cell to run your own custom experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom experiment - modify as needed\n",
    "custom_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Your prompt here\"},\n",
    "]\n",
    "\n",
    "# result = runner.run(\n",
    "#     name=\"Custom Test\",\n",
    "#     messages=custom_messages,\n",
    "#     expected=None,  # Set expected value if you want to check correctness\n",
    "#     model=\"llama3.2:3b\"  # Or try \"qwen2.5-coder:32b\", \"llama3.3:70b\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}