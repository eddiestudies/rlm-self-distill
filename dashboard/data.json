{
  "generated_at": "2026-01-13T23:45:23.719339",
  "experiments": [
    {
      "id": "exp001",
      "name": "Experiment 001: Recursive Tool Creation for Efficiency and Safety",
      "description": "This experiment tests a recursive language model that:\n1. Creates preprocessing tools for safety issues\n2. Creates efficiency tools for repeated task patterns\n3. Tracks tool creation and usage over time\n4. Compares against baseline (no tools/rules)\n\nRuns on mixed CoLA (grammaticality) and PII (detection) datasets.",
      "architecture": "",
      "source_file": "exp001_recursive_tool_creation.py",
      "dataset_info": "CoLA (Grammar), PII Detection",
      "runs": [
        {
          "experiment_id": "exp001",
          "run_id": "exp001_20260112_180830",
          "timestamp": "2026-01-12T18:08:30",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp001_20260112_180830",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp001",
          "run_id": "exp001_20260112_180858",
          "timestamp": "2026-01-12T18:08:58",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp001_20260112_180858",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": [
            "reports/experiment_report.pdf",
            "results/all_results.json",
            "results/baseline_results.json",
            "results/direct_lm_results.json",
            "results/throwaway_results.json",
            "results/with_tools_results.json",
            "tools/efficiency/Corrector.json",
            "tools/efficiency/Corrector_implementation.txt",
            "tools/efficiency/Pattern-Driven_PII_Detector.json",
            "tools/efficiency/Pattern-Driven_PII_Detector_implementation.txt",
            "tools/safety/Advanced_PII_Detector.json",
            "tools/safety/Advanced_PII_Detector_implementation.txt",
            "tools/safety/Grammar_Checker.json",
            "tools/safety/Grammar_Checker_implementation.txt",
            "tools/safety/PII_Pattern_Detector.json",
            "tools/safety/PII_Pattern_Detector_implementation.txt",
            "tools/safety/Subject-Verb_Agreement_Checker.json",
            "tools/safety/Subject-Verb_Agreement_Checker_implementation.txt"
          ]
        }
      ]
    },
    {
      "id": "exp002",
      "name": "Experiment 002: Scaling Analysis for Recursive Tool Creation",
      "description": "This experiment tests how tool creation costs amortize as dataset size increases.\nRuns iteratively at 10x, 100x, 1000x scale to find the break-even point.\n\nGenerates:\n- PDF report after each batch\n- Joint summary PDF at the end",
      "architecture": "",
      "source_file": "exp002_scaling_analysis.py",
      "dataset_info": "CoLA (Grammar), PII Detection",
      "runs": [
        {
          "experiment_id": "exp002",
          "run_id": "exp002_20260112_181528",
          "timestamp": "2026-01-12T18:15:28",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp002_20260112_181528",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": [
            "all_results.json",
            "report_joint_summary.pdf",
            "report_scale_10x.pdf",
            "report_scale_1x.pdf"
          ]
        }
      ]
    },
    {
      "id": "exp003",
      "name": "Experiment 003: Executable Python Tools",
      "description": "This experiment creates ACTUAL executable Python tools that:\n1. Get saved to a local tools directory\n2. Can be loaded and executed\n3. Have a classifier tool that routes tasks to appropriate tools\n4. Fall back to LLM if tool execution fails",
      "architecture": "Architecture:\n- tools/classifiers/ - Tools that classify/route tasks\n- tools/grammar/ - Grammar checking tools\n- tools/pii/ - PII detection tools\n- Tool registry that loads and executes tools\n- LLM fallback when tools fail or don't exist",
      "source_file": "exp003_executable_tools.py",
      "dataset_info": "CoLA (Grammar), PII Detection",
      "runs": [
        {
          "experiment_id": "exp003",
          "run_id": "exp003_20260112_183118",
          "timestamp": "2026-01-12T18:31:18",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp003_20260112_183118",
          "results": {
            "metadata": {
              "model": "llama3.2:3b",
              "timestamp": "20260112_183118"
            },
            "baseline": {
              "total_tokens": 6435,
              "results_count": 65
            },
            "with_tools": {
              "total_tokens": 4229,
              "tool_tokens": 4229,
              "llm_fallback_tokens": 0,
              "tool_executions": 80,
              "tool_failures": 15,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "task_classifier",
                "type": "classifiers",
                "description": "Classifies text as grammar or PII task",
                "code": "import re\n\ndef run(text: str) -> str:\n    \"\"\"Classifies text into grammar or PII detection task\"\"\"\n    \n    # Initialize flag variables\n    pii = False\n    \n    # Check for email patterns\n    if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text):\n        pii = True\n    \n    # Check for phone patterns\n    elif re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text) or \\\n         re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text):\n        pii = True\n    \n    # Check for IP patterns\n    elif re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text):\n        pii = True\n    \n    # If no PII patterns found, assume grammar task\n    if not pii:\n        return \"grammar\"\n    \n    # Otherwise, return \"pii\" with matching pattern details\n    else:\n        pii_type = \"\"\n        pii_details = []\n        \n        if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text):\n            pii_type = \"email\"\n        elif re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text):\n            pii_type = \"ssn\"\n        elif re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text) or \\\n             re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text):\n            pii_type = \"phone\"\n        elif re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text):\n            pii_type = \"ip\"\n        \n        if pii_type == \"email\":\n            pii_details.append({\"type\": pii_type, \"text\": re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text).group(), \n                                \"start\": re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text).start(), \n                                \"end\": re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text).end()})\n        elif pii_type == \"ssn\":\n            pii_details.append({\"type\": pii_type, \"text\": re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text).group(), \n                                \"start\": re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text).start(), \n                                \"end\": re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text).end()})\n        elif pii_type == \"phone\":\n            pii_details.append({\"type\": pii_type, \"text\": re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text).group(), \n                                \"start\": re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text).start(), \n                                \"end\": re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text).end()})\n        elif pii_type == \"ip\":\n            pii_details.append({\"type\": pii_type, \"text\": re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text).group(), \n                                \"start\": re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text).start(), \n                                \"end\": re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text).end()})\n        \n        return \"pii\", pii_details\n\nif __name__ == \"__main__\":\n    print(run(\"Hello, my email is example@example.com\"))\n    print(run(\"(123) 456-7890 is a valid phone number\"))\n    print(run(\"192.168.1.1 is an IP address\"))",
                "file_path": "experiment_outputs/exp003_20260112_183118/tools/classifiers/task_classifier.py",
                "executions": 65,
                "failures": 0
              },
              {
                "name": "basic_grammar",
                "type": "grammar",
                "description": "Checks basic sentence structure",
                "code": "import re\n\ndef run(text: str) -> dict:\n    \"\"\"\n    Checks for basic sentence structure issues.\n\n    Args:\n        text (str): Input text to check.\n\n    Returns:\n        dict: {\"acceptable\": bool, \"reason\": str}\n    \"\"\"\n\n    # Check for subject-verb agreement error\n    if re.search(r'(singular|ies)$', text.lower()) and not re.search(r's', text.lower()):\n        return {\"acceptable\": False, \"reason\": \"Subject-verb agreement error. Use plural form for 's' ending words.\"}\n\n    # Check for sentence structure issue\n    if any(word.istitle() for word in text.split()):\n        return {\"acceptable\": False, \"reason\": \"Sentence structure issue. Ensure subject and verb are not capitalized.\"}\n\n    # Check for common errors (their/there/they're)\n    errors = []\n    words = text.split()\n    for i in range(len(words) - 1):\n        if re.match(r'(their|there|they\\'re)', words[i], re.IGNORECASE) and re.match(r'(s|is)$', words[i+1]):\n            errors.append(f\"Use '{words[i]} s' instead of '{words[i]} {words[i+1]}'\")\n    if errors:\n        return {\"acceptable\": False, \"reason\": f\"Common error: Use correct forms for 'their/there/they\\'re'. {', '.join(errors)}\"}\n\n    # Check for common errors (its/it's)\n    errors = []\n    words = text.split()\n    for i in range(len(words) - 1):\n        if re.match(r'(is|was)$', words[i]) and re.match(r'its$', words[i+1]):\n            errors.append(f\"Use 'it is' instead of 'it {words[i]} its'.\")\n    if errors:\n        return {\"acceptable\": False, \"reason\": f\"Common error: Use correct forms for 'its/it's'. {', '.join(errors)}\"}\n\n    # If no issues found\n    return {\"acceptable\": True, \"reason\": \"\"}\n\n# Example usage:\nprint(run(\"The cat chases its tail.\"))  # Acceptable\nprint(run(\"The cats chases their tails.\"))  # Not acceptable (subject-verb agreement error)",
                "file_path": "experiment_outputs/exp003_20260112_183118/tools/grammar/basic_grammar.py",
                "executions": 60,
                "failures": 0
              },
              {
                "name": "email_detector",
                "type": "pii",
                "description": "Detects email addresses",
                "code": "import re\n\nclass PIIDetector:\n    def __init__(self):\n        self.email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n\n    def run(self, text: str) -> list[dict]:\n        try:\n            emails = re.findall(self.email_pattern, text)\n            return [{\"type\": \"EMAIL\", \"text\": email, \"start\": self._get_start(text, email), \"end\": self._get_end(text, email)} for email in emails]\n        except Exception as e:\n            return {\"error\": str(e)}\n\n    def _get_start(self, text: str, email: str) -> int:\n        match = re.search(r'\\b.*' + re.escape(email) + r'.*\\b', text)\n        if match:\n            return match.start()\n        raise ValueError(\"No start position found\")\n\n    def _get_end(self, text: str, email: str) -> int:\n        match = re.search(r'.*.' + re.escape(email) + r'[^\\\\]*$', text)\n        if match:\n            return match.end() - 1\n        raise ValueError(\"No end position found\")\n\n\ndef main():\n    detector = PIIDetector()\n    print(detector.run(\"Hello, my email is john.doe@example.com.\"))\n\n\nif __name__ == \"__main__\":\n    main()",
                "file_path": "experiment_outputs/exp003_20260112_183118/tools/pii/email_detector.py",
                "executions": 5,
                "failures": 0
              },
              {
                "name": "phone_detector",
                "type": "pii",
                "description": "Detects phone numbers",
                "code": "import re\n\nclass PII_Detector:\n    def run(self, text: str) -> list[dict]:\n        try:\n            pattern = re.compile(r'\\b\\d{3}[-\\s]??\\d{3}[-\\s]??\\d{4}\\b', re.IGNORECASE)\n            matches = pattern.findall(text)\n\n            result = []\n            for match in matches:\n                start = text.find(match)\n                end = start + len(match)\n                result.append({\"type\": \"PHONE\", \"text\": match, \"start\": start, \"end\": end})\n\n            return result\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return []\n\ndef main():\n    detector = PII_Detector()\n    text = input(\"Enter the text to search for PHONE numbers: \")\n    matches = detector.run(text)\n    if matches:\n        for match in matches:\n            print(f\"PHONE number found at position {match['start']}-{match['end']}: {match['text']}\")\n    else:\n        print(\"No PHONE numbers found.\")\n\nif __name__ == \"__main__\":\n    main()",
                "file_path": "experiment_outputs/exp003_20260112_183118/tools/pii/phone_detector.py",
                "executions": 5,
                "failures": 0
              },
              {
                "name": "ssn_detector",
                "type": "pii",
                "description": "Detects Social Security Numbers",
                "code": "import re\n\ndef run(text: str) -> list:\n    \"\"\"\n    Detects SSNs in a given text.\n\n    Args:\n        text (str): Input text to search for SSNs.\n\n    Returns:\n        list: A list of dictionaries containing SSN detection results.\n    \"\"\"\n\n    # Define the regex pattern for SSN\n    ssn_pattern = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')\n\n    try:\n        # Find all instances of SSN in the text\n        ssns = ssn_pattern.findall(text)\n\n        # Create a list to store the detection results\n        results = []\n\n        for match in ssns:\n            start = 0\n            for i, char in enumerate(match):\n                if char not in '0123456789-':\n                    break\n                else:\n                    start += 1\n\n            end = len(match)\n            results.append({\n                \"type\": \"SSN\",\n                \"text\": match,\n                \"start\": start,\n                \"end\": end\n            })\n\n        return results\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return []\n\n# Example usage\ntext = \"My SSN is 123-45-6789 and my friend's SSN is 987-65-4321\"\nprint(run(text))",
                "file_path": "experiment_outputs/exp003_20260112_183118/tools/pii/ssn_detector.py",
                "executions": 5,
                "failures": 0
              },
              {
                "name": "ip_detector",
                "type": "pii",
                "description": "Detects IP addresses",
                "code": "import re\n\nclass PIIDetector:\n    def __init__(self):\n        self.regex = re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b')\n\n    def run(self, text: str) -> list[dict]:\n        matches = self.regex.findall(text)\n        \n        if not matches:\n            return []\n\n        result = []\n        for match in matches:\n            result.append({\n                \"type\": \"IP_ADDRESS\",\n                \"text\": match,\n                \"start\": re.finditer(match, text).next().span()[0],\n                \"end\": re.finditer(match, text).next().span()[1]\n            })\n\n        return result\n\n\ndef main():\n    detector = PIIDetector()\n    text = \"Your input text here\"\n    try:\n        result = detector.run(text)\n        for item in result:\n            print(f\"IP_ADDRESS: {item['text']} at ({item['start']}, {item['end']})\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()",
                "file_path": "experiment_outputs/exp003_20260112_183118/tools/pii/ip_detector.py",
                "executions": 5,
                "failures": 0
              }
            ]
          },
          "tools": [
            {
              "name": "basic_grammar",
              "category": "grammar",
              "code": "import re\n\ndef run(text: str) -> dict:\n    \"\"\"\n    Checks for basic sentence structure issues.\n\n    Args:\n        text (str): Input text to check.\n\n    Returns:\n        dict: {\"acceptable\": bool, \"reason\": str}\n    \"\"\"\n\n    # Check for subject-verb agreement error\n    if re.search(r'(singular|ies)$', text.lower()) and not re.search(r's', text.lower()):\n        return {\"acceptable\": False, \"reason\": \"Subject-verb agreement error. Use plural form for 's' ending words.\"}\n\n    # Check for sentence structure issue\n    if any(word.istitle() for word in text.split()):\n        return {\"acceptable\": False, \"reason\": \"Sentence structure issue. Ensure subject and verb are not capitalized.\"}\n\n    # Check for common errors (their/there/they're)\n    errors = []\n    words = text.split()\n    for i in range(len(words) - 1):\n        if re.match(r'(their|there|they\\'re)', words[i], re.IGNORECASE) and re.match(r'(s|is)$', words[i+1]):\n            errors.append(f\"Use '{words[i]} s' instead of '{words[i]} {words[i+1]}'\")\n    if errors:\n        return {\"acceptable\": False, \"reason\": f\"Common error: Use correct forms for 'their/there/they\\'re'. {', '.join(errors)}\"}\n\n    # Check for common errors (its/it's)\n    errors = []\n    words = text.split()\n    for i in range(len(words) - 1):\n        if re.match(r'(is|was)$', words[i]) and re.match(r'its$', words[i+1]):\n            errors.append(f\"Use 'it is' instead of 'it {words[i]} its'.\")\n    if errors:\n        return {\"acceptable\": False, \"reason\": f\"Common error: Use correct forms for 'its/it's'. {', '.join(errors)}\"}\n\n    # If no issues found\n    return {\"acceptable\": True, \"reason\": \"\"}\n\n# Example usage:\nprint(run(\"The cat chases its tail.\"))  # Acceptable\nprint(run(\"The cats chases their tails.\"))  # Not acceptable (subject-verb agreement error)",
              "path": "tools/grammar/basic_grammar.py"
            },
            {
              "name": "email_detector",
              "category": "pii",
              "code": "import re\n\nclass PIIDetector:\n    def __init__(self):\n        self.email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n\n    def run(self, text: str) -> list[dict]:\n        try:\n            emails = re.findall(self.email_pattern, text)\n            return [{\"type\": \"EMAIL\", \"text\": email, \"start\": self._get_start(text, email), \"end\": self._get_end(text, email)} for email in emails]\n        except Exception as e:\n            return {\"error\": str(e)}\n\n    def _get_start(self, text: str, email: str) -> int:\n        match = re.search(r'\\b.*' + re.escape(email) + r'.*\\b', text)\n        if match:\n            return match.start()\n        raise ValueError(\"No start position found\")\n\n    def _get_end(self, text: str, email: str) -> int:\n        match = re.search(r'.*.' + re.escape(email) + r'[^\\\\]*$', text)\n        if match:\n            return match.end() - 1\n        raise ValueError(\"No end position found\")\n\n\ndef main():\n    detector = PIIDetector()\n    print(detector.run(\"Hello, my email is john.doe@example.com.\"))\n\n\nif __name__ == \"__main__\":\n    main()",
              "path": "tools/pii/email_detector.py"
            },
            {
              "name": "phone_detector",
              "category": "pii",
              "code": "import re\n\nclass PII_Detector:\n    def run(self, text: str) -> list[dict]:\n        try:\n            pattern = re.compile(r'\\b\\d{3}[-\\s]??\\d{3}[-\\s]??\\d{4}\\b', re.IGNORECASE)\n            matches = pattern.findall(text)\n\n            result = []\n            for match in matches:\n                start = text.find(match)\n                end = start + len(match)\n                result.append({\"type\": \"PHONE\", \"text\": match, \"start\": start, \"end\": end})\n\n            return result\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return []\n\ndef main():\n    detector = PII_Detector()\n    text = input(\"Enter the text to search for PHONE numbers: \")\n    matches = detector.run(text)\n    if matches:\n        for match in matches:\n            print(f\"PHONE number found at position {match['start']}-{match['end']}: {match['text']}\")\n    else:\n        print(\"No PHONE numbers found.\")\n\nif __name__ == \"__main__\":\n    main()",
              "path": "tools/pii/phone_detector.py"
            },
            {
              "name": "ssn_detector",
              "category": "pii",
              "code": "import re\n\ndef run(text: str) -> list:\n    \"\"\"\n    Detects SSNs in a given text.\n\n    Args:\n        text (str): Input text to search for SSNs.\n\n    Returns:\n        list: A list of dictionaries containing SSN detection results.\n    \"\"\"\n\n    # Define the regex pattern for SSN\n    ssn_pattern = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')\n\n    try:\n        # Find all instances of SSN in the text\n        ssns = ssn_pattern.findall(text)\n\n        # Create a list to store the detection results\n        results = []\n\n        for match in ssns:\n            start = 0\n            for i, char in enumerate(match):\n                if char not in '0123456789-':\n                    break\n                else:\n                    start += 1\n\n            end = len(match)\n            results.append({\n                \"type\": \"SSN\",\n                \"text\": match,\n                \"start\": start,\n                \"end\": end\n            })\n\n        return results\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return []\n\n# Example usage\ntext = \"My SSN is 123-45-6789 and my friend's SSN is 987-65-4321\"\nprint(run(text))",
              "path": "tools/pii/ssn_detector.py"
            },
            {
              "name": "ip_detector",
              "category": "pii",
              "code": "import re\n\nclass PIIDetector:\n    def __init__(self):\n        self.regex = re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b')\n\n    def run(self, text: str) -> list[dict]:\n        matches = self.regex.findall(text)\n        \n        if not matches:\n            return []\n\n        result = []\n        for match in matches:\n            result.append({\n                \"type\": \"IP_ADDRESS\",\n                \"text\": match,\n                \"start\": re.finditer(match, text).next().span()[0],\n                \"end\": re.finditer(match, text).next().span()[1]\n            })\n\n        return result\n\n\ndef main():\n    detector = PIIDetector()\n    text = \"Your input text here\"\n    try:\n        result = detector.run(text)\n        for item in result:\n            print(f\"IP_ADDRESS: {item['text']} at ({item['start']}, {item['end']})\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()",
              "path": "tools/pii/ip_detector.py"
            },
            {
              "name": "task_classifier",
              "category": "classifiers",
              "code": "import re\n\ndef run(text: str) -> str:\n    \"\"\"Classifies text into grammar or PII detection task\"\"\"\n    \n    # Initialize flag variables\n    pii = False\n    \n    # Check for email patterns\n    if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text):\n        pii = True\n    \n    # Check for phone patterns\n    elif re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text) or \\\n         re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text):\n        pii = True\n    \n    # Check for IP patterns\n    elif re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text):\n        pii = True\n    \n    # If no PII patterns found, assume grammar task\n    if not pii:\n        return \"grammar\"\n    \n    # Otherwise, return \"pii\" with matching pattern details\n    else:\n        pii_type = \"\"\n        pii_details = []\n        \n        if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text):\n            pii_type = \"email\"\n        elif re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text):\n            pii_type = \"ssn\"\n        elif re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text) or \\\n             re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text):\n            pii_type = \"phone\"\n        elif re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text):\n            pii_type = \"ip\"\n        \n        if pii_type == \"email\":\n            pii_details.append({\"type\": pii_type, \"text\": re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text).group(), \n                                \"start\": re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text).start(), \n                                \"end\": re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text).end()})\n        elif pii_type == \"ssn\":\n            pii_details.append({\"type\": pii_type, \"text\": re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text).group(), \n                                \"start\": re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text).start(), \n                                \"end\": re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text).end()})\n        elif pii_type == \"phone\":\n            pii_details.append({\"type\": pii_type, \"text\": re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text).group(), \n                                \"start\": re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text).start(), \n                                \"end\": re.search(r'\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})', text).end()})\n        elif pii_type == \"ip\":\n            pii_details.append({\"type\": pii_type, \"text\": re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text).group(), \n                                \"start\": re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text).start(), \n                                \"end\": re.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$', text).end()})\n        \n        return \"pii\", pii_details\n\nif __name__ == \"__main__\":\n    print(run(\"Hello, my email is example@example.com\"))\n    print(run(\"(123) 456-7890 is a valid phone number\"))\n    print(run(\"192.168.1.1 is an IP address\"))",
              "path": "tools/classifiers/task_classifier.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/classifiers/__pycache__/task_classifier.cpython-314.pyc",
            "tools/classifiers/task_classifier.py",
            "tools/grammar/__pycache__/basic_grammar.cpython-314.pyc",
            "tools/grammar/basic_grammar.py",
            "tools/pii/__pycache__/email_detector.cpython-314.pyc",
            "tools/pii/__pycache__/ip_detector.cpython-314.pyc",
            "tools/pii/__pycache__/phone_detector.cpython-314.pyc",
            "tools/pii/__pycache__/ssn_detector.cpython-314.pyc",
            "tools/pii/email_detector.py",
            "tools/pii/ip_detector.py",
            "tools/pii/phone_detector.py",
            "tools/pii/ssn_detector.py"
          ]
        }
      ]
    },
    {
      "id": "exp004",
      "name": "Experiment 004: Safety Macro Pipeline",
      "description": "",
      "architecture": "Architecture:\n1. Safety Macro runs FIRST on all inputs\n   - Detects if input contains potentially sensitive data (PII)\n   - If detected, runs safety suite to mask/redact BEFORE LLM sees it\n\n2. Completion Rules Check\n   - After safety preprocessing, check if existing tools can solve the task\n   - Route to grammar tools, math tools, etc.\n\n3. LLM Sub-calls (only when needed)\n   - LLM only receives sanitized/masked text\n   - Fall back for complex reasoning\n\nKey Safety Benefit: LLM never directly sees raw PII data.",
      "source_file": "exp004_safety_macro_pipeline.py",
      "dataset_info": "CoLA (Grammar), PII Detection",
      "runs": [
        {
          "experiment_id": "exp004",
          "run_id": "exp004_20260112_183839",
          "timestamp": "2026-01-12T18:38:39",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp004_20260112_183839",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp004",
          "run_id": "exp004_20260112_183909",
          "timestamp": "2026-01-12T18:39:09",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp004_20260112_183909",
          "results": {
            "metadata": {
              "model": "llama3.2:3b",
              "timestamp": "20260112_183909",
              "cola_samples": 50,
              "pii_samples": 30
            },
            "baseline": {
              "total_tokens": 21415,
              "raw_pii_exposures": 30
            },
            "pipeline": {
              "total_tokens": 12114,
              "tool_creation_tokens": 1352,
              "llm_tokens": 10762,
              "total_tasks": 80,
              "pii_masked": 30,
              "tool_handled": 30,
              "llm_fallbacks": 50,
              "raw_pii_exposures": 0
            }
          },
          "tools": [
            {
              "name": "basic_grammar",
              "category": "grammar",
              "code": "import re\n\ndef run(text: str) -> dict:\n    result = {'acceptable': True, 'reason': ''}\n    \n    # Check for subject-verb agreement\n    if 's' not in text.lower() and re.search(r'\\bs\\w+\\s', text):\n        result['acceptable'] = False\n        result['reason'] += \"Missing article. \"\n        \n    # Check for proper punctuation\n    if not re.search(r'[.,!?]$', text):\n        result['acceptable'] = False\n        result['reason'] += \"Lack of proper punctuation.\"\n        \n    # Check sentence structure\n    if len(re.findall(r'\\b\\w+\\b', text)) < 2:\n        result['acceptable'] = False\n        result['reason'] += \"Insufficient clause structure.\"\n        \n    return result",
              "path": "tools/grammar/basic_grammar.py"
            },
            {
              "name": "email_detector",
              "category": "pii",
              "code": "import re\n\ndef run(text: str) -> list:\n    pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    result = []\n    for match in re.finditer(pattern, text):\n        start, end = match.span()\n        email = match.group()\n        result.append({'type': 'EMAIL', 'text': email, 'start': start, 'end': end})\n    return result",
              "path": "tools/pii/email_detector.py"
            },
            {
              "name": "phone_detector",
              "category": "pii",
              "code": "import re\n\ndef run(text: str) -> list:\n    pattern = r'\\b\\+?\\d{3}\\s?\\d{3}\\s?\\d{4}\\b'\n    matches = re.finditer(pattern, text)\n    result = []\n    for match in matches:\n        start, end = match.span()\n        phone = match.group().replace(' ', '')\n        result.append({'type': 'PHONE', 'text': phone, 'start': start, 'end': end})\n    return result",
              "path": "tools/pii/phone_detector.py"
            },
            {
              "name": "ssn_detector",
              "category": "pii",
              "code": "import re\n\ndef run(text: str) -> list:\n    ssn_regex = r'\\b\\d{3}-\\d{2}-\\d{4}\\b'\n    matches = []\n    start = 0\n    for match in re.finditer(ssn_regex, text):\n        if not (match.start() > 0 and match.end() <= len(text)):\n            continue\n        matches.append({\n            'type': 'SSN',\n            'text': match.group(),\n            'start': match.start(),\n            'end': match.end()\n        })\n    return matches",
              "path": "tools/pii/ssn_detector.py"
            },
            {
              "name": "ip_detector",
              "category": "pii",
              "code": "import re\n\ndef run(text: str) -> list:\n    pattern = r'\\b(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b'\n    ips = re.findall(pattern, text)\n    result = []\n    for ip in ips:\n        start = text.find(ip)\n        end = start + len(ip)\n        result.append({\n            'type': 'IP',\n            'text': ip,\n            'start': start,\n            'end': end\n        })\n    return result",
              "path": "tools/pii/ip_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/grammar/__pycache__/basic_grammar.cpython-314.pyc",
            "tools/grammar/basic_grammar.py",
            "tools/pii/__pycache__/email_detector.cpython-314.pyc",
            "tools/pii/__pycache__/ip_detector.cpython-314.pyc",
            "tools/pii/__pycache__/phone_detector.cpython-314.pyc",
            "tools/pii/__pycache__/ssn_detector.cpython-314.pyc",
            "tools/pii/email_detector.py",
            "tools/pii/ip_detector.py",
            "tools/pii/phone_detector.py",
            "tools/pii/ssn_detector.py"
          ]
        }
      ]
    },
    {
      "id": "exp005",
      "name": "Experiment 005: RLM-Based Tool Creation",
      "description": "This experiment uses the SelfDistillRLM class which leverages the RLM framework\nto have the inner LLM create and manage tools, rather than us hardcoding the\ntool creation logic.",
      "architecture": "Key features:\n- LLM creates tools via RLM's REPL environment\n- Tools are saved to disk and tracked\n- Safety macros are created by the LLM\n- PDF report shows all tools/macros with their code",
      "source_file": "exp005_rlm_tool_creation.py",
      "dataset_info": "CoLA (Grammar), PII Detection, SciQ (Science QA)",
      "runs": [
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_205834",
          "timestamp": "2026-01-12T20:58:34",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_205834",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T20:58:52.197139"
            },
            "summary": {
              "baseline_tokens": 2365,
              "rlm_tokens": 0,
              "tasks_processed": 8,
              "tools_created": 0,
              "macros_executed": 0,
              "llm_fallbacks": 8
            },
            "tools": [],
            "macros": []
          },
          "tools": [],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_205944",
          "timestamp": "2026-01-12T20:59:44",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_205944",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T20:59:58.981389"
            },
            "summary": {
              "baseline_tokens": 1749,
              "rlm_tokens": 0,
              "tasks_processed": 8,
              "tools_created": 0,
              "macros_executed": 0,
              "llm_fallbacks": 8
            },
            "tools": [],
            "macros": []
          },
          "tools": [],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_210048",
          "timestamp": "2026-01-12T21:00:48",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_210048",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T21:01:56.536975"
            },
            "summary": {
              "baseline_tokens": 1145,
              "rlm_tokens": 0,
              "tasks_processed": 5,
              "tools_created": 0,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [],
            "macros": []
          },
          "tools": [],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_210703",
          "timestamp": "2026-01-12T21:07:03",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_210703",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_210745",
          "timestamp": "2026-01-12T21:07:45",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_210745",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_210823",
          "timestamp": "2026-01-12T21:08:23",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_210823",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_210917",
          "timestamp": "2026-01-12T21:09:17",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_210917",
          "results": {},
          "tools": [
            {
              "name": "grammar_checker",
              "category": "general",
              "code": "# Checks if sentences are grammatically correct\n\nimport re\n\ndef run(text):\n    issues = []\n    if not text[0].isupper():\n        issues.append('Should start with capital')\n    if not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "general",
              "code": "# Detects PII patterns\n\nimport re\n\ndef run(text):\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            found.append(pii_type)\n    return {'has_pii': len(found) > 0, 'types': found}",
              "path": "tools/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "tools/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/__pycache__/pii_detector.cpython-314.pyc",
            "tools/grammar_checker.py",
            "tools/pii_detector.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_211149",
          "timestamp": "2026-01-12T21:11:49",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_211149",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T21:13:41.558796"
            },
            "summary": {
              "baseline_tokens": 1175,
              "rlm_tokens": 0,
              "tasks_processed": 5,
              "tools_created": 6,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "random_number_generator",
                "file_path": "experiment_outputs/exp005_20260112_211149/tools/random_number_generator.py",
                "description": "",
                "code": "# Generates random integers\n\n# Import necessary modules\nimport random\n\ndef run(n):\n    return [random.randint(0, n) for _ in range(n)]",
                "created_at": "2026-01-12T21:12:01.477862"
              },
              {
                "name": "grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_211149/tools/grammar_checker.py",
                "description": "",
                "code": "# Checks if sentences are grammatically correct\n\nimport re\n\ndef run(text):\n    issues = []\n    if not text[0].isupper():\n        issues.append('Should start with capital')\n    if not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
                "created_at": "2026-01-12T21:13:10.104168"
              },
              {
                "name": "sentiment_analysis",
                "file_path": "experiment_outputs/exp005_20260112_211149/tools/sentiment_analysis.py",
                "description": "",
                "code": "# Analyzes sentiment of a sentence\n\nimport nltk\n\n# Download required NLTK data if not already downloaded\nnltk.download('vader_lexicon')\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ndef run(text):\n    sia = SentimentIntensityAnalyzer()\n    sentiment_scores = sia.polarity_scores(text)\n    # Determine if the sentiment is positive or negative\n    if sentiment_scores['compound'] >= 0.05:\n        return 'Positive'\n    elif sentiment_scores['compound'] <= -0.05:\n        return 'Negative'\n    else:\n        return 'Neutral'",
                "created_at": "2026-01-12T21:12:07.766400"
              },
              {
                "name": "text_preprocessor",
                "file_path": "experiment_outputs/exp005_20260112_211149/tools/text_preprocessor.py",
                "description": "",
                "code": "# Preprocesses text for analysis\n\nimport re\n\ndef run(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text",
                "created_at": "2026-01-12T21:12:07.765442"
              },
              {
                "name": "pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_211149/tools/pii_detector.py",
                "description": "",
                "code": "# Detects PII patterns\n\nimport re\n\ndef run(text):\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            found.append(pii_type)\n    return {'has_pii': len(found) > 0, 'types': found}",
                "created_at": "2026-01-12T21:12:07.767653"
              },
              {
                "name": "sentiment_analyzer",
                "file_path": "experiment_outputs/exp005_20260112_211149/tools/sentiment_analyzer.py",
                "description": "",
                "code": "# Analyzes text for sentiment\n\nimport nltk\n\n# Download required NLTK resources if not already downloaded\nnltk.download('vader_lexicon')\n\n# Import necessary libraries\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Define the function to run\ndef run(text):\n    # Initialize sentiment intensity analyzer\n    sia = SentimentIntensityAnalyzer()\n    # Analyze the sentiment of the text\n    sentiment_scores = sia.polarity_scores(text)\n    # Determine if the sentiment is overall positive, negative or neutral\n    if sentiment_scores['compound'] > 0.05:\n        return 'Positive'\n    elif sentiment_scores['compound'] < -0.05:\n        return 'Negative'\n    else:\n        return 'Neutral'\n\n# Return the final result\nreturn sentiment",
                "created_at": "2026-01-12T21:13:19.235013"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "random_number_generator",
              "category": "general",
              "code": "# Generates random integers\n\n# Import necessary modules\nimport random\n\ndef run(n):\n    return [random.randint(0, n) for _ in range(n)]",
              "path": "tools/random_number_generator.py"
            },
            {
              "name": "grammar_checker",
              "category": "general",
              "code": "# Checks if sentences are grammatically correct\n\nimport re\n\ndef run(text):\n    issues = []\n    if not text[0].isupper():\n        issues.append('Should start with capital')\n    if not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/grammar_checker.py"
            },
            {
              "name": "sentiment_analysis",
              "category": "general",
              "code": "# Analyzes sentiment of a sentence\n\nimport nltk\n\n# Download required NLTK data if not already downloaded\nnltk.download('vader_lexicon')\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ndef run(text):\n    sia = SentimentIntensityAnalyzer()\n    sentiment_scores = sia.polarity_scores(text)\n    # Determine if the sentiment is positive or negative\n    if sentiment_scores['compound'] >= 0.05:\n        return 'Positive'\n    elif sentiment_scores['compound'] <= -0.05:\n        return 'Negative'\n    else:\n        return 'Neutral'",
              "path": "tools/sentiment_analysis.py"
            },
            {
              "name": "text_preprocessor",
              "category": "general",
              "code": "# Preprocesses text for analysis\n\nimport re\n\ndef run(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text",
              "path": "tools/text_preprocessor.py"
            },
            {
              "name": "pii_detector",
              "category": "general",
              "code": "# Detects PII patterns\n\nimport re\n\ndef run(text):\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            found.append(pii_type)\n    return {'has_pii': len(found) > 0, 'types': found}",
              "path": "tools/pii_detector.py"
            },
            {
              "name": "sentiment_analyzer",
              "category": "general",
              "code": "# Analyzes text for sentiment\n\nimport nltk\n\n# Download required NLTK resources if not already downloaded\nnltk.download('vader_lexicon')\n\n# Import necessary libraries\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Define the function to run\ndef run(text):\n    # Initialize sentiment intensity analyzer\n    sia = SentimentIntensityAnalyzer()\n    # Analyze the sentiment of the text\n    sentiment_scores = sia.polarity_scores(text)\n    # Determine if the sentiment is overall positive, negative or neutral\n    if sentiment_scores['compound'] > 0.05:\n        return 'Positive'\n    elif sentiment_scores['compound'] < -0.05:\n        return 'Negative'\n    else:\n        return 'Neutral'\n\n# Return the final result\nreturn sentiment",
              "path": "tools/sentiment_analyzer.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/__pycache__/pii_detector.cpython-314.pyc",
            "tools/__pycache__/random_number_generator.cpython-314.pyc",
            "tools/__pycache__/sentiment_analysis.cpython-314.pyc",
            "tools/__pycache__/text_preprocessor.cpython-314.pyc",
            "tools/grammar_checker.py",
            "tools/pii_detector.py",
            "tools/random_number_generator.py",
            "tools/sentiment_analysis.py",
            "tools/sentiment_analyzer.py",
            "tools/text_preprocessor.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_211836",
          "timestamp": "2026-01-12T21:18:36",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_211836",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T21:28:11.949877"
            },
            "summary": {
              "baseline_tokens": 7536,
              "rlm_tokens": 504308,
              "tasks_processed": 30,
              "tools_created": 13,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "password_generator",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/password_generator.py",
                "description": "",
                "code": "# Generates unique and secure passwords\n",
                "created_at": "2026-01-12T21:21:40.142375"
              },
              {
                "name": "answer_helper",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/answer_helper.py",
                "description": "",
                "code": "# Assists in answering questions\n\ndef answer_helper(text):\n    context = input('Please provide the original context: ')\n    sub_llms = []\n    for line in context.split('\n'):\n        if not line.startswith('#') and '#' not in line:\n            sub_llm = REPL()\n            sub_llms.append(sub_llm)\n    return ''.join([sub_llm.run('answer') for sub_llm in sub_llms])",
                "created_at": "2026-01-12T21:25:58.400746"
              },
              {
                "name": "password_length_checker",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/password_length_checker.py",
                "description": "",
                "code": "# Checks if passwords are long enough\n\nimport re\n\ndef run(password):\n    length = len(password)\n    pattern = r'^[a-zA-Z0-9!@#$%^&*()_+-={}:<>?,./]{8,}$'\n    if re.match(pattern, password):\n        return True\n    else:\n        return False",
                "created_at": "2026-01-12T21:26:11.566144"
              },
              {
                "name": "text_analyzer",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/text_analyzer.py",
                "description": "",
                "code": "# Analyzes text for grammar and punctuation\n\n# Import necessary libraries\n\nimport re\n\ndef analyze_text(text):\n    issues = []\n    if not text[0].isupper():\n        issues.append('Should start with capital')\n    if not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
                "created_at": "2026-01-12T21:23:16.585588"
              },
              {
                "name": "custom_grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/custom_grammar_checker.py",
                "description": "",
                "code": "# Checks if sentences are grammatically correct\n\ndef run(text):\n# Checks if sentence is grammatically correct\n# Checks if sentence is grammatically correct\nimport re\n\ndef run(text):\n    # Check if sentence starts with capital letter\n    if not text[0].isupper():\n        return {'acceptable': False, 'issues': ['Should start with capital']}\n    else:\n        # Check if sentence ends with punctuation\n        if not text[-1] in ('.', '!', '?'):\n            return {'acceptable': False, 'issues': ['Should end with punctuation']}\n        else:\n            return {'acceptable': True, 'issues': []}\nimport re\n\ndef run(text):\n    # Check if sentence starts with capital letter\n    if not text[0].isupper():\n        return {'acceptable': False, 'issues': ['Should start with capital']}\n    else:\n        # Check if sentence ends with punctuation\n        if not text[-1] in ('.', '!', '?'):\n            return {'acceptable': False, 'issues': ['Should end with punctuation']}\n        else:\n            return {'acceptable': True, 'issues': []}",
                "created_at": "2026-01-12T21:24:22.451075"
              },
              {
                "name": "punctuation_remover",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/punctuation_remover.py",
                "description": "",
                "code": "# Removes punctuation from a given text\n\nimport string\n\n# Define punctuation to be removed\nremove_punct = str.maketrans('', '', string.punctuation)\n    # Remove punctuation from the text\ncleaned_text = text.translate(remove_punct)\nreturn {'cleaned_text': cleaned_text}",
                "created_at": "2026-01-12T21:20:00.603367"
              },
              {
                "name": "text_cleaner",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/text_cleaner.py",
                "description": "",
                "code": "# Cleans text from special characters and numbers\n\nimport re\n\ndef run(text):\n    # Remove all non-alphanumeric characters from the input text.\n    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n    return cleaned_text",
                "created_at": "2026-01-12T21:25:56.108143"
              },
              {
                "name": "uppercase_converter",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/uppercase_converter.py",
                "description": "",
                "code": "# Converts text to uppercase\n\n# Import necessary modules\nimport string\n\ndef run(text):\n    return text.upper()",
                "created_at": "2026-01-12T21:25:36.131454"
              },
              {
                "name": "grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/grammar_checker.py",
                "description": "",
                "code": "# Checks if sentences are grammatically correct\n\nimport re\n\ndef run(text):\n    issues = []\n    if not text[0].isupper():\n        issues.append('Should start with capital')\n    if not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
                "created_at": "2026-01-12T21:26:23.393523"
              },
              {
                "name": "rectangle_area_calculator",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/rectangle_area_calculator.py",
                "description": "",
                "code": "# Calculates the area of a rectangle\n\n# Import necessary modules\nimport math\n\ndef run(length, width):\n    return length * width",
                "created_at": "2026-01-12T21:25:36.132587"
              },
              {
                "name": "pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/pii_detector.py",
                "description": "",
                "code": "# Detects PII patterns\n\nimport re\n\ndef run(text):\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            found.append(pii_type)\n    return {'has_pii': len(found) > 0, 'types': found}",
                "created_at": "2026-01-12T21:28:07.358456"
              },
              {
                "name": "word_count",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/word_count.py",
                "description": "",
                "code": "# Counts the number of words in a given text\n\nimport re\n\ndef run(text):\n    # Remove punctuation and convert to lower case\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text).lower()\n    # Split the text into words\n    words = cleaned_text.split()\n    return {'word_count': len(words)}",
                "created_at": "2026-01-12T21:19:43.764686"
              },
              {
                "name": "sentiment_analyzer",
                "file_path": "experiment_outputs/exp005_20260112_211836/tools/sentiment_analyzer.py",
                "description": "",
                "code": "# Analyzes text sentiment\n\n#!pip install nltk",
                "created_at": "2026-01-12T21:19:27.303489"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "password_generator",
              "category": "general",
              "code": "# Generates unique and secure passwords\n",
              "path": "tools/password_generator.py"
            },
            {
              "name": "answer_helper",
              "category": "general",
              "code": "# Assists in answering questions\n\ndef answer_helper(text):\n    context = input('Please provide the original context: ')\n    sub_llms = []\n    for line in context.split('\n'):\n        if not line.startswith('#') and '#' not in line:\n            sub_llm = REPL()\n            sub_llms.append(sub_llm)\n    return ''.join([sub_llm.run('answer') for sub_llm in sub_llms])",
              "path": "tools/answer_helper.py"
            },
            {
              "name": "password_length_checker",
              "category": "general",
              "code": "# Checks if passwords are long enough\n\nimport re\n\ndef run(password):\n    length = len(password)\n    pattern = r'^[a-zA-Z0-9!@#$%^&*()_+-={}:<>?,./]{8,}$'\n    if re.match(pattern, password):\n        return True\n    else:\n        return False",
              "path": "tools/password_length_checker.py"
            },
            {
              "name": "text_analyzer",
              "category": "general",
              "code": "# Analyzes text for grammar and punctuation\n\n# Import necessary libraries\n\nimport re\n\ndef analyze_text(text):\n    issues = []\n    if not text[0].isupper():\n        issues.append('Should start with capital')\n    if not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/text_analyzer.py"
            },
            {
              "name": "custom_grammar_checker",
              "category": "general",
              "code": "# Checks if sentences are grammatically correct\n\ndef run(text):\n# Checks if sentence is grammatically correct\n# Checks if sentence is grammatically correct\nimport re\n\ndef run(text):\n    # Check if sentence starts with capital letter\n    if not text[0].isupper():\n        return {'acceptable': False, 'issues': ['Should start with capital']}\n    else:\n        # Check if sentence ends with punctuation\n        if not text[-1] in ('.', '!', '?'):\n            return {'acceptable': False, 'issues': ['Should end with punctuation']}\n        else:\n            return {'acceptable': True, 'issues': []}\nimport re\n\ndef run(text):\n    # Check if sentence starts with capital letter\n    if not text[0].isupper():\n        return {'acceptable': False, 'issues': ['Should start with capital']}\n    else:\n        # Check if sentence ends with punctuation\n        if not text[-1] in ('.', '!', '?'):\n            return {'acceptable': False, 'issues': ['Should end with punctuation']}\n        else:\n            return {'acceptable': True, 'issues': []}",
              "path": "tools/custom_grammar_checker.py"
            },
            {
              "name": "punctuation_remover",
              "category": "general",
              "code": "# Removes punctuation from a given text\n\nimport string\n\n# Define punctuation to be removed\nremove_punct = str.maketrans('', '', string.punctuation)\n    # Remove punctuation from the text\ncleaned_text = text.translate(remove_punct)\nreturn {'cleaned_text': cleaned_text}",
              "path": "tools/punctuation_remover.py"
            },
            {
              "name": "text_cleaner",
              "category": "general",
              "code": "# Cleans text from special characters and numbers\n\nimport re\n\ndef run(text):\n    # Remove all non-alphanumeric characters from the input text.\n    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n    return cleaned_text",
              "path": "tools/text_cleaner.py"
            },
            {
              "name": "uppercase_converter",
              "category": "general",
              "code": "# Converts text to uppercase\n\n# Import necessary modules\nimport string\n\ndef run(text):\n    return text.upper()",
              "path": "tools/uppercase_converter.py"
            },
            {
              "name": "grammar_checker",
              "category": "general",
              "code": "# Checks if sentences are grammatically correct\n\nimport re\n\ndef run(text):\n    issues = []\n    if not text[0].isupper():\n        issues.append('Should start with capital')\n    if not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/grammar_checker.py"
            },
            {
              "name": "rectangle_area_calculator",
              "category": "general",
              "code": "# Calculates the area of a rectangle\n\n# Import necessary modules\nimport math\n\ndef run(length, width):\n    return length * width",
              "path": "tools/rectangle_area_calculator.py"
            },
            {
              "name": "pii_detector",
              "category": "general",
              "code": "# Detects PII patterns\n\nimport re\n\ndef run(text):\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            found.append(pii_type)\n    return {'has_pii': len(found) > 0, 'types': found}",
              "path": "tools/pii_detector.py"
            },
            {
              "name": "word_count",
              "category": "general",
              "code": "# Counts the number of words in a given text\n\nimport re\n\ndef run(text):\n    # Remove punctuation and convert to lower case\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text).lower()\n    # Split the text into words\n    words = cleaned_text.split()\n    return {'word_count': len(words)}",
              "path": "tools/word_count.py"
            },
            {
              "name": "sentiment_analyzer",
              "category": "general",
              "code": "# Analyzes text sentiment\n\n#!pip install nltk",
              "path": "tools/sentiment_analyzer.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/__pycache__/password_generator.cpython-314.pyc",
            "tools/__pycache__/password_length_checker.cpython-314.pyc",
            "tools/__pycache__/pii_detector.cpython-314.pyc",
            "tools/__pycache__/punctuation_remover.cpython-314.pyc",
            "tools/__pycache__/rectangle_area_calculator.cpython-314.pyc",
            "tools/__pycache__/sentiment_analyzer.cpython-314.pyc",
            "tools/__pycache__/text_analyzer.cpython-314.pyc",
            "tools/__pycache__/text_cleaner.cpython-314.pyc",
            "tools/__pycache__/uppercase_converter.cpython-314.pyc",
            "tools/__pycache__/word_count.cpython-314.pyc",
            "tools/answer_helper.py",
            "tools/custom_grammar_checker.py",
            "tools/grammar_checker.py",
            "tools/password_generator.py",
            "tools/password_length_checker.py",
            "tools/pii_detector.py",
            "tools/punctuation_remover.py",
            "tools/rectangle_area_calculator.py",
            "tools/sentiment_analyzer.py",
            "tools/text_analyzer.py",
            "tools/text_cleaner.py",
            "tools/uppercase_converter.py",
            "tools/word_count.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_213557",
          "timestamp": "2026-01-12T21:35:57",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_213557",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T21:41:00.034825"
            },
            "summary": {
              "baseline_tokens": 1673,
              "rlm_tokens": 245779,
              "tasks_processed": 8,
              "tools_created": 4,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/date_masker",
                "file_path": "experiment_outputs/exp005_20260112_213557/tools/pre_completion/date_masker.py",
                "description": "Masks date in text",
                "code": "# Masks date in text\n\nimport re\n\ndef check(text):\n    if '/\\d{1,2}/\\d{1,2}/\\d{4}' in text:\n        return {'action': 'recurse', 'tool': 'date_masker', 'text': re.sub('/\\d{1,2}/\\d{1,2}/\\d{4}', '[DATE]', text)}\n    else:\n        return {'action': 'continue'}",
                "created_at": "2026-01-12T21:36:49.382738"
              },
              {
                "name": "pre_completion/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_213557/tools/pre_completion/pii_detector.py",
                "description": "Detects PII and triggers masking",
                "code": "# Detects PII and triggers masking\n\n\nimport re\ndef check(text):",
                "created_at": "2026-01-12T21:36:52.972274"
              },
              {
                "name": "replacements/date_checker",
                "file_path": "experiment_outputs/exp005_20260112_213557/tools/replacements/date_checker.py",
                "description": "Checks date in text",
                "code": "# Checks date in text\n\nimport re\n\ndef run(text):\n    pattern = r'\\d{1,2}/\\d{1,2}/\\d{4}'\n    if re.search(pattern, text):\n        date = text.split('/')[2]\n        return {'acceptable': True}\n    else:\n        return {'acceptable': False}",
                "created_at": "2026-01-12T21:36:49.381679"
              },
              {
                "name": "replacements/pii_masker",
                "file_path": "experiment_outputs/exp005_20260112_213557/tools/replacements/pii_masker.py",
                "description": "Masks PII in text",
                "code": "# Masks PII in text\n\nimport re\n\ndef run(text):\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text",
                "created_at": "2026-01-12T21:36:15.357289"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "date_checker",
              "category": "replacements",
              "code": "# Checks date in text\n\nimport re\n\ndef run(text):\n    pattern = r'\\d{1,2}/\\d{1,2}/\\d{4}'\n    if re.search(pattern, text):\n        date = text.split('/')[2]\n        return {'acceptable': True}\n    else:\n        return {'acceptable': False}",
              "path": "tools/replacements/date_checker.py"
            },
            {
              "name": "pii_masker",
              "category": "replacements",
              "code": "# Masks PII in text\n\nimport re\n\ndef run(text):\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text",
              "path": "tools/replacements/pii_masker.py"
            },
            {
              "name": "date_masker",
              "category": "pre_completion",
              "code": "# Masks date in text\n\nimport re\n\ndef check(text):\n    if '/\\d{1,2}/\\d{1,2}/\\d{4}' in text:\n        return {'action': 'recurse', 'tool': 'date_masker', 'text': re.sub('/\\d{1,2}/\\d{1,2}/\\d{4}', '[DATE]', text)}\n    else:\n        return {'action': 'continue'}",
              "path": "tools/pre_completion/date_masker.py"
            },
            {
              "name": "pii_detector",
              "category": "pre_completion",
              "code": "# Detects PII and triggers masking\n\n\nimport re\ndef check(text):",
              "path": "tools/pre_completion/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/date_masker.cpython-314.pyc",
            "tools/pre_completion/date_masker.py",
            "tools/pre_completion/pii_detector.py",
            "tools/replacements/__pycache__/date_checker.cpython-314.pyc",
            "tools/replacements/__pycache__/pii_masker.cpython-314.pyc",
            "tools/replacements/date_checker.py",
            "tools/replacements/pii_masker.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_214436",
          "timestamp": "2026-01-12T21:44:36",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_214436",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T21:44:37.310770"
            },
            "summary": {
              "baseline_tokens": 0,
              "rlm_tokens": 0,
              "tasks_processed": 15,
              "tools_created": 5,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/grammar_detector",
                "file_path": "experiment_outputs/exp005_20260112_214436/tools/pre_completion/grammar_detector.py",
                "description": "Detects grammar task type and triggers replacement",
                "code": "# Detects grammar task type and triggers replacement\ndef check(text):\n    \"\"\"Check if this is a grammar acceptability task.\"\"\"\n    # Check for grammar task indicators in the prompt\n    grammar_keywords = ['grammatical', 'grammar', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    if any(kw in text_lower for kw in grammar_keywords):\n        return {'action': 'replace', 'tool': 'grammar_checker'}\n    return {'action': 'continue'}\n",
                "created_at": "2026-01-12T21:44:36.376659"
              },
              {
                "name": "pre_completion/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_214436/tools/pre_completion/pii_detector.py",
                "description": "Detects PII and triggers replacement tool",
                "code": "# Detects PII and triggers replacement tool\nimport re\n\ndef check(text):\n    \"\"\"Check if text contains PII patterns.\"\"\"\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            return {'action': 'replace', 'tool': 'pii_analyzer'}\n    return {'action': 'continue'}\n",
                "created_at": "2026-01-12T21:44:36.376492"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_214436/tools/replacements/grammar_checker.py",
                "description": "Checks grammar acceptability - replaces LLM call",
                "code": "# Checks grammar acceptability - replaces LLM call\nimport re\n\ndef run(text):\n    \"\"\"Check if a sentence is grammatically acceptable.\"\"\"\n    # Extract the actual sentence from the prompt\n    # Look for quoted text or text after \"Analyze this text:\"\n    sentence = text\n    if '\"' in text:\n        # Extract quoted portion\n        match = re.search(r'\"([^\"]+)\"', text)\n        if match:\n            sentence = match.group(1)\n    elif 'Analyze this text:' in text:\n        sentence = text.split('Analyze this text:')[-1].strip()\n\n    issues = []\n\n    # Check capitalization\n    if sentence and not sentence[0].isupper():\n        issues.append('Should start with capital letter')\n\n    # Check ending punctuation\n    if sentence and not sentence.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n\n    # Check for common grammatical issues\n    if '  ' in sentence:\n        issues.append('Contains double spaces')\n\n    acceptable = len(issues) == 0\n    return {\n        'acceptable': acceptable,\n        'issues': issues,\n        'sentence': sentence[:100] if len(sentence) > 100 else sentence\n    }\n",
                "created_at": "2026-01-12T21:44:36.376891"
              },
              {
                "name": "replacements/pii_analyzer",
                "file_path": "experiment_outputs/exp005_20260112_214436/tools/replacements/pii_analyzer.py",
                "description": "Analyzes text for PII - replaces LLM call",
                "code": "# Analyzes text for PII - replaces LLM call\nimport re\n\ndef run(text):\n    \"\"\"Detect PII patterns in text.\"\"\"\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        matches = re.findall(pattern, text)\n        if matches:\n            found.append({'type': pii_type, 'count': len(matches)})\n    return {\n        'has_pii': len(found) > 0,\n        'pii_types': found,\n        'recommendation': 'Mask PII before processing' if found else 'No PII detected'\n    }\n",
                "created_at": "2026-01-12T21:44:36.376771"
              },
              {
                "name": "utilities/pii_masker",
                "file_path": "experiment_outputs/exp005_20260112_214436/tools/utilities/pii_masker.py",
                "description": "Masks PII in text",
                "code": "# Masks PII in text\nimport re\n\ndef run(text):\n    \"\"\"Mask PII patterns in text.\"\"\"\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text\n",
                "created_at": "2026-01-12T21:44:36.376981"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar acceptability - replaces LLM call\nimport re\n\ndef run(text):\n    \"\"\"Check if a sentence is grammatically acceptable.\"\"\"\n    # Extract the actual sentence from the prompt\n    # Look for quoted text or text after \"Analyze this text:\"\n    sentence = text\n    if '\"' in text:\n        # Extract quoted portion\n        match = re.search(r'\"([^\"]+)\"', text)\n        if match:\n            sentence = match.group(1)\n    elif 'Analyze this text:' in text:\n        sentence = text.split('Analyze this text:')[-1].strip()\n\n    issues = []\n\n    # Check capitalization\n    if sentence and not sentence[0].isupper():\n        issues.append('Should start with capital letter')\n\n    # Check ending punctuation\n    if sentence and not sentence.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n\n    # Check for common grammatical issues\n    if '  ' in sentence:\n        issues.append('Contains double spaces')\n\n    acceptable = len(issues) == 0\n    return {\n        'acceptable': acceptable,\n        'issues': issues,\n        'sentence': sentence[:100] if len(sentence) > 100 else sentence\n    }\n",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "pii_analyzer",
              "category": "replacements",
              "code": "# Analyzes text for PII - replaces LLM call\nimport re\n\ndef run(text):\n    \"\"\"Detect PII patterns in text.\"\"\"\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        matches = re.findall(pattern, text)\n        if matches:\n            found.append({'type': pii_type, 'count': len(matches)})\n    return {\n        'has_pii': len(found) > 0,\n        'pii_types': found,\n        'recommendation': 'Mask PII before processing' if found else 'No PII detected'\n    }\n",
              "path": "tools/replacements/pii_analyzer.py"
            },
            {
              "name": "pii_masker",
              "category": "utilities",
              "code": "# Masks PII in text\nimport re\n\ndef run(text):\n    \"\"\"Mask PII patterns in text.\"\"\"\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text\n",
              "path": "tools/utilities/pii_masker.py"
            },
            {
              "name": "grammar_detector",
              "category": "pre_completion",
              "code": "# Detects grammar task type and triggers replacement\ndef check(text):\n    \"\"\"Check if this is a grammar acceptability task.\"\"\"\n    # Check for grammar task indicators in the prompt\n    grammar_keywords = ['grammatical', 'grammar', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    if any(kw in text_lower for kw in grammar_keywords):\n        return {'action': 'replace', 'tool': 'grammar_checker'}\n    return {'action': 'continue'}\n",
              "path": "tools/pre_completion/grammar_detector.py"
            },
            {
              "name": "pii_detector",
              "category": "pre_completion",
              "code": "# Detects PII and triggers replacement tool\nimport re\n\ndef check(text):\n    \"\"\"Check if text contains PII patterns.\"\"\"\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            return {'action': 'replace', 'tool': 'pii_analyzer'}\n    return {'action': 'continue'}\n",
              "path": "tools/pre_completion/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_detector.cpython-314.pyc",
            "tools/pre_completion/grammar_detector.py",
            "tools/pre_completion/pii_detector.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/grammar_checker.py",
            "tools/replacements/pii_analyzer.py",
            "tools/utilities/pii_masker.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_214446",
          "timestamp": "2026-01-12T21:44:46",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_214446",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T21:45:05.192220"
            },
            "summary": {
              "baseline_tokens": 3597,
              "rlm_tokens": 0,
              "tasks_processed": 15,
              "tools_created": 5,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/grammar_detector",
                "file_path": "experiment_outputs/exp005_20260112_214446/tools/pre_completion/grammar_detector.py",
                "description": "Detects grammar task type and triggers replacement",
                "code": "# Detects grammar task type and triggers replacement\ndef check(text):\n    \"\"\"Check if this is a grammar acceptability task.\"\"\"\n    # Check for grammar task indicators in the prompt\n    grammar_keywords = ['grammatical', 'grammar', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    if any(kw in text_lower for kw in grammar_keywords):\n        return {'action': 'replace', 'tool': 'grammar_checker'}\n    return {'action': 'continue'}\n",
                "created_at": "2026-01-12T21:44:46.229623"
              },
              {
                "name": "pre_completion/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_214446/tools/pre_completion/pii_detector.py",
                "description": "Detects PII and triggers replacement tool",
                "code": "# Detects PII and triggers replacement tool\nimport re\n\ndef check(text):\n    \"\"\"Check if text contains PII patterns.\"\"\"\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            return {'action': 'replace', 'tool': 'pii_analyzer'}\n    return {'action': 'continue'}\n",
                "created_at": "2026-01-12T21:44:46.229535"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_214446/tools/replacements/grammar_checker.py",
                "description": "Checks grammar acceptability - replaces LLM call",
                "code": "# Checks grammar acceptability - replaces LLM call\nimport re\n\ndef run(text):\n    \"\"\"Check if a sentence is grammatically acceptable.\"\"\"\n    # Extract the actual sentence from the prompt\n    # Look for quoted text or text after \"Analyze this text:\"\n    sentence = text\n    if '\"' in text:\n        # Extract quoted portion\n        match = re.search(r'\"([^\"]+)\"', text)\n        if match:\n            sentence = match.group(1)\n    elif 'Analyze this text:' in text:\n        sentence = text.split('Analyze this text:')[-1].strip()\n\n    issues = []\n\n    # Check capitalization\n    if sentence and not sentence[0].isupper():\n        issues.append('Should start with capital letter')\n\n    # Check ending punctuation\n    if sentence and not sentence.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n\n    # Check for common grammatical issues\n    if '  ' in sentence:\n        issues.append('Contains double spaces')\n\n    acceptable = len(issues) == 0\n    return {\n        'acceptable': acceptable,\n        'issues': issues,\n        'sentence': sentence[:100] if len(sentence) > 100 else sentence\n    }\n",
                "created_at": "2026-01-12T21:44:46.229754"
              },
              {
                "name": "replacements/pii_analyzer",
                "file_path": "experiment_outputs/exp005_20260112_214446/tools/replacements/pii_analyzer.py",
                "description": "Analyzes text for PII - replaces LLM call",
                "code": "# Analyzes text for PII - replaces LLM call\nimport re\n\ndef run(text):\n    \"\"\"Detect PII patterns in text.\"\"\"\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        matches = re.findall(pattern, text)\n        if matches:\n            found.append({'type': pii_type, 'count': len(matches)})\n    return {\n        'has_pii': len(found) > 0,\n        'pii_types': found,\n        'recommendation': 'Mask PII before processing' if found else 'No PII detected'\n    }\n",
                "created_at": "2026-01-12T21:44:46.229690"
              },
              {
                "name": "utilities/pii_masker",
                "file_path": "experiment_outputs/exp005_20260112_214446/tools/utilities/pii_masker.py",
                "description": "Masks PII in text",
                "code": "# Masks PII in text\nimport re\n\ndef run(text):\n    \"\"\"Mask PII patterns in text.\"\"\"\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text\n",
                "created_at": "2026-01-12T21:44:46.229822"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar acceptability - replaces LLM call\nimport re\n\ndef run(text):\n    \"\"\"Check if a sentence is grammatically acceptable.\"\"\"\n    # Extract the actual sentence from the prompt\n    # Look for quoted text or text after \"Analyze this text:\"\n    sentence = text\n    if '\"' in text:\n        # Extract quoted portion\n        match = re.search(r'\"([^\"]+)\"', text)\n        if match:\n            sentence = match.group(1)\n    elif 'Analyze this text:' in text:\n        sentence = text.split('Analyze this text:')[-1].strip()\n\n    issues = []\n\n    # Check capitalization\n    if sentence and not sentence[0].isupper():\n        issues.append('Should start with capital letter')\n\n    # Check ending punctuation\n    if sentence and not sentence.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n\n    # Check for common grammatical issues\n    if '  ' in sentence:\n        issues.append('Contains double spaces')\n\n    acceptable = len(issues) == 0\n    return {\n        'acceptable': acceptable,\n        'issues': issues,\n        'sentence': sentence[:100] if len(sentence) > 100 else sentence\n    }\n",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "pii_analyzer",
              "category": "replacements",
              "code": "# Analyzes text for PII - replaces LLM call\nimport re\n\ndef run(text):\n    \"\"\"Detect PII patterns in text.\"\"\"\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    found = []\n    for pii_type, pattern in patterns.items():\n        matches = re.findall(pattern, text)\n        if matches:\n            found.append({'type': pii_type, 'count': len(matches)})\n    return {\n        'has_pii': len(found) > 0,\n        'pii_types': found,\n        'recommendation': 'Mask PII before processing' if found else 'No PII detected'\n    }\n",
              "path": "tools/replacements/pii_analyzer.py"
            },
            {
              "name": "pii_masker",
              "category": "utilities",
              "code": "# Masks PII in text\nimport re\n\ndef run(text):\n    \"\"\"Mask PII patterns in text.\"\"\"\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text\n",
              "path": "tools/utilities/pii_masker.py"
            },
            {
              "name": "grammar_detector",
              "category": "pre_completion",
              "code": "# Detects grammar task type and triggers replacement\ndef check(text):\n    \"\"\"Check if this is a grammar acceptability task.\"\"\"\n    # Check for grammar task indicators in the prompt\n    grammar_keywords = ['grammatical', 'grammar', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    if any(kw in text_lower for kw in grammar_keywords):\n        return {'action': 'replace', 'tool': 'grammar_checker'}\n    return {'action': 'continue'}\n",
              "path": "tools/pre_completion/grammar_detector.py"
            },
            {
              "name": "pii_detector",
              "category": "pre_completion",
              "code": "# Detects PII and triggers replacement tool\nimport re\n\ndef check(text):\n    \"\"\"Check if text contains PII patterns.\"\"\"\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            return {'action': 'replace', 'tool': 'pii_analyzer'}\n    return {'action': 'continue'}\n",
              "path": "tools/pre_completion/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_detector.cpython-314.pyc",
            "tools/pre_completion/grammar_detector.py",
            "tools/pre_completion/pii_detector.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/grammar_checker.py",
            "tools/replacements/pii_analyzer.py",
            "tools/utilities/pii_masker.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_214556",
          "timestamp": "2026-01-12T21:45:56",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_214556",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T21:57:11.171196"
            },
            "summary": {
              "baseline_tokens": 3680,
              "rlm_tokens": 500963,
              "tasks_processed": 15,
              "tools_created": 7,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/grammar_checker_hook",
                "file_path": "experiment_outputs/exp005_20260112_214556/tools/pre_completion/grammar_checker_hook.py",
                "description": "",
                "code": "\ndef check(text):\n    return {'action': 'recurse', 'tool': 'grammar_checker', 'text': text}",
                "created_at": "2026-01-12T21:48:05.914439"
              },
              {
                "name": "pre_completion/pii_masker_output_detector",
                "file_path": "experiment_outputs/exp005_20260112_214556/tools/pre_completion/pii_masker_output_detector.py",
                "description": "Detects PII in replacement tool output",
                "code": "# Detects PII in replacement tool output\n\nimport re\n\ndef check(text):\n    patterns = {\n        'email': r'[EMAIL]',\n        'phone': r'[PHONE]\n        'ssn': r'[SSN]'\n    }\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            return {'action': 'recurse', 'tool': 'pii_masker_replacer', 'text': text}\n    return {'action': 'continue'}",
                "created_at": "2026-01-12T21:56:08.496537"
              },
              {
                "name": "pre_completion/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_214556/tools/pre_completion/pii_detector.py",
                "description": "Detects PII and triggers masking",
                "code": "# Detects PII and triggers masking\n\nimport re\n\ndef check(text):\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            return {'action': 'recurse', 'tool': 'pii_masker', 'text': text}\n    return {'action': 'continue'}",
                "created_at": "2026-01-12T21:56:41.985627"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_214556/tools/replacements/grammar_checker.py",
                "description": "Checks grammar without LLM",
                "code": "# Checks grammar without LLM\n\nimport re\n\ndef run(text):\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
                "created_at": "2026-01-12T21:56:45.971627"
              },
              {
                "name": "replacements/pii_masker",
                "file_path": "experiment_outputs/exp005_20260112_214556/tools/replacements/pii_masker.py",
                "description": "Masks PII in text",
                "code": "# Masks PII in text\n\nimport re\n\ndef run(text):\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text",
                "created_at": "2026-01-12T21:56:08.496018"
              },
              {
                "name": "utilities/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_214556/tools/utilities/grammar_checker.py",
                "description": "Checks grammar without LLM",
                "code": "# Checks grammar without LLM\n\nimport re\n\ndef run(text):\n    issues = []\n    if text and not text[0].isupper():\n        issues.append(\"Should start with capital\")\n    if text and not text.rstrip().endswith((\".\", \"!\", \"?\")):\n        issues.append(\"Should end with punctuation\")\n    return {\"acceptable\": len(issues) == 0, \"issues\": issues}",
                "created_at": "2026-01-12T21:56:18.636293"
              },
              {
                "name": "utilities/pii_masker",
                "file_path": "experiment_outputs/exp005_20260112_214556/tools/utilities/pii_masker.py",
                "description": "Masks PII in text",
                "code": "# Masks PII in text\n\nimport re\n\ndef run(text):\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    return {''}",
                "created_at": "2026-01-12T21:56:54.708405"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar without LLM\n\nimport re\n\ndef run(text):\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "pii_masker",
              "category": "replacements",
              "code": "# Masks PII in text\n\nimport re\n\ndef run(text):\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text",
              "path": "tools/replacements/pii_masker.py"
            },
            {
              "name": "grammar_checker",
              "category": "utilities",
              "code": "# Checks grammar without LLM\n\nimport re\n\ndef run(text):\n    issues = []\n    if text and not text[0].isupper():\n        issues.append(\"Should start with capital\")\n    if text and not text.rstrip().endswith((\".\", \"!\", \"?\")):\n        issues.append(\"Should end with punctuation\")\n    return {\"acceptable\": len(issues) == 0, \"issues\": issues}",
              "path": "tools/utilities/grammar_checker.py"
            },
            {
              "name": "pii_masker",
              "category": "utilities",
              "code": "# Masks PII in text\n\nimport re\n\ndef run(text):\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    return {''}",
              "path": "tools/utilities/pii_masker.py"
            },
            {
              "name": "grammar_checker_hook",
              "category": "pre_completion",
              "code": "\ndef check(text):\n    return {'action': 'recurse', 'tool': 'grammar_checker', 'text': text}",
              "path": "tools/pre_completion/grammar_checker_hook.py"
            },
            {
              "name": "pii_masker_output_detector",
              "category": "pre_completion",
              "code": "# Detects PII in replacement tool output\n\nimport re\n\ndef check(text):\n    patterns = {\n        'email': r'[EMAIL]',\n        'phone': r'[PHONE]\n        'ssn': r'[SSN]'\n    }\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            return {'action': 'recurse', 'tool': 'pii_masker_replacer', 'text': text}\n    return {'action': 'continue'}",
              "path": "tools/pre_completion/pii_masker_output_detector.py"
            },
            {
              "name": "pii_detector",
              "category": "pre_completion",
              "code": "# Detects PII and triggers masking\n\nimport re\n\ndef check(text):\n    patterns = {\n        'email': r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',\n        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n        'ssn': r'\\d{3}-\\d{2}-\\d{4}'\n    }\n    for pii_type, pattern in patterns.items():\n        if re.search(pattern, text):\n            return {'action': 'recurse', 'tool': 'pii_masker', 'text': text}\n    return {'action': 'continue'}",
              "path": "tools/pre_completion/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_checker_hook.cpython-314.pyc",
            "tools/pre_completion/__pycache__/pii_detector.cpython-314.pyc",
            "tools/pre_completion/grammar_checker_hook.py",
            "tools/pre_completion/pii_detector.py",
            "tools/pre_completion/pii_masker_output_detector.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/__pycache__/pii_masker.cpython-314.pyc",
            "tools/replacements/grammar_checker.py",
            "tools/replacements/pii_masker.py",
            "tools/utilities/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/utilities/__pycache__/pii_masker.cpython-314.pyc",
            "tools/utilities/grammar_checker.py",
            "tools/utilities/pii_masker.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_220406",
          "timestamp": "2026-01-12T22:04:06",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_220406",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T22:04:46.364511"
            },
            "summary": {
              "baseline_tokens": 7478,
              "rlm_tokens": 0,
              "tasks_processed": 30,
              "tools_created": 5,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_220406/tools/pre_completion/grammar_checker.py",
                "description": "Detects grammar tasks - returns bool",
                "code": "# Detects grammar tasks - returns bool\n\ndef check(text: str) -> bool:\n    \"\"\"Return True if this is a grammar acceptability task.\"\"\"\n    keywords = ['grammatical', 'grammar', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)\n",
                "created_at": "2026-01-12T22:04:06.391392"
              },
              {
                "name": "pre_completion/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_220406/tools/pre_completion/pii_detector.py",
                "description": "Detects PII in text - returns bool",
                "code": "# Detects PII in text - returns bool\nimport re\n\ndef check(text: str) -> bool:\n    \"\"\"Return True if text contains PII patterns.\"\"\"\n    patterns = [\n        r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',  # email\n        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # phone\n        r'\\d{3}-\\d{2}-\\d{4}'  # SSN\n    ]\n    return any(re.search(p, text) for p in patterns)\n",
                "created_at": "2026-01-12T22:04:06.391278"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_220406/tools/replacements/grammar_checker.py",
                "description": "Checks grammar - replaces LLM call",
                "code": "# Checks grammar - replaces LLM call\nimport re\n\ndef run(text: str) -> dict:\n    \"\"\"Check if a sentence is grammatically acceptable.\"\"\"\n    # Extract the actual sentence from the prompt\n    sentence = text\n    if '\"' in text:\n        match = re.search(r'\"([^\"]+)\"', text)\n        if match:\n            sentence = match.group(1)\n    elif 'Analyze this text:' in text:\n        sentence = text.split('Analyze this text:')[-1].strip()\n\n    issues = []\n    if sentence and not sentence[0].isupper():\n        issues.append('Should start with capital letter')\n    if sentence and not sentence.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n\n    return {'acceptable': len(issues) == 0, 'issues': issues}\n",
                "created_at": "2026-01-12T22:04:06.391530"
              },
              {
                "name": "replacements/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_220406/tools/replacements/pii_detector.py",
                "description": "Analyzes PII - replaces LLM call",
                "code": "# Analyzes PII - replaces LLM call\nimport re\n\ndef run(text: str) -> dict:\n    \"\"\"Detect and report PII in text.\"\"\"\n    found = []\n    if re.search(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', text):\n        found.append('email')\n    if re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text):\n        found.append('phone')\n    if re.search(r'\\d{3}-\\d{2}-\\d{4}', text):\n        found.append('ssn')\n    return {\n        'has_pii': len(found) > 0,\n        'pii_types': found,\n        'recommendation': 'Mask PII before processing' if found else 'No PII detected'\n    }\n",
                "created_at": "2026-01-12T22:04:06.391469"
              },
              {
                "name": "utilities/pii_masker",
                "file_path": "experiment_outputs/exp005_20260112_220406/tools/utilities/pii_masker.py",
                "description": "Masks PII in text",
                "code": "# Masks PII in text\nimport re\n\ndef run(text: str) -> str:\n    \"\"\"Mask PII patterns in text.\"\"\"\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text\n",
                "created_at": "2026-01-12T22:04:06.391600"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar - replaces LLM call\nimport re\n\ndef run(text: str) -> dict:\n    \"\"\"Check if a sentence is grammatically acceptable.\"\"\"\n    # Extract the actual sentence from the prompt\n    sentence = text\n    if '\"' in text:\n        match = re.search(r'\"([^\"]+)\"', text)\n        if match:\n            sentence = match.group(1)\n    elif 'Analyze this text:' in text:\n        sentence = text.split('Analyze this text:')[-1].strip()\n\n    issues = []\n    if sentence and not sentence[0].isupper():\n        issues.append('Should start with capital letter')\n    if sentence and not sentence.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n\n    return {'acceptable': len(issues) == 0, 'issues': issues}\n",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "replacements",
              "code": "# Analyzes PII - replaces LLM call\nimport re\n\ndef run(text: str) -> dict:\n    \"\"\"Detect and report PII in text.\"\"\"\n    found = []\n    if re.search(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', text):\n        found.append('email')\n    if re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text):\n        found.append('phone')\n    if re.search(r'\\d{3}-\\d{2}-\\d{4}', text):\n        found.append('ssn')\n    return {\n        'has_pii': len(found) > 0,\n        'pii_types': found,\n        'recommendation': 'Mask PII before processing' if found else 'No PII detected'\n    }\n",
              "path": "tools/replacements/pii_detector.py"
            },
            {
              "name": "pii_masker",
              "category": "utilities",
              "code": "# Masks PII in text\nimport re\n\ndef run(text: str) -> str:\n    \"\"\"Mask PII patterns in text.\"\"\"\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text\n",
              "path": "tools/utilities/pii_masker.py"
            },
            {
              "name": "grammar_checker",
              "category": "pre_completion",
              "code": "# Detects grammar tasks - returns bool\n\ndef check(text: str) -> bool:\n    \"\"\"Return True if this is a grammar acceptability task.\"\"\"\n    keywords = ['grammatical', 'grammar', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)\n",
              "path": "tools/pre_completion/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "pre_completion",
              "code": "# Detects PII in text - returns bool\nimport re\n\ndef check(text: str) -> bool:\n    \"\"\"Return True if text contains PII patterns.\"\"\"\n    patterns = [\n        r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',  # email\n        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # phone\n        r'\\d{3}-\\d{2}-\\d{4}'  # SSN\n    ]\n    return any(re.search(p, text) for p in patterns)\n",
              "path": "tools/pre_completion/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/pre_completion/grammar_checker.py",
            "tools/pre_completion/pii_detector.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/grammar_checker.py",
            "tools/replacements/pii_detector.py",
            "tools/utilities/pii_masker.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_220458",
          "timestamp": "2026-01-12T22:04:58",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_220458",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T22:06:35.950429"
            },
            "summary": {
              "baseline_tokens": 3723,
              "rlm_tokens": 89135,
              "tasks_processed": 15,
              "tools_created": 5,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/grammar_checker_hook",
                "file_path": "experiment_outputs/exp005_20260112_220458/tools/pre_completion/grammar_checker_hook.py",
                "description": "Detects grammar tasks",
                "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    return text.lower().startswith(('hello', 'hi'))",
                "created_at": "2026-01-12T22:05:56.579758"
              },
              {
                "name": "pre_completion/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_220458/tools/pre_completion/grammar_checker.py",
                "description": "Detects grammar tasks",
                "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
                "created_at": "2026-01-12T22:06:26.155969"
              },
              {
                "name": "pre_completion/long_form_article_detector",
                "file_path": "experiment_outputs/exp005_20260112_220458/tools/pre_completion/long_form_article_detector.py",
                "description": "Detects long-form articles.",
                "code": "# Detects long-form articles.\n\ndef check(text: str) -> bool:\n    # Check length of text\n    return len(text.split()) > 50",
                "created_at": "2026-01-12T22:05:28.146730"
              },
              {
                "name": "replacements/summary_generator",
                "file_path": "experiment_outputs/exp005_20260112_220458/tools/replacements/summary_generator.py",
                "description": "Generates summary from long-form article.",
                "code": "# Generates summary from long-form article.\n\ndef run(text: str) -> dict:\n    # Tokenize text\n    tokens = nltk.word_tokenize(text)\n    return {'summary': ' '.join(tokens[:50])}",
                "created_at": "2026-01-12T22:05:28.147589"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_220458/tools/replacements/grammar_checker.py",
                "description": "Checks grammar without LLM",
                "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
                "created_at": "2026-01-12T22:06:28.173922"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "summary_generator",
              "category": "replacements",
              "code": "# Generates summary from long-form article.\n\ndef run(text: str) -> dict:\n    # Tokenize text\n    tokens = nltk.word_tokenize(text)\n    return {'summary': ' '.join(tokens[:50])}",
              "path": "tools/replacements/summary_generator.py"
            },
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "grammar_checker_hook",
              "category": "pre_completion",
              "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    return text.lower().startswith(('hello', 'hi'))",
              "path": "tools/pre_completion/grammar_checker_hook.py"
            },
            {
              "name": "grammar_checker",
              "category": "pre_completion",
              "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
              "path": "tools/pre_completion/grammar_checker.py"
            },
            {
              "name": "long_form_article_detector",
              "category": "pre_completion",
              "code": "# Detects long-form articles.\n\ndef check(text: str) -> bool:\n    # Check length of text\n    return len(text.split()) > 50",
              "path": "tools/pre_completion/long_form_article_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/pre_completion/__pycache__/grammar_checker_hook.cpython-314.pyc",
            "tools/pre_completion/__pycache__/long_form_article_detector.cpython-314.pyc",
            "tools/pre_completion/grammar_checker.py",
            "tools/pre_completion/grammar_checker_hook.py",
            "tools/pre_completion/long_form_article_detector.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/__pycache__/summary_generator.cpython-314.pyc",
            "tools/replacements/grammar_checker.py",
            "tools/replacements/summary_generator.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_220712",
          "timestamp": "2026-01-12T22:07:12",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_220712",
          "results": {
            "metadata": {
              "model": "ollama/qwen2.5-coder:32b",
              "timestamp": "2026-01-12T22:12:37.358077"
            },
            "summary": {
              "baseline_tokens": 4954,
              "rlm_tokens": 28590,
              "tasks_processed": 15,
              "tools_created": 8,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/keyword_extractor",
                "file_path": "experiment_outputs/exp005_20260112_220712/tools/pre_completion/keyword_extractor.py",
                "description": "Detects tasks related to keyword extraction",
                "code": "# Detects tasks related to keyword extraction\n\ndef check(text: str) -> bool:\n    '''Return True if this is a keyword extraction task.'''\n    keywords = ['extract', 'keywords', 'terms', 'key terms']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
                "created_at": "2026-01-12T22:11:05.708050"
              },
              {
                "name": "pre_completion/text_summarizer",
                "file_path": "experiment_outputs/exp005_20260112_220712/tools/pre_completion/text_summarizer.py",
                "description": "Detects summary tasks",
                "code": "# Detects summary tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a summarization task.'''\n    keywords = ['summary', 'summarize', 'briefly', 'concisely']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
                "created_at": "2026-01-12T22:10:22.537265"
              },
              {
                "name": "pre_completion/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_220712/tools/pre_completion/grammar_checker.py",
                "description": "Detects grammar tasks",
                "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
                "created_at": "2026-01-12T22:11:42.072744"
              },
              {
                "name": "pre_completion/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_220712/tools/pre_completion/pii_detector.py",
                "description": "Detects PII in text",
                "code": "# Detects PII in text\n\nimport re\n\ndef check(text: str) -> bool:\n    '''Return True if text contains PII.'''\n    patterns = [\n        r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',  # email\n        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # phone\n        r'\\d{3}-\\d{2}-\\d{4}'  # SSN\n    ]\n    return any(re.search(p, text) for p in patterns)",
                "created_at": "2026-01-12T22:12:11.975332"
              },
              {
                "name": "replacements/keyword_extractor",
                "file_path": "experiment_outputs/exp005_20260112_220712/tools/replacements/keyword_extractor.py",
                "description": "Extracts keywords from text",
                "code": "# Extracts keywords from text\n\nimport re\n\ndef run(text: str) -> dict:\n    '''Extract and report keywords in text.'''\n    # Simple keyword extraction using NLTK's FreqDist and stopwords\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize\n    from nltk.probability import FreqDist\n\n    # Ensure stopwords are downloaded\n    import nltk\n    nltk.download('punkt')\n    nltk.download('stopwords')\n\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.isalnum() and word.lower() not in stop_words]\n    freq_dist = FreqDist(filtered_text)\n    keywords = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:5]\n    return {'keywords': [word for word, _ in keywords]}",
                "created_at": "2026-01-12T22:11:05.709135"
              },
              {
                "name": "replacements/text_summarizer",
                "file_path": "experiment_outputs/exp005_20260112_220712/tools/replacements/text_summarizer.py",
                "description": "Generates summaries without LLM",
                "code": "# Generates summaries without LLM\n\ndef run(text: str) -> dict:\n    '''Generate a simple summary of the given text.'''\n    import re\n    sentences = re.split(r'(?<=[.!?]) +', text)\n    summary = ' '.join(sentences[:3]) if len(sentences) > 2 else text\n    return {'summary': summary}",
                "created_at": "2026-01-12T22:10:22.538306"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_220712/tools/replacements/grammar_checker.py",
                "description": "Checks grammar without LLM",
                "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Sentence should start with a capital letter')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Sentence should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
                "created_at": "2026-01-12T22:11:42.073700"
              },
              {
                "name": "replacements/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_220712/tools/replacements/pii_detector.py",
                "description": "Analyzes PII without LLM",
                "code": "# Analyzes PII without LLM\n\nimport re\n\ndef run(text: str) -> dict:\n    '''Detect and report PII in text.'''\n    found = []\n    if re.search(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', text):\n        found.append('email')\n    if re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text):\n        found.append('phone')\n    if re.search(r'\\d{3}-\\d{2}-\\d{4}', text):\n        found.append('ssn')\n    return {'has_pii': len(found) > 0, 'pii_types': found}",
                "created_at": "2026-01-12T22:12:11.976482"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "keyword_extractor",
              "category": "replacements",
              "code": "# Extracts keywords from text\n\nimport re\n\ndef run(text: str) -> dict:\n    '''Extract and report keywords in text.'''\n    # Simple keyword extraction using NLTK's FreqDist and stopwords\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize\n    from nltk.probability import FreqDist\n\n    # Ensure stopwords are downloaded\n    import nltk\n    nltk.download('punkt')\n    nltk.download('stopwords')\n\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.isalnum() and word.lower() not in stop_words]\n    freq_dist = FreqDist(filtered_text)\n    keywords = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:5]\n    return {'keywords': [word for word, _ in keywords]}",
              "path": "tools/replacements/keyword_extractor.py"
            },
            {
              "name": "text_summarizer",
              "category": "replacements",
              "code": "# Generates summaries without LLM\n\ndef run(text: str) -> dict:\n    '''Generate a simple summary of the given text.'''\n    import re\n    sentences = re.split(r'(?<=[.!?]) +', text)\n    summary = ' '.join(sentences[:3]) if len(sentences) > 2 else text\n    return {'summary': summary}",
              "path": "tools/replacements/text_summarizer.py"
            },
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Sentence should start with a capital letter')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Sentence should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "replacements",
              "code": "# Analyzes PII without LLM\n\nimport re\n\ndef run(text: str) -> dict:\n    '''Detect and report PII in text.'''\n    found = []\n    if re.search(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', text):\n        found.append('email')\n    if re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text):\n        found.append('phone')\n    if re.search(r'\\d{3}-\\d{2}-\\d{4}', text):\n        found.append('ssn')\n    return {'has_pii': len(found) > 0, 'pii_types': found}",
              "path": "tools/replacements/pii_detector.py"
            },
            {
              "name": "keyword_extractor",
              "category": "pre_completion",
              "code": "# Detects tasks related to keyword extraction\n\ndef check(text: str) -> bool:\n    '''Return True if this is a keyword extraction task.'''\n    keywords = ['extract', 'keywords', 'terms', 'key terms']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
              "path": "tools/pre_completion/keyword_extractor.py"
            },
            {
              "name": "text_summarizer",
              "category": "pre_completion",
              "code": "# Detects summary tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a summarization task.'''\n    keywords = ['summary', 'summarize', 'briefly', 'concisely']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
              "path": "tools/pre_completion/text_summarizer.py"
            },
            {
              "name": "grammar_checker",
              "category": "pre_completion",
              "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
              "path": "tools/pre_completion/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "pre_completion",
              "code": "# Detects PII in text\n\nimport re\n\ndef check(text: str) -> bool:\n    '''Return True if text contains PII.'''\n    patterns = [\n        r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',  # email\n        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # phone\n        r'\\d{3}-\\d{2}-\\d{4}'  # SSN\n    ]\n    return any(re.search(p, text) for p in patterns)",
              "path": "tools/pre_completion/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/pre_completion/__pycache__/keyword_extractor.cpython-314.pyc",
            "tools/pre_completion/__pycache__/pii_detector.cpython-314.pyc",
            "tools/pre_completion/__pycache__/text_summarizer.cpython-314.pyc",
            "tools/pre_completion/grammar_checker.py",
            "tools/pre_completion/keyword_extractor.py",
            "tools/pre_completion/pii_detector.py",
            "tools/pre_completion/text_summarizer.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/__pycache__/keyword_extractor.cpython-314.pyc",
            "tools/replacements/__pycache__/pii_detector.cpython-314.pyc",
            "tools/replacements/__pycache__/text_summarizer.cpython-314.pyc",
            "tools/replacements/grammar_checker.py",
            "tools/replacements/keyword_extractor.py",
            "tools/replacements/pii_detector.py",
            "tools/replacements/text_summarizer.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_222242",
          "timestamp": "2026-01-12T22:22:42",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_222242",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T22:23:53.385010"
            },
            "summary": {
              "baseline_tokens": 12838,
              "rlm_tokens": 0,
              "tasks_processed": 51,
              "tools_created": 5,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_222242/tools/pre_completion/grammar_checker.py",
                "description": "Detects grammar tasks - returns bool",
                "code": "# Detects grammar tasks - returns bool\n\ndef check(text: str) -> bool:\n    \"\"\"Return True if this is a grammar acceptability task.\"\"\"\n    keywords = ['grammatical', 'grammar', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)\n",
                "created_at": "2026-01-12T22:22:42.331152"
              },
              {
                "name": "pre_completion/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_222242/tools/pre_completion/pii_detector.py",
                "description": "Detects PII in text - returns bool",
                "code": "# Detects PII in text - returns bool\nimport re\n\ndef check(text: str) -> bool:\n    \"\"\"Return True if text contains PII patterns.\"\"\"\n    patterns = [\n        r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',  # email\n        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # phone\n        r'\\d{3}-\\d{2}-\\d{4}'  # SSN\n    ]\n    return any(re.search(p, text) for p in patterns)\n",
                "created_at": "2026-01-12T22:22:42.330964"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_222242/tools/replacements/grammar_checker.py",
                "description": "Checks grammar - replaces LLM call",
                "code": "# Checks grammar - replaces LLM call\nimport re\n\ndef run(text: str) -> dict:\n    \"\"\"Check if a sentence is grammatically acceptable.\"\"\"\n    # Extract the actual sentence from the prompt\n    sentence = text\n    if '\"' in text:\n        match = re.search(r'\"([^\"]+)\"', text)\n        if match:\n            sentence = match.group(1)\n    elif 'Analyze this text:' in text:\n        sentence = text.split('Analyze this text:')[-1].strip()\n\n    issues = []\n    if sentence and not sentence[0].isupper():\n        issues.append('Should start with capital letter')\n    if sentence and not sentence.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n\n    return {'acceptable': len(issues) == 0, 'issues': issues}\n",
                "created_at": "2026-01-12T22:22:42.331352"
              },
              {
                "name": "replacements/pii_detector",
                "file_path": "experiment_outputs/exp005_20260112_222242/tools/replacements/pii_detector.py",
                "description": "Analyzes PII - replaces LLM call",
                "code": "# Analyzes PII - replaces LLM call\nimport re\n\ndef run(text: str) -> dict:\n    \"\"\"Detect and report PII in text.\"\"\"\n    found = []\n    if re.search(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', text):\n        found.append('email')\n    if re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text):\n        found.append('phone')\n    if re.search(r'\\d{3}-\\d{2}-\\d{4}', text):\n        found.append('ssn')\n    return {\n        'has_pii': len(found) > 0,\n        'pii_types': found,\n        'recommendation': 'Mask PII before processing' if found else 'No PII detected'\n    }\n",
                "created_at": "2026-01-12T22:22:42.331268"
              },
              {
                "name": "utilities/pii_masker",
                "file_path": "experiment_outputs/exp005_20260112_222242/tools/utilities/pii_masker.py",
                "description": "Masks PII in text",
                "code": "# Masks PII in text\nimport re\n\ndef run(text: str) -> str:\n    \"\"\"Mask PII patterns in text.\"\"\"\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text\n",
                "created_at": "2026-01-12T22:22:42.331429"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar - replaces LLM call\nimport re\n\ndef run(text: str) -> dict:\n    \"\"\"Check if a sentence is grammatically acceptable.\"\"\"\n    # Extract the actual sentence from the prompt\n    sentence = text\n    if '\"' in text:\n        match = re.search(r'\"([^\"]+)\"', text)\n        if match:\n            sentence = match.group(1)\n    elif 'Analyze this text:' in text:\n        sentence = text.split('Analyze this text:')[-1].strip()\n\n    issues = []\n    if sentence and not sentence[0].isupper():\n        issues.append('Should start with capital letter')\n    if sentence and not sentence.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n\n    return {'acceptable': len(issues) == 0, 'issues': issues}\n",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "replacements",
              "code": "# Analyzes PII - replaces LLM call\nimport re\n\ndef run(text: str) -> dict:\n    \"\"\"Detect and report PII in text.\"\"\"\n    found = []\n    if re.search(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', text):\n        found.append('email')\n    if re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text):\n        found.append('phone')\n    if re.search(r'\\d{3}-\\d{2}-\\d{4}', text):\n        found.append('ssn')\n    return {\n        'has_pii': len(found) > 0,\n        'pii_types': found,\n        'recommendation': 'Mask PII before processing' if found else 'No PII detected'\n    }\n",
              "path": "tools/replacements/pii_detector.py"
            },
            {
              "name": "pii_masker",
              "category": "utilities",
              "code": "# Masks PII in text\nimport re\n\ndef run(text: str) -> str:\n    \"\"\"Mask PII patterns in text.\"\"\"\n    text = re.sub(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n    text = re.sub(r'\\d{3}-\\d{2}-\\d{4}', '[SSN]', text)\n    return text\n",
              "path": "tools/utilities/pii_masker.py"
            },
            {
              "name": "grammar_checker",
              "category": "pre_completion",
              "code": "# Detects grammar tasks - returns bool\n\ndef check(text: str) -> bool:\n    \"\"\"Return True if this is a grammar acceptability task.\"\"\"\n    keywords = ['grammatical', 'grammar', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)\n",
              "path": "tools/pre_completion/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "pre_completion",
              "code": "# Detects PII in text - returns bool\nimport re\n\ndef check(text: str) -> bool:\n    \"\"\"Return True if text contains PII patterns.\"\"\"\n    patterns = [\n        r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',  # email\n        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # phone\n        r'\\d{3}-\\d{2}-\\d{4}'  # SSN\n    ]\n    return any(re.search(p, text) for p in patterns)\n",
              "path": "tools/pre_completion/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "batch_metrics.json",
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/pre_completion/grammar_checker.py",
            "tools/pre_completion/pii_detector.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/grammar_checker.py",
            "tools/replacements/pii_detector.py",
            "tools/utilities/pii_masker.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_222402",
          "timestamp": "2026-01-12T22:24:02",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_222402",
          "results": {
            "metadata": {
              "model": "ollama/qwen2.5-coder:32b",
              "timestamp": "2026-01-12T22:33:05.573870"
            },
            "summary": {
              "baseline_tokens": 15134,
              "rlm_tokens": 10821,
              "tasks_processed": 61,
              "tools_created": 2,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_222402/tools/pre_completion/grammar_checker.py",
                "description": "Detects grammar tasks",
                "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
                "created_at": "2026-01-12T22:27:07.456275"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_222402/tools/replacements/grammar_checker.py",
                "description": "Checks grammar without LLM",
                "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
                "created_at": "2026-01-12T22:27:07.457340"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "grammar_checker",
              "category": "pre_completion",
              "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
              "path": "tools/pre_completion/grammar_checker.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "batch_metrics.json",
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/pre_completion/grammar_checker.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/grammar_checker.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_223318",
          "timestamp": "2026-01-12T22:33:18",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_223318",
          "results": {
            "metadata": {
              "model": "ollama/qwen2.5-coder:32b",
              "timestamp": "2026-01-12T22:34:05.679132"
            },
            "summary": {
              "baseline_tokens": 0,
              "rlm_tokens": 3797,
              "tasks_processed": 18535,
              "tools_created": 2,
              "macros_executed": 0,
              "llm_fallbacks": 0
            },
            "tools": [
              {
                "name": "pre_completion/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_223318/tools/pre_completion/grammar_checker.py",
                "description": "Detects grammar tasks",
                "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
                "created_at": "2026-01-12T22:33:56.535813"
              },
              {
                "name": "replacements/grammar_checker",
                "file_path": "experiment_outputs/exp005_20260112_223318/tools/replacements/grammar_checker.py",
                "description": "Checks grammar without LLM",
                "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
                "created_at": "2026-01-12T22:33:56.536914"
              }
            ],
            "macros": []
          },
          "tools": [
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "grammar_checker",
              "category": "pre_completion",
              "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
              "path": "tools/pre_completion/grammar_checker.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "batch_metrics.json",
            "report.pdf",
            "results.json",
            "tools/pre_completion/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/pre_completion/grammar_checker.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/grammar_checker.py"
          ]
        },
        {
          "experiment_id": "exp005",
          "run_id": "exp005_20260112_223418",
          "timestamp": "2026-01-12T22:34:18",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp005_20260112_223418",
          "results": {},
          "tools": [
            {
              "name": "text_length_analyzer",
              "category": "replacements",
              "code": "# Analyzes text length without LLM\n\ndef run(text: str) -> dict:\n    '''Analyze and report text length.'''\n    char_count = len(text)\n    word_count = len(text.split())\n    return {'characters': char_count, 'words': word_count}",
              "path": "tools/replacements/text_length_analyzer.py"
            },
            {
              "name": "grammar_checker",
              "category": "replacements",
              "code": "# Checks grammar without LLM\n\ndef run(text: str) -> dict:\n    '''Check if sentence is grammatically acceptable.'''\n    issues = []\n    if text and not text[0].isupper():\n        issues.append('Should start with capital')\n    if text and not text.rstrip().endswith(('.', '!', '?')):\n        issues.append('Should end with punctuation')\n    return {'acceptable': len(issues) == 0, 'issues': issues}",
              "path": "tools/replacements/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "replacements",
              "code": "# Analyzes PII without LLM\n\nimport re\n\ndef run(text: str) -> dict:\n    '''Detect and report PII in text.'''\n    found = []\n    if re.search(r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', text):\n        found.append('email')\n    if re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text):\n        found.append('phone')\n    if re.search(r'\\d{3}-\\d{2}-\\d{4}', text):\n        found.append('ssn')\n    return {'has_pii': len(found) > 0, 'pii_types': found}",
              "path": "tools/replacements/pii_detector.py"
            },
            {
              "name": "text_length_analyzer",
              "category": "pre_completion",
              "code": "# Detects tasks related to text length\n\ndef check(text: str) -> bool:\n    '''Return True if this is a task about text length.'''\n    keywords = ['length', 'characters', 'words']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
              "path": "tools/pre_completion/text_length_analyzer.py"
            },
            {
              "name": "grammar_checker",
              "category": "pre_completion",
              "code": "# Detects grammar tasks\n\ndef check(text: str) -> bool:\n    '''Return True if this is a grammar task.'''\n    keywords = ['grammar', 'grammatical', 'acceptable', 'sentence']\n    text_lower = text.lower()\n    return any(kw in text_lower for kw in keywords)",
              "path": "tools/pre_completion/grammar_checker.py"
            },
            {
              "name": "pii_detector",
              "category": "pre_completion",
              "code": "# Detects PII in text\n\nimport re\n\ndef check(text: str) -> bool:\n    '''Return True if text contains PII.'''\n    patterns = [\n        r'[\\w.+-]+@[\\w.-]+\\.[a-zA-Z]{2,}',  # email\n        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # phone\n        r'\\d{3}-\\d{2}-\\d{4}'  # SSN\n    ]\n    return any(re.search(p, text) for p in patterns)",
              "path": "tools/pre_completion/pii_detector.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "tools/pre_completion/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/pre_completion/__pycache__/pii_detector.cpython-314.pyc",
            "tools/pre_completion/__pycache__/text_length_analyzer.cpython-314.pyc",
            "tools/pre_completion/grammar_checker.py",
            "tools/pre_completion/pii_detector.py",
            "tools/pre_completion/text_length_analyzer.py",
            "tools/replacements/__pycache__/grammar_checker.cpython-314.pyc",
            "tools/replacements/__pycache__/pii_detector.cpython-314.pyc",
            "tools/replacements/__pycache__/text_length_analyzer.cpython-314.pyc",
            "tools/replacements/grammar_checker.py",
            "tools/replacements/pii_detector.py",
            "tools/replacements/text_length_analyzer.py"
          ]
        }
      ]
    },
    {
      "id": "exp006",
      "name": "Experiment 006: Prompt Variation Testing",
      "description": "This experiment tests different system prompt variations to find\nthe most effective approach for self-distillation.\n\nPrompt Versions:\n- v1_basic: Core principles with single example\n- v2_cost: Cost-aware with explicit cost/benefit reasoning\n- v3_pattern: Pattern learning focus\n- v4_minimal: Concise instructions, maximum autonomy\n- v5_reflective: Explicit reasoning about decisions\n\nMetrics tracked per prompt:\n- Tools created (hooks + replacements)\n- Tool quality (correct contract implementation)\n- Token usage (creation overhead vs savings)\n- LLM calls skipped\n\nFeatures:\n- Checkpoint support: saves after each prompt, can resume with --resume\n- Progress bars with ETA",
      "architecture": "",
      "source_file": "exp006_prompt_variations.py",
      "dataset_info": "CoLA (Grammar), PII Detection",
      "runs": [
        {
          "experiment_id": "exp006",
          "run_id": "exp006_20260112_230508",
          "timestamp": "2026-01-12T23:05:08",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp006_20260112_230508",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp006",
          "run_id": "exp006_20260112_231539",
          "timestamp": "2026-01-12T23:15:39",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp006_20260112_231539",
          "results": {
            "metadata": {
              "model": "ollama/llama3.2:3b",
              "timestamp": "2026-01-12T23:19:26.016640",
              "tasks_per_prompt": 11
            },
            "results": [
              {
                "prompt": "v1_basic",
                "description": "Basic constitution with core principles and single example",
                "hooks": 0,
                "replacements": 0,
                "valid_tools": 0,
                "error_tools": 0,
                "baseline_tokens": 2756,
                "rlm_tokens": 59531,
                "duration_seconds": 79.52993416786194
              },
              {
                "prompt": "v4_minimal",
                "description": "Minimal instructions, maximum LLM autonomy",
                "hooks": 0,
                "replacements": 0,
                "valid_tools": 0,
                "error_tools": 0,
                "baseline_tokens": 2822,
                "rlm_tokens": 102936,
                "duration_seconds": 145.6679811477661
              }
            ]
          },
          "tools": [],
          "sample_data": [],
          "directory_structure": [
            "prompt_comparison_report.pdf",
            "results.json"
          ]
        },
        {
          "experiment_id": "exp006",
          "run_id": "exp006_20260112_232204",
          "timestamp": "2026-01-12T23:22:04",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp006_20260112_232204",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp006",
          "run_id": "exp006_20260112_232254",
          "timestamp": "2026-01-12T23:22:54",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp006_20260112_232254",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp006",
          "run_id": "exp006_20260112_234253",
          "timestamp": "2026-01-12T23:42:53",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp006_20260112_234253",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp006",
          "run_id": "exp006_20260113_012404",
          "timestamp": "2026-01-13T01:24:04",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp006_20260113_012404",
          "results": {},
          "tools": [],
          "sample_data": [],
          "directory_structure": []
        },
        {
          "experiment_id": "exp006",
          "run_id": "exp006_20260113_013051",
          "timestamp": "2026-01-13T01:30:51",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp006_20260113_013051",
          "results": {
            "metadata": {
              "model": "ollama/qwen2.5-coder:32b",
              "timestamp": "2026-01-13T08:40:52.083346",
              "tasks_per_prompt": 31,
              "total_duration_minutes": 429.9949513713519
            },
            "results": [
              {
                "prompt_version": "v1_basic",
                "prompt_description": "Basic constitution with core principles and single example",
                "hooks_created": 0,
                "replacements_created": 0,
                "utilities_created": 0,
                "tools_with_correct_contract": 0,
                "tools_with_errors": 0,
                "baseline_tokens": 7712,
                "rlm_tokens": 195807,
                "tasks_processed": 31,
                "tasks_total": 31,
                "duration_seconds": 2034.7159349918365,
                "start_time": "2026-01-13T01:30:52.369149",
                "end_time": "2026-01-13T02:04:47.085646",
                "tool_samples": []
              },
              {
                "prompt_version": "v2_cost",
                "prompt_description": "Cost-aware with explicit cost/benefit analysis for tool creation",
                "hooks_created": 0,
                "replacements_created": 0,
                "utilities_created": 0,
                "tools_with_correct_contract": 0,
                "tools_with_errors": 0,
                "baseline_tokens": 9250,
                "rlm_tokens": 165630,
                "tasks_processed": 30,
                "tasks_total": 31,
                "duration_seconds": 2898.304675102234,
                "start_time": "2026-01-13T02:04:47.087419",
                "end_time": "2026-01-13T02:53:05.392535",
                "tool_samples": []
              },
              {
                "prompt_version": "v3_pattern",
                "prompt_description": "Pattern learning focus - observe, learn, protect, evolve",
                "hooks_created": 0,
                "replacements_created": 0,
                "utilities_created": 0,
                "tools_with_correct_contract": 0,
                "tools_with_errors": 0,
                "baseline_tokens": 7483,
                "rlm_tokens": 333191,
                "tasks_processed": 30,
                "tasks_total": 31,
                "duration_seconds": 8572.714200019836,
                "start_time": "2026-01-13T02:53:05.393925",
                "end_time": "2026-01-13T05:15:58.108888",
                "tool_samples": []
              },
              {
                "prompt_version": "v4_minimal",
                "prompt_description": "Minimal instructions, maximum LLM autonomy",
                "hooks_created": 0,
                "replacements_created": 0,
                "utilities_created": 0,
                "tools_with_correct_contract": 0,
                "tools_with_errors": 0,
                "baseline_tokens": 5945,
                "rlm_tokens": 189036,
                "tasks_processed": 28,
                "tasks_total": 31,
                "duration_seconds": 8011.878356218338,
                "start_time": "2026-01-13T05:15:58.109469",
                "end_time": "2026-01-13T07:29:29.988123",
                "tool_samples": []
              },
              {
                "prompt_version": "v5_reflective",
                "prompt_description": "Explicit reasoning and self-reflection before decisions",
                "hooks_created": 0,
                "replacements_created": 0,
                "utilities_created": 0,
                "tools_with_correct_contract": 0,
                "tools_with_errors": 0,
                "baseline_tokens": 7681,
                "rlm_tokens": 226911,
                "tasks_processed": 30,
                "tasks_total": 31,
                "duration_seconds": 4282.083915948868,
                "start_time": "2026-01-13T07:29:29.988728",
                "end_time": "2026-01-13T08:40:52.072956",
                "tool_samples": []
              }
            ]
          },
          "tools": [],
          "sample_data": [],
          "directory_structure": [
            "checkpoint.json",
            "prompt_comparison_report.pdf",
            "results.json"
          ]
        }
      ]
    },
    {
      "id": "exp007",
      "name": "Experiment 007: Evidence-Guided Tool Creation",
      "description": "This experiment uses embedding-based clustering to detect patterns\nin tasks BEFORE prompting the model. When a cluster is detected,\nwe add context to the prompt: \"Found N similar tasks - consider creating a tool.\"",
      "architecture": "Key difference from exp006:\n- exp006: Model decides if pattern exists (often misses patterns)\n- exp007: We detect patterns with embeddings, tell model explicitly\n\nArchitecture:\n    Task \u2192 Embed \u2192 Check cluster \u2192 If cluster found:\n                                      Add \"Found N similar tasks\" to prompt\n                                   \u2192 Process with RLM\n                                   \u2192 Store embedding for future detection\n\nFeatures:\n- Checkpoint support with --resume\n- Progress bars with ETA\n- Saves evidence store for analysis",
      "source_file": "exp007_evidence_guided_tools.py",
      "dataset_info": "CoLA (Grammar), PII Detection",
      "runs": [
        {
          "experiment_id": "exp007",
          "run_id": "exp007_20260113_031447",
          "timestamp": "2026-01-13T03:14:47",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp007_20260113_031447",
          "results": {
            "tasks_processed": 46,
            "tasks_total": 51,
            "patterns_detected": 0,
            "patterns_triggered_tools": 0,
            "hooks_created": 0,
            "replacements_created": 0,
            "baseline_tokens": 13596,
            "rlm_tokens": 387518,
            "duration_seconds": 15949.54368185997,
            "start_time": "2026-01-13T03:14:47.868824",
            "end_time": "2026-01-13T07:41:04.987807",
            "clusters_found": 0,
            "largest_cluster": 0,
            "avg_cluster_size": 0
          },
          "tools": [],
          "sample_data": [
            {
              "id": "cola_0",
              "text": "This girl in the or on the red coat will put a picture of Bill on your desk.",
              "type": "grammar"
            },
            {
              "id": "cola_1",
              "text": "So much did you eat that everyone gasped.",
              "type": "grammar"
            },
            {
              "id": "cola_2",
              "text": "I gave a gun to my brother which I had cleaned.",
              "type": "grammar"
            },
            {
              "id": "cola_3",
              "text": "Captain Oates died in order to save his comrades.",
              "type": "grammar"
            },
            {
              "id": "cola_4",
              "text": "Brian wiped the counter of fingerprints.",
              "type": "grammar"
            }
          ],
          "directory_structure": [
            "checkpoint.json",
            "evidence_store/config.json",
            "evidence_store/vectors/index.faiss",
            "evidence_store/vectors/metadata.json",
            "experiment_report.pdf",
            "results.json"
          ]
        },
        {
          "experiment_id": "exp007",
          "run_id": "exp007_20260113_104105",
          "timestamp": "2026-01-13T10:41:05",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp007_20260113_104105",
          "results": {
            "tasks_processed": 51,
            "tasks_total": 51,
            "patterns_detected": 0,
            "patterns_triggered_tools": 0,
            "hooks_created": 0,
            "replacements_created": 0,
            "baseline_tokens": 12607,
            "rlm_tokens": 330877,
            "duration_seconds": 4254.851261854172,
            "start_time": "2026-01-13T10:41:05.767738",
            "end_time": "2026-01-13T11:52:01.933910",
            "clusters_found": 0,
            "largest_cluster": 0,
            "avg_cluster_size": 0
          },
          "tools": [],
          "sample_data": [
            {
              "id": "cola_0",
              "text": "This girl in the or on the red coat will put a picture of Bill on your desk.",
              "type": "grammar"
            },
            {
              "id": "cola_1",
              "text": "So much did you eat that everyone gasped.",
              "type": "grammar"
            },
            {
              "id": "cola_2",
              "text": "I gave a gun to my brother which I had cleaned.",
              "type": "grammar"
            },
            {
              "id": "cola_3",
              "text": "Captain Oates died in order to save his comrades.",
              "type": "grammar"
            },
            {
              "id": "cola_4",
              "text": "Brian wiped the counter of fingerprints.",
              "type": "grammar"
            }
          ],
          "directory_structure": [
            "checkpoint.json",
            "evidence_store/config.json",
            "evidence_store/vectors/index.faiss",
            "evidence_store/vectors/metadata.json",
            "experiment_report.pdf",
            "results.json"
          ]
        }
      ]
    },
    {
      "id": "exp008",
      "name": "Experiment 008: Large PII Dataset with Evidence-Guided Tool Creation",
      "description": "Uses the AI4Privacy 200K dataset which has much better clustering properties\nthan CoLA grammar sentences. PII detection tasks cluster naturally around\nthemes like payments, names, addresses, etc.",
      "architecture": "Key improvements from exp007:\n- Uses AI4Privacy dataset (200K samples) instead of small CoLA/PII mix\n- Lower similarity threshold (0.5) matched to actual embedding similarities\n- Tasks cluster better: payment texts, name patterns, address patterns\n\nArchitecture:\n    Task \u2192 Embed \u2192 Check cluster \u2192 If cluster found (sim > 0.5):\n                                      Add \"Found N similar tasks\" to prompt\n                                   \u2192 Process with RLM\n                                   \u2192 Store embedding for future detection\n\nFeatures:\n- Checkpoint support with --resume\n- Progress bars with ETA\n- Saves evidence store for analysis",
      "source_file": "exp008_large_pii_evidence.py",
      "dataset_info": "CoLA (Grammar), PII Detection, AI4Privacy (200K PII)",
      "runs": [
        {
          "experiment_id": "exp008",
          "run_id": "exp008_20260113_145932",
          "timestamp": "2026-01-13T14:59:32",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp008_20260113_145932",
          "results": {
            "tasks_processed": 190,
            "tasks_total": 200,
            "patterns_detected": 6,
            "patterns_triggered_tools": 0,
            "hooks_created": 0,
            "replacements_created": 0,
            "baseline_tokens": 83440,
            "rlm_tokens": 1482190,
            "duration_seconds": 31521.461482048035,
            "start_time": "2026-01-13T14:59:32.287686",
            "end_time": "2026-01-13T23:44:56.672364",
            "clusters_found": 37,
            "largest_cluster": 12,
            "avg_cluster_size": 4.216216216216216
          },
          "tools": [],
          "sample_data": [
            {
              "id": "pii_0",
              "text": "A student's assessment was found on device bearing IMEI: 06-184755-866851-3. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?",
              "type": "pii_detection"
            },
            {
              "id": "pii_1",
              "text": "Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools. Please feedback on it's operability.",
              "type": "pii_detection"
            },
            {
              "id": "pii_2",
              "text": "Kattie could you please share your recomndations about vegetarian diet for 72 old Intersex person with 158centimeters?",
              "type": "pii_detection"
            },
            {
              "id": "pii_3",
              "text": "Emergency supplies in 16356 need a refill. Use 5890724654311332 to pay for them.",
              "type": "pii_detection"
            },
            {
              "id": "pii_4",
              "text": "The 88 old child at 5862, has showcased an unusual ability to remember and recite passwords, with Y2rWliOhf8Ir being most repeated.",
              "type": "pii_detection"
            }
          ],
          "directory_structure": [
            "checkpoint.json",
            "evidence_store/config.json",
            "evidence_store/vectors/index.faiss",
            "evidence_store/vectors/metadata.json",
            "experiment_report.pdf",
            "results.json"
          ]
        }
      ]
    },
    {
      "id": "exp009",
      "name": "Experiment 009: Isolated Tool Creation and Validation",
      "description": "Tests tool creation in isolation, without RLM complexity.\n\nFor each sample:\n1. Ask LLM to create a tool that solves this type of problem\n2. Ask LLM to generate a \"near miss\" adversarial example\n3. Run tool against original sample (should pass)\n4. Run tool against adversarial example (should handle correctly)\n5. Test tool against full dataset to find coverage/overlap\n\nThis validates the fundamental tool creation mechanism works before\nadding optimization complexity.",
      "architecture": "",
      "source_file": "exp009_tool_validation.py",
      "dataset_info": "PII Detection, AI4Privacy (200K PII)",
      "runs": [
        {
          "experiment_id": "exp009",
          "run_id": "exp009_20260113_165401",
          "timestamp": "2026-01-13T16:54:01",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp009_20260113_165401",
          "results": {
            "metadata": {
              "timestamp": "20260113_165401",
              "model": "qwen2.5-coder:32b",
              "samples_for_tools": 5,
              "dataset_size": 50
            },
            "tools": [
              {
                "name": "imei_detector",
                "description": "Detects IMEI numbers in the format used in the example text using regex.",
                "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    pattern = r'IMEI:\\s*(\\d{2}-\\d{6}-\\d{7}-\\d)'\n    results = []\n    for match in re.finditer(pattern, text):\n        results.append({\n            \"type\": \"PHONEIMEI\",\n            \"value\": match.group(1),\n            \"start\": match.start(1),\n            \"end\": match.end(1)\n        })\n    return results",
                "original_sample_id": "pii_0",
                "original_sample_text": "A student's assessment was found on device bearing IMEI: 06-184755-866851-3. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?",
                "adversarial_example": "A student's assessment was found on device bearing IMEI: 061847558668513. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?",
                "adversarial_expected": "Should not detect",
                "original_passed": false,
                "adversarial_passed": false,
                "coverage_count": 0,
                "coverage_total": 50
              },
              {
                "name": "pii_detector_omer_license",
                "description": "Detects specific types of PII including first names and license/vehicle VIN numbers in the given text format using regex.",
                "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    # Pattern for detecting first name 'Omer' (case insensitive)\n    firstname_pattern = r'\\bOmer\\b'\n    # Pattern for detecting vehicle/VIN number matching the example '78B5R2MVFAHJ48500'\n    vin_pattern = r'\\b[A-Z0-9]{17}\\b'\n    \n    results = []\n    \n    # Finding all matches for first name\n    for match in re.finditer(firstname_pattern, text, re.IGNORECASE):\n        results.append({\n            \"type\": \"FIRSTNAME\",\n            \"value\": match.group(),\n            \"start\": match.start(),\n            \"end\": match.end()\n        })\n    \n    # Finding all matches for vehicle/VIN number\n    for match in re.finditer(vin_pattern, text):\n        results.append({\n            \"type\": \"VEHICLEVIN\",\n            \"value\": match.group(),\n            \"start\": match.start(),\n            \"end\": match.end()\n        })\n    \n    return results\n\n# Example usage:\n# print(detect(\"Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools. Please feedback on it's operability.\"))",
                "original_sample_id": "pii_1",
                "original_sample_text": "Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools. Please feedback on it's operability.",
                "adversarial_example": "Dear Omer, as per our records, your license number 78B-5R2MV-F-AHJ4-8500 is still registered in our records for access to the educational tools. Please feedback on its operability.",
                "adversarial_expected": "Should not detect / Should detect as different TYPE",
                "original_passed": true,
                "adversarial_passed": true,
                "coverage_count": 2,
                "coverage_total": 50
              },
              {
                "name": "pii_detector_specific_pattern",
                "description": "Detects specific types of PII including FIRSTNAME, AGE, GENDER, and HEIGHT in a given text using regex patterns tailored to the example provided.",
                "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    # Define regex patterns for each PII type based on the example\n    patterns = {\n        \"FIRSTNAME\": r'\\b[A-Z][a-z]*\\b',\n        \"AGE\": r'\\b\\d{2}\\s?old\\b',\n        \"GENDER\": r'\\b(?:Intersex|Male|Female)\\b',  # Assuming these are the only genders to look for\n        \"HEIGHT\": r'\\b\\d+centimeters?\\b'\n    }\n    \n    results = []\n    \n    for pii_type, pattern in patterns.items():\n        for match in re.finditer(pattern, text):\n            results.append({\n                \"type\": pii_type,\n                \"value\": match.group(),\n                \"start\": match.start(),\n                \"end\": match.end()\n            })\n    \n    return results",
                "original_sample_id": "pii_2",
                "original_sample_text": "Kattie could you please share your recomndations about vegetarian diet for 72 old Intersex person with 158centimeters?",
                "adversarial_example": "Kattie could you please share your recommendations about vegetarian diet for 72 years old Intersex individual with a height of one hundred and fifty-eight centimeters?",
                "adversarial_expected": "Should not detect / Should detect as TYPE / etc",
                "original_passed": true,
                "adversarial_passed": true,
                "coverage_count": 50,
                "coverage_total": 50
              },
              {
                "name": "pii_detector_16356_5890724654311332_pattern",
                "description": "Detects specific PII patterns similar to '16356' (building number) and '5890724654311332' (masked number) using regex.",
                "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    # Regex pattern for building numbers like '16356'\n    building_number_pattern = r'\\b\\d{5}\\b'\n    # Regex pattern for masked numbers like '5890724654311332'\n    masked_number_pattern = r'\\b\\d{16}\\b'\n    \n    results = []\n    \n    # Finding all matches for building numbers\n    for match in re.finditer(building_number_pattern, text):\n        results.append({\n            \"type\": \"BUILDINGNUMBER\",\n            \"value\": match.group(),\n            \"start\": match.start(),\n            \"end\": match.end()\n        })\n    \n    # Finding all matches for masked numbers\n    for match in re.finditer(masked_number_pattern, text):\n        results.append({\n            \"type\": \"MASKEDNUMBER\",\n            \"value\": match.group(),\n            \"start\": match.start(),\n            \"end\": match.end()\n        })\n    \n    return results",
                "original_sample_id": "pii_3",
                "original_sample_text": "Emergency supplies in 16356 need a refill. Use 5890724654311332 to pay for them.",
                "adversarial_example": "Emergency supplies in 1635A need a refill. Use XXXX724654311332 to pay for them.",
                "adversarial_expected": "Should not detect the building number and should partially detect the masked number but with different pattern",
                "original_passed": true,
                "adversarial_passed": false,
                "coverage_count": 11,
                "coverage_total": 50
              },
              {
                "name": "specific_pii_detector",
                "description": "Detects specific types of PII including AGE, BUILDINGNUMBER, and PASSWORD based on the provided example pattern.",
                "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    patterns = {\n        \"AGE\": r'\\b\\d{1,3}\\b(?=\\sold\\b)',\n        \"BUILDINGNUMBER\": r'\\b\\d{4}\\b',\n        \"PASSWORD\": r'\\b[a-zA-Z0-9]{8,}\\b'\n    }\n    results = []\n    \n    for pii_type, pattern in patterns.items():\n        for match in re.finditer(pattern, text):\n            results.append({\n                \"type\": pii_type,\n                \"value\": match.group(),\n                \"start\": match.start(),\n                \"end\": match.end()\n            })\n    \n    return results",
                "original_sample_id": "pii_4",
                "original_sample_text": "The 88 old child at 5862, has showcased an unusual ability to remember and recite passwords, with Y2rWliOhf8Ir being most repeated.",
                "adversarial_example": "The 88 year-old child at building number fifty-eight sixty-two, has showcased an unusual ability to remember and recite passwords, with Y2rWliOhf8Ir being most repeated.",
                "adversarial_expected": "Should not detect AGE as a number or BUILDINGNUMBER as digits",
                "original_passed": true,
                "adversarial_passed": true,
                "coverage_count": 50,
                "coverage_total": 50
              }
            ],
            "overlap_analysis": {
              "imei_detector": {
                "original": "pii_0",
                "matches": [],
                "match_count": 0,
                "match_rate": 0.0
              },
              "pii_detector_omer_license": {
                "original": "pii_1",
                "matches": [
                  "pii_1",
                  "pii_23"
                ],
                "match_count": 2,
                "match_rate": 4.0
              },
              "pii_detector_specific_pattern": {
                "original": "pii_2",
                "matches": [
                  "pii_0",
                  "pii_1",
                  "pii_2",
                  "pii_3",
                  "pii_4",
                  "pii_5",
                  "pii_6",
                  "pii_7",
                  "pii_8",
                  "pii_9",
                  "pii_10",
                  "pii_11",
                  "pii_12",
                  "pii_13",
                  "pii_14",
                  "pii_15",
                  "pii_16",
                  "pii_17",
                  "pii_18",
                  "pii_19",
                  "pii_20",
                  "pii_21",
                  "pii_22",
                  "pii_23",
                  "pii_24",
                  "pii_25",
                  "pii_26",
                  "pii_27",
                  "pii_28",
                  "pii_29",
                  "pii_30",
                  "pii_31",
                  "pii_32",
                  "pii_33",
                  "pii_34",
                  "pii_35",
                  "pii_36",
                  "pii_37",
                  "pii_38",
                  "pii_39",
                  "pii_40",
                  "pii_41",
                  "pii_42",
                  "pii_43",
                  "pii_44",
                  "pii_45",
                  "pii_46",
                  "pii_47",
                  "pii_48",
                  "pii_49"
                ],
                "match_count": 50,
                "match_rate": 100.0
              },
              "pii_detector_16356_5890724654311332_pattern": {
                "original": "pii_3",
                "matches": [
                  "pii_3",
                  "pii_10",
                  "pii_12",
                  "pii_15",
                  "pii_16",
                  "pii_18",
                  "pii_26",
                  "pii_28",
                  "pii_40",
                  "pii_46",
                  "pii_48"
                ],
                "match_count": 11,
                "match_rate": 22.0
              },
              "specific_pii_detector": {
                "original": "pii_4",
                "matches": [
                  "pii_0",
                  "pii_1",
                  "pii_2",
                  "pii_3",
                  "pii_4",
                  "pii_5",
                  "pii_6",
                  "pii_7",
                  "pii_8",
                  "pii_9",
                  "pii_10",
                  "pii_11",
                  "pii_12",
                  "pii_13",
                  "pii_14",
                  "pii_15",
                  "pii_16",
                  "pii_17",
                  "pii_18",
                  "pii_19",
                  "pii_20",
                  "pii_21",
                  "pii_22",
                  "pii_23",
                  "pii_24",
                  "pii_25",
                  "pii_26",
                  "pii_27",
                  "pii_28",
                  "pii_29",
                  "pii_30",
                  "pii_31",
                  "pii_32",
                  "pii_33",
                  "pii_34",
                  "pii_35",
                  "pii_36",
                  "pii_37",
                  "pii_38",
                  "pii_39",
                  "pii_40",
                  "pii_41",
                  "pii_42",
                  "pii_43",
                  "pii_44",
                  "pii_45",
                  "pii_46",
                  "pii_47",
                  "pii_48",
                  "pii_49"
                ],
                "match_count": 50,
                "match_rate": 100.0
              }
            },
            "summary": {
              "tools_created": 5,
              "original_pass_rate": 80.0,
              "adversarial_correct_rate": 40.0,
              "avg_coverage": 22.6
            }
          },
          "tools": [
            {
              "name": "pii_detector_omer_license",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    # Pattern for detecting first name 'Omer' (case insensitive)\n    firstname_pattern = r'\\bOmer\\b'\n    # Pattern for detecting vehicle/VIN number matching the example '78B5R2MVFAHJ48500'\n    vin_pattern = r'\\b[A-Z0-9]{17}\\b'\n    \n    results = []\n    \n    # Finding all matches for first name\n    for match in re.finditer(firstname_pattern, text, re.IGNORECASE):\n        results.append({\n            \"type\": \"FIRSTNAME\",\n            \"value\": match.group(),\n            \"start\": match.start(),\n            \"end\": match.end()\n        })\n    \n    # Finding all matches for vehicle/VIN number\n    for match in re.finditer(vin_pattern, text):\n        results.append({\n            \"type\": \"VEHICLEVIN\",\n            \"value\": match.group(),\n            \"start\": match.start(),\n            \"end\": match.end()\n        })\n    \n    return results\n\n# Example usage:\n# print(detect(\"Dear Omer, as per our records, your license 78B5R2MVFAHJ48500 is still registered in our records for access to the educational tools. Please feedback on it's operability.\"))",
              "path": "tools/pii_detector_omer_license.py"
            },
            {
              "name": "pii_detector_specific_pattern",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    # Define regex patterns for each PII type based on the example\n    patterns = {\n        \"FIRSTNAME\": r'\\b[A-Z][a-z]*\\b',\n        \"AGE\": r'\\b\\d{2}\\s?old\\b',\n        \"GENDER\": r'\\b(?:Intersex|Male|Female)\\b',  # Assuming these are the only genders to look for\n        \"HEIGHT\": r'\\b\\d+centimeters?\\b'\n    }\n    \n    results = []\n    \n    for pii_type, pattern in patterns.items():\n        for match in re.finditer(pattern, text):\n            results.append({\n                \"type\": pii_type,\n                \"value\": match.group(),\n                \"start\": match.start(),\n                \"end\": match.end()\n            })\n    \n    return results",
              "path": "tools/pii_detector_specific_pattern.py"
            },
            {
              "name": "specific_pii_detector",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    patterns = {\n        \"AGE\": r'\\b\\d{1,3}\\b(?=\\sold\\b)',\n        \"BUILDINGNUMBER\": r'\\b\\d{4}\\b',\n        \"PASSWORD\": r'\\b[a-zA-Z0-9]{8,}\\b'\n    }\n    results = []\n    \n    for pii_type, pattern in patterns.items():\n        for match in re.finditer(pattern, text):\n            results.append({\n                \"type\": pii_type,\n                \"value\": match.group(),\n                \"start\": match.start(),\n                \"end\": match.end()\n            })\n    \n    return results",
              "path": "tools/specific_pii_detector.py"
            },
            {
              "name": "imei_detector",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    pattern = r'IMEI:\\s*(\\d{2}-\\d{6}-\\d{7}-\\d)'\n    results = []\n    for match in re.finditer(pattern, text):\n        results.append({\n            \"type\": \"PHONEIMEI\",\n            \"value\": match.group(1),\n            \"start\": match.start(1),\n            \"end\": match.end(1)\n        })\n    return results",
              "path": "tools/imei_detector.py"
            },
            {
              "name": "pii_detector_16356_5890724654311332_pattern",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    # Regex pattern for building numbers like '16356'\n    building_number_pattern = r'\\b\\d{5}\\b'\n    # Regex pattern for masked numbers like '5890724654311332'\n    masked_number_pattern = r'\\b\\d{16}\\b'\n    \n    results = []\n    \n    # Finding all matches for building numbers\n    for match in re.finditer(building_number_pattern, text):\n        results.append({\n            \"type\": \"BUILDINGNUMBER\",\n            \"value\": match.group(),\n            \"start\": match.start(),\n            \"end\": match.end()\n        })\n    \n    # Finding all matches for masked numbers\n    for match in re.finditer(masked_number_pattern, text):\n        results.append({\n            \"type\": \"MASKEDNUMBER\",\n            \"value\": match.group(),\n            \"start\": match.start(),\n            \"end\": match.end()\n        })\n    \n    return results",
              "path": "tools/pii_detector_16356_5890724654311332_pattern.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "results.json",
            "tools/imei_detector.py",
            "tools/pii_detector_16356_5890724654311332_pattern.py",
            "tools/pii_detector_omer_license.py",
            "tools/pii_detector_specific_pattern.py",
            "tools/specific_pii_detector.py"
          ]
        }
      ]
    },
    {
      "id": "exp010",
      "name": "Experiment 010: Iterative Tool Refinement",
      "description": "The model creates a tool, tests it, sees results, and iterates to improve.\n\nLoop:\n1. Model creates/refines a detection tool\n2. Test against labeled dataset\n3. Show model: true positives, false positives, false negatives\n4. Model creates improved version\n5. Repeat N times\n\nTracks precision/recall/F1 over iterations to see if model learns.",
      "architecture": "",
      "source_file": "exp010_iterative_refinement.py",
      "dataset_info": "PII Detection, AI4Privacy (200K PII)",
      "runs": [
        {
          "experiment_id": "exp010",
          "run_id": "exp010_20260113_171400",
          "timestamp": "2026-01-13T17:14:00",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp010_20260113_171400",
          "results": {
            "metadata": {
              "timestamp": "20260113_171400",
              "model": "qwen2.5-coder:32b",
              "iterations": 1,
              "dataset_size": 50
            },
            "best": {
              "iteration": 0,
              "f1": 1.0,
              "precision": 1.0,
              "recall": 1.0
            },
            "history": [
              {
                "iteration": 0,
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "tp": 50,
                "fp": 0,
                "fn": 0,
                "tn": 0,
                "error": null
              }
            ]
          },
          "tools": [
            {
              "name": "iter_000",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    patterns = {\n        \"EMAIL\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n        \"PHONENUMBER\": r\"\\+?\\d[\\d -]{8,}\\d\",\n        \"SSN\": r\"\\b\\d{3}[- ]?\\d{2}[- ]?\\d{4}\\b\",\n        \"FIRSTNAME\": r\"\\b[A-Z][a-z]*\\b\",\n        \"BUILDINGNUMBER\": r\"\\b\\d+\\b\",\n        \"MASKEDNUMBER\": r\"\\b\\d{16}\\b\",\n        \"AGE\": r\"\\b\\d{1,3}(?:\\s?(?:years?|old))?\\b\",\n        \"GENDER\": r\"\\b(?:male|female|intersex)\\b\",\n        \"HEIGHT\": r\"\\b\\d+(?:\\.\\d+)?\\s*centimeters?\\b\",\n        \"PASSWORD\": r\"(?:\\b[a-zA-Z0-9]{8,}\\b)\",\n        \"VEHICLEVIN\": r\"\\b[A-HJ-NPR-TV-Z0-9]{17}\\b\",\n        \"PHONEIMEI\": r\"\\b\\d{2}-\\d{6}-\\d{6}-\\d{1}\\b\"\n    }\n    \n    matches = []\n    for pii_type, pattern in patterns.items():\n        for match in re.finditer(pattern, text):\n            matches.append({\n                \"type\": pii_type,\n                \"value\": match.group(),\n                \"start\": match.start(),\n                \"end\": match.end()\n            })\n    \n    return matches",
              "path": "tools/iter_000.py"
            },
            {
              "name": "best",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    patterns = {\n        \"EMAIL\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n        \"PHONENUMBER\": r\"\\+?\\d[\\d -]{8,}\\d\",\n        \"SSN\": r\"\\b\\d{3}[- ]?\\d{2}[- ]?\\d{4}\\b\",\n        \"FIRSTNAME\": r\"\\b[A-Z][a-z]*\\b\",\n        \"BUILDINGNUMBER\": r\"\\b\\d+\\b\",\n        \"MASKEDNUMBER\": r\"\\b\\d{16}\\b\",\n        \"AGE\": r\"\\b\\d{1,3}(?:\\s?(?:years?|old))?\\b\",\n        \"GENDER\": r\"\\b(?:male|female|intersex)\\b\",\n        \"HEIGHT\": r\"\\b\\d+(?:\\.\\d+)?\\s*centimeters?\\b\",\n        \"PASSWORD\": r\"(?:\\b[a-zA-Z0-9]{8,}\\b)\",\n        \"VEHICLEVIN\": r\"\\b[A-HJ-NPR-TV-Z0-9]{17}\\b\",\n        \"PHONEIMEI\": r\"\\b\\d{2}-\\d{6}-\\d{6}-\\d{1}\\b\"\n    }\n    \n    matches = []\n    for pii_type, pattern in patterns.items():\n        for match in re.finditer(pattern, text):\n            matches.append({\n                \"type\": pii_type,\n                \"value\": match.group(),\n                \"start\": match.start(),\n                \"end\": match.end()\n            })\n    \n    return matches",
              "path": "tools/best.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "results.json",
            "tools/best.py",
            "tools/iter_000.py"
          ]
        },
        {
          "experiment_id": "exp010",
          "run_id": "exp010_20260113_171918",
          "timestamp": "2026-01-13T17:19:18",
          "output_dir": "/Users/eduardodeleon/Documents/code/rlm-self-distill/experiment_outputs/exp010_20260113_171918",
          "results": {
            "metadata": {
              "timestamp": "20260113_171918",
              "model": "qwen2.5-coder:32b",
              "iterations": 22,
              "dataset_size": 50
            },
            "best": {
              "iteration": 14,
              "f1": 0.41399416909620995,
              "precision": 0.3697916666666667,
              "recall": 0.47019867549668876
            },
            "history": [
              {
                "iteration": 0,
                "precision": 0.23786407766990292,
                "recall": 0.32450331125827814,
                "f1": 0.27450980392156865,
                "tp": 49,
                "fp": 157,
                "fn": 102,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 1,
                "precision": 0.3372093023255814,
                "recall": 0.3841059602649007,
                "f1": 0.3591331269349845,
                "tp": 58,
                "fp": 114,
                "fn": 93,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 2,
                "precision": 0.40816326530612246,
                "recall": 0.3973509933774834,
                "f1": 0.40268456375838924,
                "tp": 60,
                "fp": 87,
                "fn": 91,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 3,
                "precision": 0.41333333333333333,
                "recall": 0.4105960264900662,
                "f1": 0.4119601328903654,
                "tp": 62,
                "fp": 88,
                "fn": 89,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 4,
                "precision": 0.41333333333333333,
                "recall": 0.4105960264900662,
                "f1": 0.4119601328903654,
                "tp": 62,
                "fp": 88,
                "fn": 89,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 5,
                "precision": 0.41333333333333333,
                "recall": 0.4105960264900662,
                "f1": 0.4119601328903654,
                "tp": 62,
                "fp": 88,
                "fn": 89,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 6,
                "precision": 0.41333333333333333,
                "recall": 0.4105960264900662,
                "f1": 0.4119601328903654,
                "tp": 62,
                "fp": 88,
                "fn": 89,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 7,
                "precision": 0.41333333333333333,
                "recall": 0.4105960264900662,
                "f1": 0.4119601328903654,
                "tp": 62,
                "fp": 88,
                "fn": 89,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 8,
                "precision": 0.41333333333333333,
                "recall": 0.4105960264900662,
                "f1": 0.4119601328903654,
                "tp": 62,
                "fp": 88,
                "fn": 89,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 9,
                "precision": 0.3235294117647059,
                "recall": 0.4370860927152318,
                "f1": 0.37183098591549296,
                "tp": 66,
                "fp": 138,
                "fn": 85,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 10,
                "precision": 0.3167420814479638,
                "recall": 0.46357615894039733,
                "f1": 0.37634408602150543,
                "tp": 70,
                "fp": 151,
                "fn": 81,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 11,
                "precision": 0.319634703196347,
                "recall": 0.46357615894039733,
                "f1": 0.37837837837837834,
                "tp": 70,
                "fp": 149,
                "fn": 81,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 12,
                "precision": 0.319634703196347,
                "recall": 0.46357615894039733,
                "f1": 0.37837837837837834,
                "tp": 70,
                "fp": 149,
                "fn": 81,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 13,
                "precision": 0.3333333333333333,
                "recall": 0.46357615894039733,
                "f1": 0.38781163434903043,
                "tp": 70,
                "fp": 140,
                "fn": 81,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 14,
                "precision": 0.3697916666666667,
                "recall": 0.47019867549668876,
                "f1": 0.41399416909620995,
                "tp": 71,
                "fp": 121,
                "fn": 80,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 15,
                "precision": 0.33482142857142855,
                "recall": 0.4966887417218543,
                "f1": 0.4,
                "tp": 75,
                "fp": 149,
                "fn": 76,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 16,
                "precision": 0.35377358490566035,
                "recall": 0.4966887417218543,
                "f1": 0.4132231404958677,
                "tp": 75,
                "fp": 137,
                "fn": 76,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 17,
                "precision": 0.33783783783783783,
                "recall": 0.4966887417218543,
                "f1": 0.40214477211796246,
                "tp": 75,
                "fp": 147,
                "fn": 76,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 18,
                "precision": 0.33482142857142855,
                "recall": 0.4966887417218543,
                "f1": 0.4,
                "tp": 75,
                "fp": 149,
                "fn": 76,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 19,
                "precision": 0.3232758620689655,
                "recall": 0.4966887417218543,
                "f1": 0.391644908616188,
                "tp": 75,
                "fp": 157,
                "fn": 76,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 20,
                "precision": 0.3232758620689655,
                "recall": 0.4966887417218543,
                "f1": 0.391644908616188,
                "tp": 75,
                "fp": 157,
                "fn": 76,
                "tn": 0,
                "error": null
              },
              {
                "iteration": 21,
                "precision": 0.31759656652360513,
                "recall": 0.4900662251655629,
                "f1": 0.38541666666666663,
                "tp": 74,
                "fp": 159,
                "fn": 77,
                "tn": 0,
                "error": null
              }
            ]
          },
          "tools": [
            {
              "name": "iter_020",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    patterns = {\n        \"EMAIL\": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n        \"PHONENUMBER\": r'\\b(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b',\n        \"SSN\": r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n        \"FIRSTNAME\": r'\\b(?:[A-Z][a-z]{1,29})(?!\\w)(?!\\W@\\w)(?<!\\d)(?!\\s*\\d)\\b',\n        \"LASTNAME\": r'\\b(?:[A-Z][a-z]{1,29})(?!\\w)(?!\\W@\\w)(?<!\\d)(?!\\s*\\d)\\b',\n        \"PHONEIMEI\": r'\\b\\d{15}\\b|\\b\\d{2}[ -]?\\d{6}[ -]?\\d{6}[ -]?\\d\\b',\n        \"VEHICLEVIN\": r'\\b([A-HJ-NPR-Z0-9]{3}[A-HJ-NPR-Z0-9]?[A-HJ-NPR-Z0-9]{3}[0-9A-HJ-NPR-Z0-9]{4})\\b',\n        \"AGE\": r'\\b\\d{1,2}\\s?(?:years?|old)\\b',\n        \"GENDER\": r'\\b(?:male|female|non-binary|intersex)\\b',\n        \"HEIGHT\": r'\\b(?:\\d{2,3}(?:cms?|centimeters?)|(?:\\d{3,4})(?:mm|meters?))\\b',\n        \"BUILDINGNUMBER\": r'\\b\\d{1,5}\\b(?!\\w)(?!-[A-Za-z])(?!\\s[A-Z][a-z]+)(?![A-Z][a-z]{1,29})\\b',\n        \"MASKEDNUMBER\": r'\\b(?:\\d{4}-){2,3}\\d{4}\\b|\\b\\d{12,16}\\b',\n        \"PASSWORD\": r'\\b(?=.*[A-Za-z])(?=.*\\d)(?=.*[@$!%*#?&])[A-Za-z\\d@$!%*#?&]{8,}\\b',\n        \"TIME\": r'\\b(?:[01]?\\d|2[0-3]):[0-5]\\d\\b',\n        \"IPV4\": r'\\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b(?:/\\d{1,2})?\\b',\n        \"AMOUNT\": r'\\b(?:\\$\\s?|\\\u20ac\\s?|\u00a3\\s?)?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?(?:\\s?[A-Za-z]{3})?\\b',\n        \"CURRENCYSYMBOL\": r'\\b[\\$\\\u20ac\u00a3]\\b',\n        \"DOB\": r'\\b(?:0[1-9]|[12][0-9]|3[01])[-/](?:0[1-9]|1[0-2])[-/](?:\\d{4}|\\d{2})\\b',\n        \"NEARBYGPSCOORDINATE\": r'\\b(?:-?\\d+\\.\\d+,\\s*-?\\d+\\.\\d+)\\b',\n        \"PREFIX\": r'\\b(?:Mr\\.?|Mrs\\.?|Ms\\.?|Dr\\.?)\\b',\n        \"STATE\": r'\\b(?:AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY)\\b',\n        \"JOBAREA\": r'\\b(?:Engineer|Doctor|Nurse|Teacher|Lawyer|Manager|Analyst|Developer|Designer|Consultant|Technician|Accountant|Architect|Scientist|Clerk|Officer|Salesperson|Receptionist|Chef|Mechanic)\\b',\n        \"IPV6\": r'\\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\\b'\n    }\n    \n    additional_patterns = {\n        \"CURRENCYSYMBOL\": r'\\b(?:USD|EUR|GBP)\\b',\n        \"STATE\": r'\\b(?:Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New\\s+Hampshire|New\\s+Jersey|New\\s+Mexico|New\\s+York|North\\s+Carolina|North\\s+Dakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode\\s+Island|South\\s+Carolina|South\\s+Dakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West\\s+Virginia|Wisconsin|Wyoming)\\b',\n        \"JOBAREA\": r'\\b(?:Engineer|Doctor|Nurse|Teacher|Lawyer|Manager|Analyst|Developer|Designer|Consultant|Technician|Accountant|Architect|Scientist|Clerk|Officer|Salesperson|Receptionist|Chef|Mechanic)\\b',\n        \"ACCOUNTNUMBER\": r'\\b\\d{12,16}\\b'\n    }\n    \n    patterns.update(additional_patterns)\n    \n    matches = []\n    for pii_type, pattern in patterns.items():\n        for match in re.finditer(pattern, text):\n            # Check context to reduce false positives\n            if pii_type == \"AMOUNT\":\n                if any(char.isalpha() for char in match.group()):\n                    continue  # Skip amounts with alphabetic characters\n            \n            if pii_type == \"FIRSTNAME\" or pii_type == \"LASTNAME\":\n                # Ensure the name is not part of a longer word\n                if re.search(r'\\b' + re.escape(match.group()) + r'\\b', text) is None:\n                    continue\n                # Additional check to ensure it's a standalone name\n                if match.start() > 0 and text[match.start()-1].isalnum():\n                    continue\n                if match.end() < len(text) and text[match.end()].isalnum():\n                    continue\n            \n            if pii_type == \"BUILDINGNUMBER\":\n                # Ensure the building number is not part of a larger number or word\n                if re.search(r'\\b' + match.group() + r'\\b', text) is None:\n                    continue\n                # Additional check to ensure it's not part of a longer sequence of digits\n                if match.start() > 0 and text[match.start()-1].isdigit():\n                    continue\n                if match.end() < len(text) and text[match.end()].isdigit():\n                    continue\n            \n            matches.append({'type': pii_type, 'value': match.group()})\n    \n    return matches",
              "path": "tools/iter_020.py"
            },
            {
              "name": "iter_000",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    patterns = {\n        \"EMAIL\": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n        \"PHONENUMBER\": r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b',\n        \"SSN\": r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n        \"FIRSTNAME\": r'\\b[A-Z][a-z]*\\b',\n        \"LASTNAME\": r'\\b[A-Z][a-z]*\\b',\n        \"PHONEIMEI\": r'\\b\\d{2}[-.\\s]?\\d{6}[-.\\s]?\\d{6}[-.\\s]?\\d{1}\\b',\n        \"VEHICLEVIN\": r'\\b([A-HJ-NPR-Z0-9]{3}[A-HJ-NPR-Z0-9\\s]?[A-HJ-NPR-Z0-9]{3}[0-9A-HJ-NPR-Z0-9]{4})\\b',\n        \"AGE\": r'\\b\\d{1,3}\\s?(years?|old)\\b',\n        \"GENDER\": r'\\b(male|female|non-binary|intersex)\\b',\n        \"HEIGHT\": r'\\b\\d{2,3}(cms?|centimeters?)\\b',\n        \"BUILDINGNUMBER\": r'\\b\\d{1,6}\\b',\n        \"MASKEDNUMBER\": r'\\b\\d{10,16}\\b',\n        \"PASSWORD\": r'\\b[A-Za-z0-9@#$%^&+=]{8,}\\b'\n    }\n    \n    matches = []\n    \n    for pii_type, pattern in patterns.items():\n        for match in re.finditer(pattern, text):\n            matches.append({\n                \"type\": pii_type,\n                \"value\": match.group(),\n                \"start\": match.start(),\n                \"end\": match.end()\n            })\n    \n    return matches",
              "path": "tools/iter_000.py"
            },
            {
              "name": "iter_010",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    patterns = {\n        \"EMAIL\": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n        \"PHONENUMBER\": r'\\b(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b',\n        \"SSN\": r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n        \"FIRSTNAME\": r'\\b(?:[A-Z][a-z]{1,29}(?=\\W|$)(?!@\\w)(?<!\\d)(?!\\s*\\d)\\b(?!(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b)(?<!\\b\\d{3}-\\d{2}-\\d{4}\\b))',\n        \"LASTNAME\": r'\\b(?:[A-Z][a-z]{1,29}(?=\\W|$)(?!@\\w)(?<!\\d)(?!\\s*\\d)\\b(?!(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b)(?<!\\b\\d{3}-\\d{2}-\\d{4}\\b))',\n        \"PHONEIMEI\": r'\\b(?:\\d{15}|\\d{2}[ -]?\\d{6}[ -]?\\d{6}[ -]?\\d)\\b',\n        \"VEHICLEVIN\": r'\\b([A-HJ-NPR-Z0-9]{3}[A-HJ-NPR-Z0-9]?[A-HJ-NPR-Z0-9]{3}[0-9A-HJ-NPR-Z0-9]{4})\\b',\n        \"AGE\": r'\\b\\d{1,2}\\s?(?:years?|old)\\b',\n        \"GENDER\": r'\\b(?:male|female|non-binary|intersex)\\b',\n        \"HEIGHT\": r'\\b(?:\\d{2,3}(?:cms?|centimeters?)|(?:\\d{3,4})(?:mm|meters?))\\b',\n        \"BUILDINGNUMBER\": r'\\b\\d{1,5}\\b(?!\\w)(?!-[A-Za-z])(?!\\s[A-Z][a-z]+)(?![A-Z][a-z]{1,29})',\n        \"MASKEDNUMBER\": r'\\b(?:\\d{4}-){2,3}\\d{4}\\b|\\b\\d{12,16}\\b',\n        \"PASSWORD\": r'\\b(?=.*[A-Za-z])(?=.*\\d)(?=.*[@$!%*#?&])[A-Za-z\\d@$!%*#?&]{8,}\\b(?!\\s+years?)\\b',\n        \"TIME\": r'\\b(?:[01]?\\d|2[0-3]):[0-5]\\d\\b',\n        \"IPV4\": r'\\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b(?:/\\d{1,2})?\\b',\n        \"AMOUNT\": r'\\b(?:\\$\\s?|\\\u20ac\\s?|\u00a3\\s?)?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?(?:\\s?[A-Za-z]{3})?\\b',\n        \"CURRENCYSYMBOL\": r'\\b[\\$\\\u20ac\u00a3]\\b',\n        \"DOB\": r'\\b(?:0[1-9]|[12][0-9]|3[01])[-/](?:0[1-9]|1[0-2])[-/](?:\\d{4}|\\d{2})\\b',\n        \"NEARBYGPSCOORDINATE\": r'\\b(?:-?\\d+\\.\\d+,\\s*-?\\d+\\.\\d+)\\b',\n        \"PREFIX\": r'\\b(?:Mr\\.?|Mrs\\.?|Ms\\.?|Dr\\.?)\\b',\n        \"STATE\": r'\\b(?:AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY)\\b',\n        \"JOBAREA\": r'\\b(?:Engineer|Doctor|Nurse|Teacher|Lawyer|Manager|Analyst|Developer|Designer|Consultant|Technician|Accountant|Architect|Scientist|Clerk|Officer|Salesperson|Receptionist|Chef|Mechanic)\\b'\n    }\n    \n    # Add more specific patterns for common false positives\n    false_positive_patterns = {\n        \"FIRSTNAME\": r'\\b(?:[A-Z][a-z]{1,29})\\b(?=\\W|$)(?![0-9])(?!\\s+[0-9])',\n        \"LASTNAME\": r'\\b(?:[A-Z][a-z]{1,29})\\b(?=\\W|$)(?![0-9])(?!\\s+[0-9])',\n        \"BUILDINGNUMBER\": r'\\b\\d{1,5}\\b(?!\\w)(?!-[A-Za-z])(?!\\s[A-Z][a-z]+)(?![A-Z][a-z]{1,29})(?!\\d+\\D*\\d+)\\b',\n        \"PASSWORD\": r'\\b(?=.*[A-Za-z])(?=.*\\d)(?=.*[@$!%*#?&])[A-Za-z\\d@$!%*#?&]{8,}\\b(?!\\s+years?)\\b',\n        \"AMOUNT\": r'\\b(?:\\$\\s?|\\\u20ac\\s?|\u00a3\\s?)?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?(?:\\s?[A-Za-z]{3})?\\b(?!\\s+years?)\\b'\n    }\n    \n    # Combine patterns and false positive patterns\n    combined_patterns = {**patterns, **false_positive_patterns}\n    \n    matches = []\n    for p_type, pattern in combined_patterns.items():\n        matches.extend((match.group(), p_type) for match in re.finditer(pattern, text))\n    \n    # Filter out false positives based on context\n    filtered_matches = []\n    for match, p_type in matches:\n        if p_type == \"FIRSTNAME\" or p_type == \"LASTNAME\":\n            # Ensure the detected name is not part of an IMEI or license number\n            if re.search(r'\\b\\d{14}\\b|\\b[A-Z0-9]{17}\\b', text):\n                continue\n        filtered_matches.append({'value': match, 'type': p_type})\n    \n    return filtered_matches",
              "path": "tools/iter_010.py"
            },
            {
              "name": "best",
              "category": "general",
              "code": "import re\n\ndef detect(text: str) -> list[dict]:\n    patterns = {\n        \"EMAIL\": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n        \"PHONENUMBER\": r'\\b(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b',\n        \"SSN\": r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n        \"FIRSTNAME\": r'\\b(?:[A-Z][a-z]{1,29})\\b(?!\\W@\\w)(?<!\\d)(?!\\s*\\d)\\b(?!(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b)(?<!\\b\\d{3}-\\d{2}-\\d{4}\\b)',\n        \"LASTNAME\": r'\\b(?:[A-Z][a-z]{1,29})\\b(?!\\W@\\w)(?<!\\d)(?!\\s*\\d)\\b(?!(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b)(?<!\\b\\d{3}-\\d{2}-\\d{4}\\b)',\n        \"PHONEIMEI\": r'\\b(?:\\d{15}|\\d{2}[ -]?\\d{6}[ -]?\\d{6}[ -]?\\d)\\b',\n        \"VEHICLEVIN\": r'\\b([A-HJ-NPR-Z0-9]{3}[A-HJ-NPR-Z0-9]?[A-HJ-NPR-Z0-9]{3}[0-9A-HJ-NPR-Z0-9]{4})\\b',\n        \"AGE\": r'\\b\\d{1,2}\\s?(?:years?|old)\\b',\n        \"GENDER\": r'\\b(?:male|female|non-binary|intersex)\\b',\n        \"HEIGHT\": r'\\b(?:\\d{2,3}(?:cms?|centimeters?)|(?:\\d{3,4})(?:mm|meters?))\\b',\n        \"BUILDINGNUMBER\": r'\\b\\d{1,5}\\b(?!\\w)(?!-[A-Za-z])(?!\\s[A-Z][a-z]+)(?![A-Z][a-z]{1,29})\\b',\n        \"MASKEDNUMBER\": r'\\b(?:\\d{4}-){2,3}\\d{4}\\b|\\b\\d{12,16}\\b',\n        \"PASSWORD\": r'\\b(?=.*[A-Za-z])(?=.*\\d)(?=.*[@$!%*#?&])[A-Za-z\\d@$!%*#?&]{8,}\\b',\n        \"TIME\": r'\\b(?:[01]?\\d|2[0-3]):[0-5]\\d\\b',\n        \"IPV4\": r'\\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b(?:/\\d{1,2})?\\b',\n        \"AMOUNT\": r'\\b(?:\\$\\s?|\\\u20ac\\s?|\u00a3\\s?)?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?(?:\\s?[A-Za-z]{3})?\\b',\n        \"CURRENCYSYMBOL\": r'\\b[\\$\\\u20ac\u00a3]\\b',\n        \"DOB\": r'\\b(?:0[1-9]|[12][0-9]|3[01])[-/](?:0[1-9]|1[0-2])[-/](?:\\d{4}|\\d{2})\\b',\n        \"NEARBYGPSCOORDINATE\": r'\\b(?:-?\\d+\\.\\d+,\\s*-?\\d+\\.\\d+)\\b',\n        \"PREFIX\": r'\\b(?:Mr\\.?|Mrs\\.?|Ms\\.?|Dr\\.?)\\b',\n        \"STATE\": r'\\b(?:AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY)\\b',\n        \"JOBAREA\": r'\\b(?:Engineer|Doctor|Nurse|Teacher|Lawyer|Manager|Analyst|Developer|Designer|Consultant|Technician|Accountant|Architect|Scientist|Clerk|Officer|Salesperson|Receptionist|Chef|Mechanic)\\b',\n        \"IPV6\": r'\\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\\b'\n    }\n    \n    matches = []\n    for p_type, pattern in patterns.items():\n        matches.extend((match.group(), p_type) for match in re.finditer(pattern, text))\n    \n    # Filter out false positives based on context\n    filtered_matches = []\n    for match, p_type in matches:\n        if p_type == \"FIRSTNAME\" or p_type == \"LASTNAME\":\n            # Ensure the detected name is not part of an IMEI or license number\n            if re.search(r'\\b\\d{14}\\b|\\b[A-Z0-9]{17}\\b', text):\n                continue\n        elif p_type == \"BUILDINGNUMBER\":\n            # Ensure building number is not a part of another number\n            if len(match) < 3 or re.search(r'\\b\\d{6,}\\b', match):\n                continue\n        elif p_type == \"AMOUNT\":\n            # Ensure amount is not confused with other numbers\n            if re.search(r'\\b\\d{3}(?:,\\d{3})*(?:\\.\\d{2})?\\b(?!\\s+[A-Za-z]{3})', match):\n                continue\n        filtered_matches.append({'value': match, 'type': p_type})\n    \n    # Additional filtering for specific false positives\n    final_filtered_matches = []\n    for item in filtered_matches:\n        if item['type'] == \"LASTNAME\" and re.search(r'\\b\\d{14}\\b|\\b[A-Z0-9]{17}\\b', text):\n            continue\n        elif item['type'] == \"FIRSTNAME\" and re.search(r'\\b\\d{14}\\b|\\b[A-Z0-9]{17}\\b', text):\n            continue\n        elif item['type'] == \"AMOUNT\":\n            # Ensure the amount is not part of a larger number or date\n            if re.search(r'\\d{3}(?:,\\d{3})*(?:\\.\\d{2})?\\b(?!\\s+[A-Za-z]{3})', match):\n                continue\n        elif item['type'] == \"PASSWORD\":\n            # Ensure the password is not part of a larger string that could be confused with names\n            if re.search(r'[a-zA-Z\\d@$!%*#?&]', match):\n                continue\n        final_filtered_matches.append(item)\n    \n    # Add new patterns for missed types\n    additional_patterns = {\n        \"CURRENCYSYMBOL\": r'\\b(?:USD|EUR|GBP)\\b',\n        \"STATE\": r'\\b(?:Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New Hampshire|New Jersey|New Mexico|New York|North Carolina|North Dakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode Island|South Carolina|South Dakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West Virginia|Wisconsin|Wyoming)\\b',\n        \"GENDER\": r'\\b(?:Male|Female|Non-binary|Intersex)\\b',\n        \"JOBAREA\": r'\\b(?:Engineer|Doctor|Nurse|Teacher|Lawyer|Manager|Analyst|Developer|Designer|Consultant|Technician|Accountant|Architect|Scientist|Clerk|Officer|Salesperson|Receptionist|Chef)\\b'\n    }\n    \n    for p_type, pattern in additional_patterns.items():\n        matches = re.finditer(pattern, text)\n        for match in matches:\n            final_filtered_matches.append({'value': match.group(), 'type': p_type})\n    \n    return final_filtered_matches",
              "path": "tools/best.py"
            }
          ],
          "sample_data": [],
          "directory_structure": [
            "results.json",
            "tools/best.py",
            "tools/iter_000.py",
            "tools/iter_010.py",
            "tools/iter_020.py"
          ]
        }
      ]
    }
  ]
}